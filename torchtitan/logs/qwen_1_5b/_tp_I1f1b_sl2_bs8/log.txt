
============================================================
- exec time: 2025-06-15 10:13:37
- command: ./run_train.sh --parallelism.tensor_parallel_degree=2 --parallelism.pipeline_parallel_degree=2 --parallelism.pipeline_parallel_schedule=Interleaved1F1B --parallelism.pipeline_parallel_num_stages_per_rank=2 --training.seq_len=2048 --training.local_batch_size=8 --job.dump_folder=logs/qwen_1_5b/_tp_I1f1b_sl2_bs8 --profiling.save_traces_folder= --metrics.save_tb_folder=
- config file: /home2/yeonjae/tp_partition/torchtitan/torchtitan/models/llama3/train_configs/qwen_1_5b.toml
- overrides: {'tensor_parallel_degree': 2, 'pipeline_parallel_degree': 2, 'pipeline_parallel_schedule': 'Interleaved1F1B', 'pipeline_parallel_num_stages_per_rank': 2, 'seq_len': 2048, 'local_batch_size': 8}
============================================================

--- STDOUT ---

--- STDERR ---
+ NGPU=4
+ export LOG_RANK=0,1,2,3
+ LOG_RANK=0,1,2,3
+ CONFIG_FILE=/home2/yeonjae/tp_partition/torchtitan/torchtitan/models/llama3/train_configs/qwen_1_5b.toml
+ overrides=
+ '[' 9 -ne 0 ']'
+ overrides='--parallelism.tensor_parallel_degree=2 --parallelism.pipeline_parallel_degree=2 --parallelism.pipeline_parallel_schedule=Interleaved1F1B --parallelism.pipeline_parallel_num_stages_per_rank=2 --training.seq_len=2048 --training.local_batch_size=8 --job.dump_folder=logs/qwen_1_5b/_tp_I1f1b_sl2_bs8 --profiling.save_traces_folder= --metrics.save_tb_folder='
+ TORCHFT_LIGHTHOUSE=http://localhost:29510
+ PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
+ TORCHFT_LIGHTHOUSE=http://localhost:29510
+ torchrun --nproc_per_node=4 --rdzv_backend c10d --rdzv_endpoint=localhost:0 --local-ranks-filter 0,1,2,3 --role rank --tee 3 -m torchtitan.train --job.config_file /home2/yeonjae/tp_partition/torchtitan/torchtitan/models/llama3/train_configs/qwen_1_5b.toml --parallelism.tensor_parallel_degree=2 --parallelism.pipeline_parallel_degree=2 --parallelism.pipeline_parallel_schedule=Interleaved1F1B --parallelism.pipeline_parallel_num_stages_per_rank=2 --training.seq_len=2048 --training.local_batch_size=8 --job.dump_folder=logs/qwen_1_5b/_tp_I1f1b_sl2_bs8 --profiling.save_traces_folder= --metrics.save_tb_folder=
W0615 10:13:38.307000 2117478 torch/distributed/run.py:766] 
W0615 10:13:38.307000 2117478 torch/distributed/run.py:766] *****************************************
W0615 10:13:38.307000 2117478 torch/distributed/run.py:766] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0615 10:13:38.307000 2117478 torch/distributed/run.py:766] *****************************************
[rank2]:[titan] 2025-06-15 10:13:44,117 - root - INFO - Starting job: Llama 3 8B training
[rank0]:[titan] 2025-06-15 10:13:44,195 - root - INFO - Starting job: Llama 3 8B training
[rank3]:[titan] 2025-06-15 10:13:44,170 - root - INFO - Starting job: Llama 3 8B training
[rank1]:[titan] 2025-06-15 10:13:44,234 - root - INFO - Starting job: Llama 3 8B training
[rank0]:[titan] 2025-06-15 10:13:44,368 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank0]:[titan] 2025-06-15 10:13:44,372 - root - INFO - Building 2-D device mesh with ['pp', 'tp'], [2, 2]
[rank0]:[titan] 2025-06-15 10:13:44,375 - root - INFO - [GC] Initial GC collection. 0.00 seconds.
[rank2]:[titan] 2025-06-15 10:13:44,781 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank2]:[titan] 2025-06-15 10:13:44,786 - root - INFO - Building 2-D device mesh with ['pp', 'tp'], [2, 2]
[rank2]:[titan] 2025-06-15 10:13:44,789 - root - INFO - [GC] Initial GC collection. 0.00 seconds.
[rank3]:[titan] 2025-06-15 10:13:45,213 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank3]:[titan] 2025-06-15 10:13:45,218 - root - INFO - Building 2-D device mesh with ['pp', 'tp'], [2, 2]
[rank3]:[titan] 2025-06-15 10:13:45,221 - root - INFO - [GC] Initial GC collection. 0.00 seconds.
[rank1]:[titan] 2025-06-15 10:13:45,224 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank1]:[titan] 2025-06-15 10:13:45,229 - root - INFO - Building 2-D device mesh with ['pp', 'tp'], [2, 2]
[rank1]:[titan] 2025-06-15 10:13:45,233 - root - INFO - [GC] Initial GC collection. 0.00 seconds.
[rank0]:[rank0]:[W615 10:13:45.778875154 Utils.hpp:137] Warning: Environment variable TORCH_NCCL_TRACE_BUFFER_SIZE is deprecated; use TORCH_FR_BUFFER_SIZE instead (function operator())
[rank3]:[rank3]:[W615 10:13:45.778173254 Utils.hpp:137] Warning: Environment variable TORCH_NCCL_TRACE_BUFFER_SIZE is deprecated; use TORCH_FR_BUFFER_SIZE instead (function operator())
[rank1]:[rank1]:[W615 10:13:45.778499554 Utils.hpp:137] Warning: Environment variable TORCH_NCCL_TRACE_BUFFER_SIZE is deprecated; use TORCH_FR_BUFFER_SIZE instead (function operator())
[rank2]:[rank2]:[W615 10:13:45.778826834 Utils.hpp:137] Warning: Environment variable TORCH_NCCL_TRACE_BUFFER_SIZE is deprecated; use TORCH_FR_BUFFER_SIZE instead (function operator())
[rank0]:[titan] 2025-06-15 10:13:45,947 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
[rank0]:[titan] 2025-06-15 10:13:45,947 - root - INFO - Preparing c4 dataset from allenai/c4
[rank3]:[titan] 2025-06-15 10:13:45,937 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
[rank3]:[titan] 2025-06-15 10:13:45,937 - root - INFO - Preparing c4 dataset from allenai/c4
[rank1]:[titan] 2025-06-15 10:13:45,948 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
[rank1]:[titan] 2025-06-15 10:13:45,948 - root - INFO - Preparing c4 dataset from allenai/c4
[rank2]:[titan] 2025-06-15 10:13:45,947 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
[rank2]:[titan] 2025-06-15 10:13:45,947 - root - INFO - Preparing c4 dataset from allenai/c4
[rank1]:[titan] 2025-06-15 10:14:02,425 - root - INFO - Building llama3 1.5B with TransformerModelArgs(_enforced='This field is used to enforce all fields have defaults.', dim=1536, n_layers=28, n_heads=12, n_kv_heads=2, vocab_size=128256, multiple_of=256, intermediate_size=8960, ffn_dim_multiplier=None, norm_eps=1e-05, rope_theta=500000, max_seq_len=2048, depth_init=True, use_flex_attn=False, attn_mask_type='causal', eos_id=128001)
[rank1]:[titan] 2025-06-15 10:14:02,693 - root - INFO - CUDA capacity: NVIDIA A100-PCIE-40GB with 39.50GiB memory
[rank1]:[titan] 2025-06-15 10:14:02,733 - root - INFO - [34mModel llama3 1.5B [31msize: 1,704,285,696 total parameters[39m
[rank1]:[titan] 2025-06-15 10:14:02,733 - root - INFO - Compiling the loss function with torch.compile
[rank1]:[titan] 2025-06-15 10:14:02,780 - root - INFO - No 'pipeline_parallel_split_points' provided. Here is the auto-generated split, which may be sub-optimal: ['layers.14'].
[rank1]:[titan] 2025-06-15 10:14:02,800 - root - INFO - PP rank 0 is building stage_idx 0 with start_layer None, stop_layer layers.14
[rank1]:[titan] 2025-06-15 10:14:02,855 - root - INFO - Applied Tensor Parallelism to the model
[rank1]:[titan] 2025-06-15 10:14:02,856 - root - INFO - Applied full activation checkpointing to the model
[rank1]:[titan] 2025-06-15 10:14:02,860 - root - INFO - Compiling each TransformerBlock with torch.compile
[rank1]:[titan] 2025-06-15 10:14:02,860 - root - INFO - Using pipeline schedule Interleaved1F1B with 8 microbatches and 2 stages.
[rank1]:[titan] 2025-06-15 10:14:03,133 - root - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank1]:[titan] 2025-06-15 10:14:03,134 - root - INFO - CUDA memory usage for model: 1.61GiB(4.07%)
[rank1]:[titan] 2025-06-15 10:14:03,135 - root - INFO - Trainer is initialized with local batch size 8, global batch size 8, gradient accumulation steps 1, sequence length 2048, total steps 10 (warmup 2).
[rank1]:[titan] 2025-06-15 10:14:03,136 - root - INFO - Training starts at step 1.
[rank1]:[titan] 2025-06-15 10:14:03,136 - root - INFO - Profiling active. Traces will be saved at logs/qwen_1_5b/_tp_I1f1b_sl2_bs8/
[rank3]:[titan] 2025-06-15 10:14:03,519 - root - INFO - Building llama3 1.5B with TransformerModelArgs(_enforced='This field is used to enforce all fields have defaults.', dim=1536, n_layers=28, n_heads=12, n_kv_heads=2, vocab_size=128256, multiple_of=256, intermediate_size=8960, ffn_dim_multiplier=None, norm_eps=1e-05, rope_theta=500000, max_seq_len=2048, depth_init=True, use_flex_attn=False, attn_mask_type='causal', eos_id=128001)
[rank0]:[titan] 2025-06-15 10:14:03,685 - root - INFO - Building llama3 1.5B with TransformerModelArgs(_enforced='This field is used to enforce all fields have defaults.', dim=1536, n_layers=28, n_heads=12, n_kv_heads=2, vocab_size=128256, multiple_of=256, intermediate_size=8960, ffn_dim_multiplier=None, norm_eps=1e-05, rope_theta=500000, max_seq_len=2048, depth_init=True, use_flex_attn=False, attn_mask_type='causal', eos_id=128001)
[rank3]:[titan] 2025-06-15 10:14:03,786 - root - INFO - CUDA capacity: NVIDIA A100-PCIE-40GB with 39.50GiB memory
[rank3]:[titan] 2025-06-15 10:14:03,827 - root - INFO - [34mModel llama3 1.5B [31msize: 1,704,285,696 total parameters[39m
[rank3]:[titan] 2025-06-15 10:14:03,827 - root - INFO - Compiling the loss function with torch.compile
[rank0]:[titan] 2025-06-15 10:14:03,952 - root - INFO - CUDA capacity: NVIDIA A100-PCIE-40GB with 39.50GiB memory
[rank3]:[titan] 2025-06-15 10:14:03,877 - root - INFO - No 'pipeline_parallel_split_points' provided. Here is the auto-generated split, which may be sub-optimal: ['layers.14'].
[rank3]:[titan] 2025-06-15 10:14:03,901 - root - INFO - PP rank 1 is building stage_idx 1 with start_layer layers.14, stop_layer None
[rank0]:[titan] 2025-06-15 10:14:03,990 - root - INFO - [34mModel llama3 1.5B [31msize: 1,704,285,696 total parameters[39m
[rank0]:[titan] 2025-06-15 10:14:03,990 - root - INFO - Compiling the loss function with torch.compile
[rank0]:[titan] 2025-06-15 10:14:04,040 - root - INFO - No 'pipeline_parallel_split_points' provided. Here is the auto-generated split, which may be sub-optimal: ['layers.14'].
[rank3]:[titan] 2025-06-15 10:14:03,959 - root - INFO - Applied Tensor Parallelism to the model
[rank3]:[titan] 2025-06-15 10:14:03,960 - root - INFO - Applied full activation checkpointing to the model
[rank3]:[titan] 2025-06-15 10:14:03,965 - root - INFO - Compiling each TransformerBlock with torch.compile
[rank3]:[titan] 2025-06-15 10:14:03,965 - root - INFO - Using pipeline schedule Interleaved1F1B with 8 microbatches and 2 stages.
[rank0]:[titan] 2025-06-15 10:14:04,060 - root - INFO - PP rank 0 is building stage_idx 0 with start_layer None, stop_layer layers.14
[rank0]:[titan] 2025-06-15 10:14:04,115 - root - INFO - Applied Tensor Parallelism to the model
[rank0]:[titan] 2025-06-15 10:14:04,115 - root - INFO - Applied full activation checkpointing to the model
[rank0]:[titan] 2025-06-15 10:14:04,120 - root - INFO - Compiling each TransformerBlock with torch.compile
[rank0]:[titan] 2025-06-15 10:14:04,120 - root - INFO - Using pipeline schedule Interleaved1F1B with 8 microbatches and 2 stages.
[rank3]:[titan] 2025-06-15 10:14:04,241 - root - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank3]:[titan] 2025-06-15 10:14:04,241 - root - INFO - CUDA memory usage for model: 1.61GiB(4.07%)
[rank3]:[titan] 2025-06-15 10:14:04,243 - root - INFO - Trainer is initialized with local batch size 8, global batch size 8, gradient accumulation steps 1, sequence length 2048, total steps 10 (warmup 2).
[rank3]:[titan] 2025-06-15 10:14:04,243 - root - INFO - Training starts at step 1.
[rank3]:[titan] 2025-06-15 10:14:04,243 - root - INFO - Profiling active. Traces will be saved at logs/qwen_1_5b/_tp_I1f1b_sl2_bs8/
[rank0]:[titan] 2025-06-15 10:14:04,389 - root - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank0]:[titan] 2025-06-15 10:14:04,390 - root - INFO - CUDA memory usage for model: 1.61GiB(4.07%)
[rank0]:[titan] 2025-06-15 10:14:04,391 - root - INFO - Trainer is initialized with local batch size 8, global batch size 8, gradient accumulation steps 1, sequence length 2048, total steps 10 (warmup 2).
[rank0]:[titan] 2025-06-15 10:14:04,392 - root - INFO - Training starts at step 1.
[rank0]:[titan] 2025-06-15 10:14:04,392 - root - INFO - Profiling active. Traces will be saved at logs/qwen_1_5b/_tp_I1f1b_sl2_bs8/
[rank2]:[titan] 2025-06-15 10:14:06,115 - root - INFO - Building llama3 1.5B with TransformerModelArgs(_enforced='This field is used to enforce all fields have defaults.', dim=1536, n_layers=28, n_heads=12, n_kv_heads=2, vocab_size=128256, multiple_of=256, intermediate_size=8960, ffn_dim_multiplier=None, norm_eps=1e-05, rope_theta=500000, max_seq_len=2048, depth_init=True, use_flex_attn=False, attn_mask_type='causal', eos_id=128001)
[rank2]:[titan] 2025-06-15 10:14:06,382 - root - INFO - TensorBoard logging enabled. Logs will be saved at logs/qwen_1_5b/_tp_I1f1b_sl2_bs8/20250615-1014
[rank2]:[titan] 2025-06-15 10:14:06,383 - root - INFO - CUDA capacity: NVIDIA A100-PCIE-40GB with 39.50GiB memory
[rank2]:[titan] 2025-06-15 10:14:06,423 - root - INFO - [34mModel llama3 1.5B [31msize: 1,704,285,696 total parameters[39m
[rank2]:[titan] 2025-06-15 10:14:06,424 - root - INFO - Compiling the loss function with torch.compile
[rank2]:[titan] 2025-06-15 10:14:06,470 - root - INFO - No 'pipeline_parallel_split_points' provided. Here is the auto-generated split, which may be sub-optimal: ['layers.14'].
[rank2]:[titan] 2025-06-15 10:14:06,490 - root - INFO - PP rank 1 is building stage_idx 1 with start_layer layers.14, stop_layer None
[rank2]:[titan] 2025-06-15 10:14:06,545 - root - INFO - Applied Tensor Parallelism to the model
[rank2]:[titan] 2025-06-15 10:14:06,546 - root - INFO - Applied full activation checkpointing to the model
[rank2]:[titan] 2025-06-15 10:14:06,550 - root - INFO - Compiling each TransformerBlock with torch.compile
[rank2]:[titan] 2025-06-15 10:14:06,550 - root - INFO - Using pipeline schedule Interleaved1F1B with 8 microbatches and 2 stages.
[rank2]:[titan] 2025-06-15 10:14:06,829 - root - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank2]:[titan] 2025-06-15 10:14:06,829 - root - INFO - CUDA memory usage for model: 1.61GiB(4.07%)
[rank2]:[titan] 2025-06-15 10:14:06,831 - root - INFO - Trainer is initialized with local batch size 8, global batch size 8, gradient accumulation steps 1, sequence length 2048, total steps 10 (warmup 2).
[rank2]:[titan] 2025-06-15 10:14:06,831 - root - INFO - Training starts at step 1.
[rank2]:[titan] 2025-06-15 10:14:06,831 - root - INFO - Profiling active. Traces will be saved at logs/qwen_1_5b/_tp_I1f1b_sl2_bs8/
[rank0]:/home2/yeonjae/tp_partition/pytorch/torch/_inductor/lowering.py:1884: UserWarning: Torchinductor does not support code generation for complex operators. Performance may be worse than eager.
[rank0]:  warnings.warn(
[rank1]:/home2/yeonjae/tp_partition/pytorch/torch/_inductor/lowering.py:1884: UserWarning: Torchinductor does not support code generation for complex operators. Performance may be worse than eager.
[rank1]:  warnings.warn(
[rank3]:/home2/yeonjae/tp_partition/pytorch/torch/_inductor/lowering.py:1884: UserWarning: Torchinductor does not support code generation for complex operators. Performance may be worse than eager.
[rank3]:  warnings.warn(
[rank2]:/home2/yeonjae/tp_partition/pytorch/torch/_inductor/lowering.py:1884: UserWarning: Torchinductor does not support code generation for complex operators. Performance may be worse than eager.
[rank2]:  warnings.warn(
[rank0]:[titan] 2025-06-15 10:14:19,455 - root - INFO - [31mstep:  1  [32mloss: -1.0000  [33mmemory:  7.25GiB(18.37%)  [34mtps: 265  [36mtflops: 2.68  [35mmfu: 0.86%[39m
[rank0]:[titan] 2025-06-15 10:14:19,455 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:01:40
[rank3]:[titan] 2025-06-15 10:14:19,410 - root - INFO - [31mstep:  1  [32mloss: 12.2505  [33mmemory:  7.43GiB(18.81%)  [34mtps: 263  [36mtflops: 2.65  [35mmfu: 0.85%[39m
[rank3]:[titan] 2025-06-15 10:14:19,411 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:01:40
[rank2]:[titan] 2025-06-15 10:14:19,410 - root - INFO - [31mstep:  1  [32mloss: 12.2505  [33mmemory:  7.43GiB(18.81%)  [34mtps: 315  [36mtflops: 3.19  [35mmfu: 1.02%[39m
[rank2]:[titan] 2025-06-15 10:14:19,410 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:01:40
[rank1]:[titan] 2025-06-15 10:14:19,456 - root - INFO - [31mstep:  1  [32mloss: -1.0000  [33mmemory:  7.25GiB(18.37%)  [34mtps: 245  [36mtflops: 2.47  [35mmfu: 0.79%[39m
[rank1]:[titan] 2025-06-15 10:14:19,456 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:01:40
[rank0]:[titan] 2025-06-15 10:14:20,658 - root - INFO - [31mstep:  2  [32mloss: -1.0000  [33mmemory:  8.35GiB(21.15%)  [34mtps: 3,406  [36mtflops: 34.40  [35mmfu: 11.03%[39m
[rank3]:[titan] 2025-06-15 10:14:20,684 - root - INFO - [31mstep:  2  [32mloss: 11.3556  [33mmemory: 10.11GiB(25.60%)  [34mtps: 3,218  [36mtflops: 32.50  [35mmfu: 10.42%[39m
[rank2]:[titan] 2025-06-15 10:14:20,683 - root - INFO - [31mstep:  2  [32mloss: 11.3556  [33mmemory: 10.11GiB(25.60%)  [34mtps: 3,219  [36mtflops: 32.52  [35mmfu: 10.42%[39m
[rank1]:[titan] 2025-06-15 10:14:20,659 - root - INFO - [31mstep:  2  [32mloss: -1.0000  [33mmemory:  8.35GiB(21.15%)  [34mtps: 3,406  [36mtflops: 34.40  [35mmfu: 11.03%[39m
[rank3]:[titan] 2025-06-15 10:14:21,850 - root - INFO - [31mstep:  3  [32mloss: 13.1583  [33mmemory: 10.85GiB(27.47%)  [34mtps: 3,514  [36mtflops: 35.49  [35mmfu: 11.38%[39m
[rank0]:[titan] 2025-06-15 10:14:21,862 - root - INFO - [31mstep:  3  [32mloss: -1.0000  [33mmemory:  8.35GiB(21.15%)  [34mtps: 3,402  [36mtflops: 34.37  [35mmfu: 11.02%[39m
[rank2]:[titan] 2025-06-15 10:14:21,850 - root - INFO - [31mstep:  3  [32mloss: 13.1583  [33mmemory: 10.85GiB(27.47%)  [34mtps: 3,513  [36mtflops: 35.48  [35mmfu: 11.37%[39m
[rank1]:[titan] 2025-06-15 10:14:21,862 - root - INFO - [31mstep:  3  [32mloss: -1.0000  [33mmemory:  8.35GiB(21.15%)  [34mtps: 3,405  [36mtflops: 34.40  [35mmfu: 11.02%[39m
[rank3]:[titan] 2025-06-15 10:14:23,047 - root - INFO - [31mstep:  4  [32mloss: 11.8286  [33mmemory: 10.85GiB(27.47%)  [34mtps: 3,422  [36mtflops: 34.56  [35mmfu: 11.08%[39m
[rank0]:[titan] 2025-06-15 10:14:23,057 - root - INFO - [31mstep:  4  [32mloss: -1.0000  [33mmemory:  8.35GiB(21.15%)  [34mtps: 3,429  [36mtflops: 34.63  [35mmfu: 11.10%[39m
[rank2]:[titan] 2025-06-15 10:14:23,048 - root - INFO - [31mstep:  4  [32mloss: 11.8286  [33mmemory: 10.85GiB(27.47%)  [34mtps: 3,423  [36mtflops: 34.57  [35mmfu: 11.08%[39m
[rank1]:[titan] 2025-06-15 10:14:23,058 - root - INFO - [31mstep:  4  [32mloss: -1.0000  [33mmemory:  8.35GiB(21.15%)  [34mtps: 3,427  [36mtflops: 34.61  [35mmfu: 11.09%[39m
[rank0]:[titan] 2025-06-15 10:14:24,258 - root - INFO - [31mstep:  5  [32mloss: -1.0000  [33mmemory:  8.35GiB(21.15%)  [34mtps: 3,412  [36mtflops: 34.46  [35mmfu: 11.05%[39m
[rank3]:[titan] 2025-06-15 10:14:24,249 - root - INFO - [31mstep:  5  [32mloss: 10.5182  [33mmemory: 10.85GiB(27.47%)  [34mtps: 3,412  [36mtflops: 34.46  [35mmfu: 11.05%[39m
[rank2]:[titan] 2025-06-15 10:14:24,249 - root - INFO - [31mstep:  5  [32mloss: 10.5182  [33mmemory: 10.85GiB(27.47%)  [34mtps: 3,413  [36mtflops: 34.48  [35mmfu: 11.05%[39m
[rank1]:[titan] 2025-06-15 10:14:24,258 - root - INFO - [31mstep:  5  [32mloss: -1.0000  [33mmemory:  8.35GiB(21.15%)  [34mtps: 3,413  [36mtflops: 34.47  [35mmfu: 11.05%[39m
[rank0]:[titan] 2025-06-15 10:14:25,456 - root - INFO - [31mstep:  6  [32mloss: -1.0000  [33mmemory:  8.35GiB(21.15%)  [34mtps: 3,420  [36mtflops: 34.54  [35mmfu: 11.07%[39m
[rank3]:[titan] 2025-06-15 10:14:25,442 - root - INFO - [31mstep:  6  [32mloss:  9.9283  [33mmemory: 10.85GiB(27.47%)  [34mtps: 3,457  [36mtflops: 34.92  [35mmfu: 11.19%[39m
[rank2]:[titan] 2025-06-15 10:14:25,441 - root - INFO - [31mstep:  6  [32mloss:  9.9283  [33mmemory: 10.85GiB(27.47%)  [34mtps: 3,438  [36mtflops: 34.73  [35mmfu: 11.13%[39m
[rank1]:[titan] 2025-06-15 10:14:25,456 - root - INFO - [31mstep:  6  [32mloss: -1.0000  [33mmemory:  8.35GiB(21.15%)  [34mtps: 3,421  [36mtflops: 34.56  [35mmfu: 11.08%[39m
[rank0]:[titan] 2025-06-15 10:14:26,739 - root - INFO - [31mstep:  7  [32mloss: -1.0000  [33mmemory:  8.35GiB(21.15%)  [34mtps: 3,196  [36mtflops: 32.28  [35mmfu: 10.35%[39m
[rank3]:[titan] 2025-06-15 10:14:26,726 - root - INFO - [31mstep:  7  [32mloss:  9.6456  [33mmemory: 10.85GiB(27.47%)  [34mtps: 3,191  [36mtflops: 32.23  [35mmfu: 10.33%[39m
[rank2]:[titan] 2025-06-15 10:14:26,726 - root - INFO - [31mstep:  7  [32mloss:  9.6456  [33mmemory: 10.85GiB(27.47%)  [34mtps: 3,189  [36mtflops: 32.22  [35mmfu: 10.33%[39m
[rank1]:[titan] 2025-06-15 10:14:26,738 - root - INFO - [31mstep:  7  [32mloss: -1.0000  [33mmemory:  8.35GiB(21.15%)  [34mtps: 3,197  [36mtflops: 32.29  [35mmfu: 10.35%[39m
[rank0]:[titan] 2025-06-15 10:14:27,960 - root - INFO - [31mstep:  8  [32mloss: -1.0000  [33mmemory:  8.35GiB(21.15%)  [34mtps: 3,355  [36mtflops: 33.89  [35mmfu: 10.86%[39m
[rank3]:[titan] 2025-06-15 10:14:27,938 - root - INFO - [31mstep:  8  [32mloss:  9.4569  [33mmemory: 10.85GiB(27.47%)  [34mtps: 3,383  [36mtflops: 34.17  [35mmfu: 10.95%[39m
[rank2]:[titan] 2025-06-15 10:14:27,938 - root - INFO - [31mstep:  8  [32mloss:  9.4569  [33mmemory: 10.85GiB(27.47%)  [34mtps: 3,383  [36mtflops: 34.17  [35mmfu: 10.95%[39m
[rank1]:[titan] 2025-06-15 10:14:27,961 - root - INFO - [31mstep:  8  [32mloss: -1.0000  [33mmemory:  8.35GiB(21.15%)  [34mtps: 3,350  [36mtflops: 33.84  [35mmfu: 10.84%[39m
[rank3]:[titan] 2025-06-15 10:14:29,163 - root - INFO - [31mstep:  9  [32mloss:  9.1107  [33mmemory: 10.85GiB(27.47%)  [34mtps: 3,346  [36mtflops: 33.79  [35mmfu: 10.83%[39m
[rank0]:[titan] 2025-06-15 10:14:29,174 - root - INFO - [31mstep:  9  [32mloss: -1.0000  [33mmemory:  8.35GiB(21.15%)  [34mtps: 3,373  [36mtflops: 34.07  [35mmfu: 10.92%[39m
[rank2]:[titan] 2025-06-15 10:14:29,163 - root - INFO - [31mstep:  9  [32mloss:  9.1107  [33mmemory: 10.85GiB(27.47%)  [34mtps: 3,347  [36mtflops: 33.80  [35mmfu: 10.83%[39m
[rank1]:[titan] 2025-06-15 10:14:29,174 - root - INFO - [31mstep:  9  [32mloss: -1.0000  [33mmemory:  8.35GiB(21.15%)  [34mtps: 3,378  [36mtflops: 34.12  [35mmfu: 10.93%[39m
[rank3]:[titan] 2025-06-15 10:14:30,442 - root - INFO - [31mstep: 10  [32mloss:  8.8316  [33mmemory: 10.85GiB(27.47%)  [34mtps: 3,203  [36mtflops: 32.36  [35mmfu: 10.37%[39m
[rank0]:[titan] 2025-06-15 10:14:30,461 - root - INFO - [31mstep: 10  [32mloss: -1.0000  [33mmemory:  8.35GiB(21.15%)  [34mtps: 3,184  [36mtflops: 32.16  [35mmfu: 10.31%[39m
[rank2]:[titan] 2025-06-15 10:14:30,442 - root - INFO - [31mstep: 10  [32mloss:  8.8316  [33mmemory: 10.85GiB(27.47%)  [34mtps: 3,204  [36mtflops: 32.36  [35mmfu: 10.37%[39m
[rank1]:[titan] 2025-06-15 10:14:30,462 - root - INFO - [31mstep: 10  [32mloss: -1.0000  [33mmemory:  8.35GiB(21.15%)  [34mtps: 3,181  [36mtflops: 32.13  [35mmfu: 10.30%[39m
[rank2]:[titan] 2025-06-15 10:14:31,459 - root - INFO - Dumping profiler traces at step 10
[rank3]:[titan] 2025-06-15 10:14:31,469 - root - INFO - Dumping profiler traces at step 10
[rank0]:[titan] 2025-06-15 10:14:31,433 - root - INFO - Dumping profiler traces at step 10
[rank1]:[titan] 2025-06-15 10:14:31,453 - root - INFO - Dumping profiler traces at step 10
[rank2]:[titan] 2025-06-15 10:14:31,872 - root - INFO - Finished dumping profiler traces in 0.41 seconds
[rank2]:[titan] 2025-06-15 10:14:31,874 - root - INFO - Training completed
[rank3]:[titan] 2025-06-15 10:14:31,877 - root - INFO - Finished dumping profiler traces in 0.41 seconds
[rank3]:[titan] 2025-06-15 10:14:31,878 - root - INFO - Training completed
[rank0]:[titan] 2025-06-15 10:14:31,821 - root - INFO - Finished dumping profiler traces in 0.39 seconds
[rank0]:[titan] 2025-06-15 10:14:31,821 - root - INFO - Sleeping 2 seconds for other ranks to complete
[rank1]:[titan] 2025-06-15 10:14:31,879 - root - INFO - Finished dumping profiler traces in 0.43 seconds
[rank1]:[titan] 2025-06-15 10:14:31,880 - root - INFO - Training completed
[rank0]:[titan] 2025-06-15 10:14:33,823 - root - INFO - Training completed
[rank3]:[titan] 2025-06-15 10:14:34,280 - root - INFO - Process group destroyed.
[rank2]:[titan] 2025-06-15 10:14:34,358 - root - INFO - Process group destroyed.
[rank0]:[titan] 2025-06-15 10:14:34,786 - root - INFO - Process group destroyed.
[rank1]:[titan] 2025-06-15 10:14:34,791 - root - INFO - Process group destroyed.
