
============================================================
- exec time: 2025-06-15 10:22:37
- command: ./run_train.sh --parallelism.tensor_parallel_degree=2 --parallelism.pipeline_parallel_degree=2 --parallelism.pipeline_parallel_schedule=Interleaved1F1B --parallelism.pipeline_parallel_num_stages_per_rank=2 --training.seq_len=2048 --training.local_batch_size=16 --job.dump_folder=logs/qwen_1_5b/_tp_I1f1b_sl2_bs16 --profiling.save_traces_folder= --metrics.save_tb_folder=
- config file: /home2/yeonjae/tp_partition/torchtitan/torchtitan/models/llama3/train_configs/qwen_1_5b.toml
- overrides: {'tensor_parallel_degree': 2, 'pipeline_parallel_degree': 2, 'pipeline_parallel_schedule': 'Interleaved1F1B', 'pipeline_parallel_num_stages_per_rank': 2, 'seq_len': 2048, 'local_batch_size': 16}
============================================================

--- STDOUT ---

--- STDERR ---
+ NGPU=4
+ export LOG_RANK=0,1,2,3
+ LOG_RANK=0,1,2,3
+ CONFIG_FILE=/home2/yeonjae/tp_partition/torchtitan/torchtitan/models/llama3/train_configs/qwen_1_5b.toml
+ overrides=
+ '[' 9 -ne 0 ']'
+ overrides='--parallelism.tensor_parallel_degree=2 --parallelism.pipeline_parallel_degree=2 --parallelism.pipeline_parallel_schedule=Interleaved1F1B --parallelism.pipeline_parallel_num_stages_per_rank=2 --training.seq_len=2048 --training.local_batch_size=16 --job.dump_folder=logs/qwen_1_5b/_tp_I1f1b_sl2_bs16 --profiling.save_traces_folder= --metrics.save_tb_folder='
+ TORCHFT_LIGHTHOUSE=http://localhost:29510
+ PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
+ TORCHFT_LIGHTHOUSE=http://localhost:29510
+ torchrun --nproc_per_node=4 --rdzv_backend c10d --rdzv_endpoint=localhost:0 --local-ranks-filter 0,1,2,3 --role rank --tee 3 -m torchtitan.train --job.config_file /home2/yeonjae/tp_partition/torchtitan/torchtitan/models/llama3/train_configs/qwen_1_5b.toml --parallelism.tensor_parallel_degree=2 --parallelism.pipeline_parallel_degree=2 --parallelism.pipeline_parallel_schedule=Interleaved1F1B --parallelism.pipeline_parallel_num_stages_per_rank=2 --training.seq_len=2048 --training.local_batch_size=16 --job.dump_folder=logs/qwen_1_5b/_tp_I1f1b_sl2_bs16 --profiling.save_traces_folder= --metrics.save_tb_folder=
W0615 10:22:38.537000 2131766 torch/distributed/run.py:766] 
W0615 10:22:38.537000 2131766 torch/distributed/run.py:766] *****************************************
W0615 10:22:38.537000 2131766 torch/distributed/run.py:766] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0615 10:22:38.537000 2131766 torch/distributed/run.py:766] *****************************************
[rank1]:[titan] 2025-06-15 10:22:44,273 - root - INFO - Starting job: Llama 3 8B training
[rank3]:[titan] 2025-06-15 10:22:44,299 - root - INFO - Starting job: Llama 3 8B training
[rank2]:[titan] 2025-06-15 10:22:44,337 - root - INFO - Starting job: Llama 3 8B training
[rank0]:[titan] 2025-06-15 10:22:44,318 - root - INFO - Starting job: Llama 3 8B training
[rank0]:[titan] 2025-06-15 10:22:44,582 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank0]:[titan] 2025-06-15 10:22:44,590 - root - INFO - Building 2-D device mesh with ['pp', 'tp'], [2, 2]
[rank0]:[titan] 2025-06-15 10:22:44,594 - root - INFO - [GC] Initial GC collection. 0.00 seconds.
[rank1]:[titan] 2025-06-15 10:22:45,081 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank1]:[titan] 2025-06-15 10:22:45,085 - root - INFO - Building 2-D device mesh with ['pp', 'tp'], [2, 2]
[rank1]:[titan] 2025-06-15 10:22:45,088 - root - INFO - [GC] Initial GC collection. 0.00 seconds.
[rank3]:[titan] 2025-06-15 10:22:45,309 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank3]:[titan] 2025-06-15 10:22:45,313 - root - INFO - Building 2-D device mesh with ['pp', 'tp'], [2, 2]
[rank3]:[titan] 2025-06-15 10:22:45,317 - root - INFO - [GC] Initial GC collection. 0.00 seconds.
[rank2]:[titan] 2025-06-15 10:22:45,419 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank2]:[titan] 2025-06-15 10:22:45,424 - root - INFO - Building 2-D device mesh with ['pp', 'tp'], [2, 2]
[rank2]:[titan] 2025-06-15 10:22:45,429 - root - INFO - [GC] Initial GC collection. 0.00 seconds.
[rank1]:[rank1]:[W615 10:22:45.936437723 Utils.hpp:137] Warning: Environment variable TORCH_NCCL_TRACE_BUFFER_SIZE is deprecated; use TORCH_FR_BUFFER_SIZE instead (function operator())
[rank2]:[rank2]:[W615 10:22:45.935897934 Utils.hpp:137] Warning: Environment variable TORCH_NCCL_TRACE_BUFFER_SIZE is deprecated; use TORCH_FR_BUFFER_SIZE instead (function operator())
[rank3]:[rank3]:[W615 10:22:45.936593111 Utils.hpp:137] Warning: Environment variable TORCH_NCCL_TRACE_BUFFER_SIZE is deprecated; use TORCH_FR_BUFFER_SIZE instead (function operator())
[rank0]:[rank0]:[W615 10:22:45.936776477 Utils.hpp:137] Warning: Environment variable TORCH_NCCL_TRACE_BUFFER_SIZE is deprecated; use TORCH_FR_BUFFER_SIZE instead (function operator())
[rank1]:[titan] 2025-06-15 10:22:46,080 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
[rank1]:[titan] 2025-06-15 10:22:46,080 - root - INFO - Preparing c4 dataset from allenai/c4
[rank2]:[titan] 2025-06-15 10:22:46,148 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
[rank2]:[titan] 2025-06-15 10:22:46,148 - root - INFO - Preparing c4 dataset from allenai/c4
[rank0]:[titan] 2025-06-15 10:22:46,109 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
[rank0]:[titan] 2025-06-15 10:22:46,109 - root - INFO - Preparing c4 dataset from allenai/c4
[rank3]:[titan] 2025-06-15 10:22:46,100 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
[rank3]:[titan] 2025-06-15 10:22:46,100 - root - INFO - Preparing c4 dataset from allenai/c4
[rank3]:[titan] 2025-06-15 10:23:02,966 - root - INFO - Building llama3 1.5B with TransformerModelArgs(_enforced='This field is used to enforce all fields have defaults.', dim=1536, n_layers=28, n_heads=12, n_kv_heads=2, vocab_size=128256, multiple_of=256, intermediate_size=8960, ffn_dim_multiplier=None, norm_eps=1e-05, rope_theta=500000, max_seq_len=2048, depth_init=True, use_flex_attn=False, attn_mask_type='causal', eos_id=128001)
[rank3]:[titan] 2025-06-15 10:23:03,232 - root - INFO - CUDA capacity: NVIDIA A100-PCIE-40GB with 39.50GiB memory
[rank3]:[titan] 2025-06-15 10:23:03,270 - root - INFO - [34mModel llama3 1.5B [31msize: 1,704,285,696 total parameters[39m
[rank3]:[titan] 2025-06-15 10:23:03,270 - root - INFO - Compiling the loss function with torch.compile
[rank3]:[titan] 2025-06-15 10:23:03,318 - root - INFO - No 'pipeline_parallel_split_points' provided. Here is the auto-generated split, which may be sub-optimal: ['layers.14'].
[rank3]:[titan] 2025-06-15 10:23:03,339 - root - INFO - PP rank 1 is building stage_idx 1 with start_layer layers.14, stop_layer None
[rank3]:[titan] 2025-06-15 10:23:03,395 - root - INFO - Applied Tensor Parallelism to the model
[rank3]:[titan] 2025-06-15 10:23:03,395 - root - INFO - Applied full activation checkpointing to the model
[rank3]:[titan] 2025-06-15 10:23:03,399 - root - INFO - Compiling each TransformerBlock with torch.compile
[rank3]:[titan] 2025-06-15 10:23:03,399 - root - INFO - Using pipeline schedule Interleaved1F1B with 16 microbatches and 2 stages.
[rank3]:[titan] 2025-06-15 10:23:03,670 - root - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank3]:[titan] 2025-06-15 10:23:03,671 - root - INFO - CUDA memory usage for model: 1.61GiB(4.07%)
[rank3]:[titan] 2025-06-15 10:23:03,673 - root - INFO - Trainer is initialized with local batch size 16, global batch size 16, gradient accumulation steps 1, sequence length 2048, total steps 10 (warmup 2).
[rank3]:[titan] 2025-06-15 10:23:03,673 - root - INFO - Training starts at step 1.
[rank3]:[titan] 2025-06-15 10:23:03,674 - root - INFO - Profiling active. Traces will be saved at logs/qwen_1_5b/_tp_I1f1b_sl2_bs16/
[rank0]:[titan] 2025-06-15 10:23:04,774 - root - INFO - Building llama3 1.5B with TransformerModelArgs(_enforced='This field is used to enforce all fields have defaults.', dim=1536, n_layers=28, n_heads=12, n_kv_heads=2, vocab_size=128256, multiple_of=256, intermediate_size=8960, ffn_dim_multiplier=None, norm_eps=1e-05, rope_theta=500000, max_seq_len=2048, depth_init=True, use_flex_attn=False, attn_mask_type='causal', eos_id=128001)
[rank0]:[titan] 2025-06-15 10:23:05,048 - root - INFO - CUDA capacity: NVIDIA A100-PCIE-40GB with 39.50GiB memory
[rank0]:[titan] 2025-06-15 10:23:05,086 - root - INFO - [34mModel llama3 1.5B [31msize: 1,704,285,696 total parameters[39m
[rank0]:[titan] 2025-06-15 10:23:05,086 - root - INFO - Compiling the loss function with torch.compile
[rank0]:[titan] 2025-06-15 10:23:05,133 - root - INFO - No 'pipeline_parallel_split_points' provided. Here is the auto-generated split, which may be sub-optimal: ['layers.14'].
[rank0]:[titan] 2025-06-15 10:23:05,153 - root - INFO - PP rank 0 is building stage_idx 0 with start_layer None, stop_layer layers.14
[rank0]:[titan] 2025-06-15 10:23:05,208 - root - INFO - Applied Tensor Parallelism to the model
[rank0]:[titan] 2025-06-15 10:23:05,209 - root - INFO - Applied full activation checkpointing to the model
[rank0]:[titan] 2025-06-15 10:23:05,213 - root - INFO - Compiling each TransformerBlock with torch.compile
[rank0]:[titan] 2025-06-15 10:23:05,213 - root - INFO - Using pipeline schedule Interleaved1F1B with 16 microbatches and 2 stages.
[rank2]:[titan] 2025-06-15 10:23:05,276 - root - INFO - Building llama3 1.5B with TransformerModelArgs(_enforced='This field is used to enforce all fields have defaults.', dim=1536, n_layers=28, n_heads=12, n_kv_heads=2, vocab_size=128256, multiple_of=256, intermediate_size=8960, ffn_dim_multiplier=None, norm_eps=1e-05, rope_theta=500000, max_seq_len=2048, depth_init=True, use_flex_attn=False, attn_mask_type='causal', eos_id=128001)
[rank0]:[titan] 2025-06-15 10:23:05,476 - root - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank0]:[titan] 2025-06-15 10:23:05,477 - root - INFO - CUDA memory usage for model: 1.61GiB(4.07%)
[rank0]:[titan] 2025-06-15 10:23:05,479 - root - INFO - Trainer is initialized with local batch size 16, global batch size 16, gradient accumulation steps 1, sequence length 2048, total steps 10 (warmup 2).
[rank0]:[titan] 2025-06-15 10:23:05,479 - root - INFO - Training starts at step 1.
[rank0]:[titan] 2025-06-15 10:23:05,479 - root - INFO - Profiling active. Traces will be saved at logs/qwen_1_5b/_tp_I1f1b_sl2_bs16/
[rank2]:[titan] 2025-06-15 10:23:05,545 - root - INFO - TensorBoard logging enabled. Logs will be saved at logs/qwen_1_5b/_tp_I1f1b_sl2_bs16/20250615-1023
[rank2]:[titan] 2025-06-15 10:23:05,546 - root - INFO - CUDA capacity: NVIDIA A100-PCIE-40GB with 39.50GiB memory
[rank2]:[titan] 2025-06-15 10:23:05,587 - root - INFO - [34mModel llama3 1.5B [31msize: 1,704,285,696 total parameters[39m
[rank2]:[titan] 2025-06-15 10:23:05,587 - root - INFO - Compiling the loss function with torch.compile
[rank2]:[titan] 2025-06-15 10:23:05,634 - root - INFO - No 'pipeline_parallel_split_points' provided. Here is the auto-generated split, which may be sub-optimal: ['layers.14'].
[rank2]:[titan] 2025-06-15 10:23:05,656 - root - INFO - PP rank 1 is building stage_idx 1 with start_layer layers.14, stop_layer None
[rank2]:[titan] 2025-06-15 10:23:05,712 - root - INFO - Applied Tensor Parallelism to the model
[rank2]:[titan] 2025-06-15 10:23:05,713 - root - INFO - Applied full activation checkpointing to the model
[rank2]:[titan] 2025-06-15 10:23:05,717 - root - INFO - Compiling each TransformerBlock with torch.compile
[rank2]:[titan] 2025-06-15 10:23:05,717 - root - INFO - Using pipeline schedule Interleaved1F1B with 16 microbatches and 2 stages.
[rank2]:[titan] 2025-06-15 10:23:05,987 - root - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank2]:[titan] 2025-06-15 10:23:05,988 - root - INFO - CUDA memory usage for model: 1.61GiB(4.07%)
[rank2]:[titan] 2025-06-15 10:23:05,989 - root - INFO - Trainer is initialized with local batch size 16, global batch size 16, gradient accumulation steps 1, sequence length 2048, total steps 10 (warmup 2).
[rank2]:[titan] 2025-06-15 10:23:05,989 - root - INFO - Training starts at step 1.
[rank2]:[titan] 2025-06-15 10:23:05,989 - root - INFO - Profiling active. Traces will be saved at logs/qwen_1_5b/_tp_I1f1b_sl2_bs16/
[rank1]:[titan] 2025-06-15 10:23:06,872 - root - INFO - Building llama3 1.5B with TransformerModelArgs(_enforced='This field is used to enforce all fields have defaults.', dim=1536, n_layers=28, n_heads=12, n_kv_heads=2, vocab_size=128256, multiple_of=256, intermediate_size=8960, ffn_dim_multiplier=None, norm_eps=1e-05, rope_theta=500000, max_seq_len=2048, depth_init=True, use_flex_attn=False, attn_mask_type='causal', eos_id=128001)
[rank1]:[titan] 2025-06-15 10:23:07,137 - root - INFO - CUDA capacity: NVIDIA A100-PCIE-40GB with 39.50GiB memory
[rank1]:[titan] 2025-06-15 10:23:07,176 - root - INFO - [34mModel llama3 1.5B [31msize: 1,704,285,696 total parameters[39m
[rank1]:[titan] 2025-06-15 10:23:07,176 - root - INFO - Compiling the loss function with torch.compile
[rank1]:[titan] 2025-06-15 10:23:07,235 - root - INFO - No 'pipeline_parallel_split_points' provided. Here is the auto-generated split, which may be sub-optimal: ['layers.14'].
[rank1]:[titan] 2025-06-15 10:23:07,254 - root - INFO - PP rank 0 is building stage_idx 0 with start_layer None, stop_layer layers.14
[rank1]:[titan] 2025-06-15 10:23:07,308 - root - INFO - Applied Tensor Parallelism to the model
[rank1]:[titan] 2025-06-15 10:23:07,308 - root - INFO - Applied full activation checkpointing to the model
[rank1]:[titan] 2025-06-15 10:23:07,312 - root - INFO - Compiling each TransformerBlock with torch.compile
[rank1]:[titan] 2025-06-15 10:23:07,313 - root - INFO - Using pipeline schedule Interleaved1F1B with 16 microbatches and 2 stages.
[rank1]:[titan] 2025-06-15 10:23:07,583 - root - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank1]:[titan] 2025-06-15 10:23:07,583 - root - INFO - CUDA memory usage for model: 1.61GiB(4.07%)
[rank1]:[titan] 2025-06-15 10:23:07,584 - root - INFO - Trainer is initialized with local batch size 16, global batch size 16, gradient accumulation steps 1, sequence length 2048, total steps 10 (warmup 2).
[rank1]:[titan] 2025-06-15 10:23:07,584 - root - INFO - Training starts at step 1.
[rank1]:[titan] 2025-06-15 10:23:07,584 - root - INFO - Profiling active. Traces will be saved at logs/qwen_1_5b/_tp_I1f1b_sl2_bs16/
[rank1]:/home2/yeonjae/tp_partition/pytorch/torch/_inductor/lowering.py:1884: UserWarning: Torchinductor does not support code generation for complex operators. Performance may be worse than eager.
[rank1]:  warnings.warn(
[rank0]:/home2/yeonjae/tp_partition/pytorch/torch/_inductor/lowering.py:1884: UserWarning: Torchinductor does not support code generation for complex operators. Performance may be worse than eager.
[rank0]:  warnings.warn(
[rank2]:/home2/yeonjae/tp_partition/pytorch/torch/_inductor/lowering.py:1884: UserWarning: Torchinductor does not support code generation for complex operators. Performance may be worse than eager.
[rank2]:  warnings.warn(
[rank3]:/home2/yeonjae/tp_partition/pytorch/torch/_inductor/lowering.py:1884: UserWarning: Torchinductor does not support code generation for complex operators. Performance may be worse than eager.
[rank3]:  warnings.warn(
[rank2]:[titan] 2025-06-15 10:23:22,958 - root - INFO - [31mstep:  1  [32mloss: 12.2680  [33mmemory: 11.28GiB(28.55%)  [34mtps: 472  [36mtflops: 4.76  [35mmfu: 1.53%[39m
[rank2]:[titan] 2025-06-15 10:23:22,959 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:01:40
[rank3]:[titan] 2025-06-15 10:23:22,957 - root - INFO - [31mstep:  1  [32mloss: 12.2680  [33mmemory: 11.28GiB(28.55%)  [34mtps: 416  [36mtflops: 4.20  [35mmfu: 1.35%[39m
[rank3]:[titan] 2025-06-15 10:23:22,957 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:01:40
[rank1]:[titan] 2025-06-15 10:23:23,032 - root - INFO - [31mstep:  1  [32mloss: -1.0000  [33mmemory:  7.29GiB(18.46%)  [34mtps: 517  [36mtflops: 5.22  [35mmfu: 1.67%[39m
[rank1]:[titan] 2025-06-15 10:23:23,033 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:01:40
[rank0]:[titan] 2025-06-15 10:23:23,033 - root - INFO - [31mstep:  1  [32mloss: -1.0000  [33mmemory:  7.29GiB(18.46%)  [34mtps: 456  [36mtflops: 4.61  [35mmfu: 1.48%[39m
[rank0]:[titan] 2025-06-15 10:23:23,034 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:01:40
[rank1]:[titan] 2025-06-15 10:23:25,230 - root - INFO - [31mstep:  2  [32mloss: -1.0000  [33mmemory:  8.41GiB(21.29%)  [34mtps: 3,728  [36mtflops: 37.66  [35mmfu: 12.07%[39m
[rank0]:[titan] 2025-06-15 10:23:25,229 - root - INFO - [31mstep:  2  [32mloss: -1.0000  [33mmemory:  8.41GiB(21.29%)  [34mtps: 3,732  [36mtflops: 37.70  [35mmfu: 12.08%[39m
[rank2]:[titan] 2025-06-15 10:23:25,280 - root - INFO - [31mstep:  2  [32mloss: 11.5823  [33mmemory: 14.11GiB(35.73%)  [34mtps: 3,529  [36mtflops: 35.65  [35mmfu: 11.43%[39m
[rank3]:[titan] 2025-06-15 10:23:25,279 - root - INFO - [31mstep:  2  [32mloss: 11.5823  [33mmemory: 14.11GiB(35.73%)  [34mtps: 3,528  [36mtflops: 35.63  [35mmfu: 11.42%[39m
[rank2]:[titan] 2025-06-15 10:23:27,441 - root - INFO - [31mstep:  3  [32mloss: 13.6707  [33mmemory: 14.86GiB(37.61%)  [34mtps: 3,796  [36mtflops: 38.34  [35mmfu: 12.29%[39m
[rank3]:[titan] 2025-06-15 10:23:27,439 - root - INFO - [31mstep:  3  [32mloss: 13.6707  [33mmemory: 14.86GiB(37.61%)  [34mtps: 3,794  [36mtflops: 38.32  [35mmfu: 12.28%[39m
[rank1]:[titan] 2025-06-15 10:23:27,453 - root - INFO - [31mstep:  3  [32mloss: -1.0000  [33mmemory:  8.41GiB(21.29%)  [34mtps: 3,686  [36mtflops: 37.24  [35mmfu: 11.93%[39m
[rank0]:[titan] 2025-06-15 10:23:27,452 - root - INFO - [31mstep:  3  [32mloss: -1.0000  [33mmemory:  8.41GiB(21.29%)  [34mtps: 3,686  [36mtflops: 37.23  [35mmfu: 11.93%[39m
[rank3]:[titan] 2025-06-15 10:23:29,640 - root - INFO - [31mstep:  4  [32mloss: 11.5703  [33mmemory: 14.86GiB(37.61%)  [34mtps: 3,724  [36mtflops: 37.62  [35mmfu: 12.06%[39m
[rank1]:[titan] 2025-06-15 10:23:29,651 - root - INFO - [31mstep:  4  [32mloss: -1.0000  [33mmemory:  8.41GiB(21.29%)  [34mtps: 3,728  [36mtflops: 37.65  [35mmfu: 12.07%[39m
[rank2]:[titan] 2025-06-15 10:23:29,640 - root - INFO - [31mstep:  4  [32mloss: 11.5703  [33mmemory: 14.86GiB(37.61%)  [34mtps: 3,727  [36mtflops: 37.65  [35mmfu: 12.07%[39m
[rank0]:[titan] 2025-06-15 10:23:29,650 - root - INFO - [31mstep:  4  [32mloss: -1.0000  [33mmemory:  8.41GiB(21.29%)  [34mtps: 3,727  [36mtflops: 37.65  [35mmfu: 12.07%[39m
[rank3]:[titan] 2025-06-15 10:23:31,828 - root - INFO - [31mstep:  5  [32mloss: 10.1858  [33mmemory: 14.86GiB(37.61%)  [34mtps: 3,745  [36mtflops: 37.83  [35mmfu: 12.12%[39m
[rank1]:[titan] 2025-06-15 10:23:31,841 - root - INFO - [31mstep:  5  [32mloss: -1.0000  [33mmemory:  8.41GiB(21.29%)  [34mtps: 3,741  [36mtflops: 37.79  [35mmfu: 12.11%[39m
[rank2]:[titan] 2025-06-15 10:23:31,829 - root - INFO - [31mstep:  5  [32mloss: 10.1858  [33mmemory: 14.86GiB(37.61%)  [34mtps: 3,746  [36mtflops: 37.83  [35mmfu: 12.13%[39m
[rank0]:[titan] 2025-06-15 10:23:31,842 - root - INFO - [31mstep:  5  [32mloss: -1.0000  [33mmemory:  8.41GiB(21.29%)  [34mtps: 3,738  [36mtflops: 37.76  [35mmfu: 12.10%[39m
[rank3]:[titan] 2025-06-15 10:23:34,024 - root - INFO - [31mstep:  6  [32mloss:  9.9285  [33mmemory: 14.86GiB(37.61%)  [34mtps: 3,733  [36mtflops: 37.70  [35mmfu: 12.08%[39m
[rank1]:[titan] 2025-06-15 10:23:34,034 - root - INFO - [31mstep:  6  [32mloss: -1.0000  [33mmemory:  8.41GiB(21.29%)  [34mtps: 3,736  [36mtflops: 37.73  [35mmfu: 12.09%[39m
[rank2]:[titan] 2025-06-15 10:23:34,024 - root - INFO - [31mstep:  6  [32mloss:  9.9285  [33mmemory: 14.86GiB(37.61%)  [34mtps: 3,734  [36mtflops: 37.72  [35mmfu: 12.09%[39m
[rank0]:[titan] 2025-06-15 10:23:34,035 - root - INFO - [31mstep:  6  [32mloss: -1.0000  [33mmemory:  8.41GiB(21.29%)  [34mtps: 3,738  [36mtflops: 37.75  [35mmfu: 12.10%[39m
[rank3]:[titan] 2025-06-15 10:23:36,296 - root - INFO - [31mstep:  7  [32mloss:  9.6671  [33mmemory: 14.86GiB(37.61%)  [34mtps: 3,607  [36mtflops: 36.43  [35mmfu: 11.68%[39m
[rank2]:[titan] 2025-06-15 10:23:36,296 - root - INFO - [31mstep:  7  [32mloss:  9.6671  [33mmemory: 14.86GiB(37.61%)  [34mtps: 3,607  [36mtflops: 36.43  [35mmfu: 11.68%[39m
[rank1]:[titan] 2025-06-15 10:23:36,307 - root - INFO - [31mstep:  7  [32mloss: -1.0000  [33mmemory:  8.41GiB(21.29%)  [34mtps: 3,605  [36mtflops: 36.42  [35mmfu: 11.67%[39m
[rank0]:[titan] 2025-06-15 10:23:36,308 - root - INFO - [31mstep:  7  [32mloss: -1.0000  [33mmemory:  8.41GiB(21.29%)  [34mtps: 3,605  [36mtflops: 36.41  [35mmfu: 11.67%[39m
[rank3]:[titan] 2025-06-15 10:23:38,504 - root - INFO - [31mstep:  8  [32mloss:  9.4302  [33mmemory: 14.86GiB(37.61%)  [34mtps: 3,711  [36mtflops: 37.48  [35mmfu: 12.01%[39m
[rank2]:[titan] 2025-06-15 10:23:38,504 - root - INFO - [31mstep:  8  [32mloss:  9.4302  [33mmemory: 14.86GiB(37.61%)  [34mtps: 3,711  [36mtflops: 37.49  [35mmfu: 12.02%[39m
[rank1]:[titan] 2025-06-15 10:23:38,516 - root - INFO - [31mstep:  8  [32mloss: -1.0000  [33mmemory:  8.41GiB(21.29%)  [34mtps: 3,710  [36mtflops: 37.47  [35mmfu: 12.01%[39m
[rank0]:[titan] 2025-06-15 10:23:38,515 - root - INFO - [31mstep:  8  [32mloss: -1.0000  [33mmemory:  8.41GiB(21.29%)  [34mtps: 3,712  [36mtflops: 37.50  [35mmfu: 12.02%[39m
[rank2]:[titan] 2025-06-15 10:23:40,706 - root - INFO - [31mstep:  9  [32mloss:  9.1157  [33mmemory: 14.86GiB(37.61%)  [34mtps: 3,722  [36mtflops: 37.60  [35mmfu: 12.05%[39m
[rank3]:[titan] 2025-06-15 10:23:40,706 - root - INFO - [31mstep:  9  [32mloss:  9.1157  [33mmemory: 14.86GiB(37.61%)  [34mtps: 3,722  [36mtflops: 37.59  [35mmfu: 12.05%[39m
[rank1]:[titan] 2025-06-15 10:23:40,720 - root - INFO - [31mstep:  9  [32mloss: -1.0000  [33mmemory:  8.41GiB(21.29%)  [34mtps: 3,718  [36mtflops: 37.55  [35mmfu: 12.04%[39m
[rank0]:[titan] 2025-06-15 10:23:40,720 - root - INFO - [31mstep:  9  [32mloss: -1.0000  [33mmemory:  8.41GiB(21.29%)  [34mtps: 3,715  [36mtflops: 37.53  [35mmfu: 12.03%[39m
[rank2]:[titan] 2025-06-15 10:23:43,241 - root - INFO - [31mstep: 10  [32mloss:  8.9435  [33mmemory: 14.86GiB(37.61%)  [34mtps: 3,234  [36mtflops: 32.66  [35mmfu: 10.47%[39m
[rank3]:[titan] 2025-06-15 10:23:43,241 - root - INFO - [31mstep: 10  [32mloss:  8.9435  [33mmemory: 14.86GiB(37.61%)  [34mtps: 3,233  [36mtflops: 32.66  [35mmfu: 10.47%[39m
[rank1]:[titan] 2025-06-15 10:23:43,260 - root - INFO - [31mstep: 10  [32mloss: -1.0000  [33mmemory:  8.41GiB(21.29%)  [34mtps: 3,225  [36mtflops: 32.57  [35mmfu: 10.44%[39m
[rank0]:[titan] 2025-06-15 10:23:43,260 - root - INFO - [31mstep: 10  [32mloss: -1.0000  [33mmemory:  8.41GiB(21.29%)  [34mtps: 3,227  [36mtflops: 32.59  [35mmfu: 10.45%[39m
[rank1]:[titan] 2025-06-15 10:23:45,170 - root - INFO - Dumping profiler traces at step 10
[rank0]:[titan] 2025-06-15 10:23:45,170 - root - INFO - Dumping profiler traces at step 10
[rank2]:[titan] 2025-06-15 10:23:45,401 - root - INFO - Dumping profiler traces at step 10
[rank3]:[titan] 2025-06-15 10:23:45,428 - root - INFO - Dumping profiler traces at step 10
[rank1]:[titan] 2025-06-15 10:23:45,946 - root - INFO - Finished dumping profiler traces in 0.78 seconds
[rank1]:[titan] 2025-06-15 10:23:45,946 - root - INFO - Training completed
[rank0]:[titan] 2025-06-15 10:23:45,938 - root - INFO - Finished dumping profiler traces in 0.77 seconds
[rank0]:[titan] 2025-06-15 10:23:45,938 - root - INFO - Sleeping 2 seconds for other ranks to complete
[rank2]:[titan] 2025-06-15 10:23:46,223 - root - INFO - Finished dumping profiler traces in 0.82 seconds
[rank2]:[titan] 2025-06-15 10:23:46,226 - root - INFO - Training completed
[rank3]:[titan] 2025-06-15 10:23:46,247 - root - INFO - Finished dumping profiler traces in 0.82 seconds
[rank3]:[titan] 2025-06-15 10:23:46,249 - root - INFO - Training completed
[rank0]:[titan] 2025-06-15 10:23:47,940 - root - INFO - Training completed
[rank2]:[titan] 2025-06-15 10:23:48,413 - root - INFO - Process group destroyed.
[rank1]:[titan] 2025-06-15 10:23:48,724 - root - INFO - Process group destroyed.
[rank3]:[titan] 2025-06-15 10:23:48,725 - root - INFO - Process group destroyed.
[rank0]:[titan] 2025-06-15 10:23:48,911 - root - INFO - Process group destroyed.
