
============================================================
- exec time: 2025-06-15 10:36:50
- command: ./run_train.sh --parallelism.pipeline_parallel_degree=4 --parallelism.tensor_parallel_degree=1 --parallelism.pipeline_parallel_schedule=Interleaved1F1B --parallelism.pipeline_parallel_num_stages_per_rank=2 --training.seq_len=4096 --training.local_batch_size=8 --job.dump_folder=logs/qwen_1_5b/_I1f1b_sl4_bs8 --profiling.save_traces_folder= --metrics.save_tb_folder=
- config file: /home2/yeonjae/tp_partition/torchtitan/torchtitan/models/llama3/train_configs/qwen_1_5b.toml
- overrides: {'pipeline_parallel_degree': 4, 'tensor_parallel_degree': 1, 'pipeline_parallel_schedule': 'Interleaved1F1B', 'pipeline_parallel_num_stages_per_rank': 2, 'seq_len': 4096, 'local_batch_size': 8}
============================================================

--- STDOUT ---

--- STDERR ---
+ NGPU=4
+ export LOG_RANK=0,1,2,3
+ LOG_RANK=0,1,2,3
+ CONFIG_FILE=/home2/yeonjae/tp_partition/torchtitan/torchtitan/models/llama3/train_configs/qwen_1_5b.toml
+ overrides=
+ '[' 9 -ne 0 ']'
+ overrides='--parallelism.pipeline_parallel_degree=4 --parallelism.tensor_parallel_degree=1 --parallelism.pipeline_parallel_schedule=Interleaved1F1B --parallelism.pipeline_parallel_num_stages_per_rank=2 --training.seq_len=4096 --training.local_batch_size=8 --job.dump_folder=logs/qwen_1_5b/_I1f1b_sl4_bs8 --profiling.save_traces_folder= --metrics.save_tb_folder='
+ TORCHFT_LIGHTHOUSE=http://localhost:29510
+ PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
+ TORCHFT_LIGHTHOUSE=http://localhost:29510
+ torchrun --nproc_per_node=4 --rdzv_backend c10d --rdzv_endpoint=localhost:0 --local-ranks-filter 0,1,2,3 --role rank --tee 3 -m torchtitan.train --job.config_file /home2/yeonjae/tp_partition/torchtitan/torchtitan/models/llama3/train_configs/qwen_1_5b.toml --parallelism.pipeline_parallel_degree=4 --parallelism.tensor_parallel_degree=1 --parallelism.pipeline_parallel_schedule=Interleaved1F1B --parallelism.pipeline_parallel_num_stages_per_rank=2 --training.seq_len=4096 --training.local_batch_size=8 --job.dump_folder=logs/qwen_1_5b/_I1f1b_sl4_bs8 --profiling.save_traces_folder= --metrics.save_tb_folder=
W0615 10:36:51.888000 2146636 torch/distributed/run.py:766] 
W0615 10:36:51.888000 2146636 torch/distributed/run.py:766] *****************************************
W0615 10:36:51.888000 2146636 torch/distributed/run.py:766] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0615 10:36:51.888000 2146636 torch/distributed/run.py:766] *****************************************
[rank0]:[titan] 2025-06-15 10:36:57,488 - root - INFO - Starting job: Llama 3 8B training
[rank0]:[titan] 2025-06-15 10:36:57,545 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank0]:[titan] 2025-06-15 10:36:57,549 - root - INFO - Building 1-D device mesh with ['pp'], [4]
[rank0]:[titan] 2025-06-15 10:36:57,550 - root - INFO - [GC] Initial GC collection. 0.00 seconds.
[rank1]:[titan] 2025-06-15 10:36:57,553 - root - INFO - Starting job: Llama 3 8B training
[rank2]:[titan] 2025-06-15 10:36:57,642 - root - INFO - Starting job: Llama 3 8B training
[rank3]:[titan] 2025-06-15 10:36:57,787 - root - INFO - Starting job: Llama 3 8B training
[rank2]:[titan] 2025-06-15 10:36:58,539 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank1]:[titan] 2025-06-15 10:36:58,465 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank1]:[titan] 2025-06-15 10:36:58,469 - root - INFO - Building 1-D device mesh with ['pp'], [4]
[rank1]:[titan] 2025-06-15 10:36:58,470 - root - INFO - [GC] Initial GC collection. 0.00 seconds.
[rank3]:[titan] 2025-06-15 10:36:58,614 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank3]:[titan] 2025-06-15 10:36:58,619 - root - INFO - Building 1-D device mesh with ['pp'], [4]
[rank3]:[titan] 2025-06-15 10:36:58,621 - root - INFO - [GC] Initial GC collection. 0.00 seconds.
[rank2]:[titan] 2025-06-15 10:36:58,561 - root - INFO - Building 1-D device mesh with ['pp'], [4]
[rank2]:[titan] 2025-06-15 10:36:58,565 - root - INFO - [GC] Initial GC collection. 0.00 seconds.
[rank3]:[rank3]:[W615 10:36:59.216619800 Utils.hpp:137] Warning: Environment variable TORCH_NCCL_TRACE_BUFFER_SIZE is deprecated; use TORCH_FR_BUFFER_SIZE instead (function operator())
[rank0]:[rank0]:[W615 10:36:59.231119441 Utils.hpp:137] Warning: Environment variable TORCH_NCCL_TRACE_BUFFER_SIZE is deprecated; use TORCH_FR_BUFFER_SIZE instead (function operator())
[rank2]:[rank2]:[W615 10:36:59.217993281 Utils.hpp:137] Warning: Environment variable TORCH_NCCL_TRACE_BUFFER_SIZE is deprecated; use TORCH_FR_BUFFER_SIZE instead (function operator())
[rank1]:[rank1]:[W615 10:36:59.217807412 Utils.hpp:137] Warning: Environment variable TORCH_NCCL_TRACE_BUFFER_SIZE is deprecated; use TORCH_FR_BUFFER_SIZE instead (function operator())
[rank1]:[titan] 2025-06-15 10:36:59,346 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
[rank1]:[titan] 2025-06-15 10:36:59,346 - root - INFO - Preparing c4 dataset from allenai/c4
[rank3]:[titan] 2025-06-15 10:36:59,365 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
[rank3]:[titan] 2025-06-15 10:36:59,365 - root - INFO - Preparing c4 dataset from allenai/c4
[rank0]:[titan] 2025-06-15 10:36:59,370 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
[rank0]:[titan] 2025-06-15 10:36:59,370 - root - INFO - Preparing c4 dataset from allenai/c4
[rank2]:[titan] 2025-06-15 10:36:59,368 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
[rank2]:[titan] 2025-06-15 10:36:59,368 - root - INFO - Preparing c4 dataset from allenai/c4
[rank2]:[titan] 2025-06-15 10:37:17,068 - root - INFO - Building llama3 1.5B with TransformerModelArgs(_enforced='This field is used to enforce all fields have defaults.', dim=1536, n_layers=28, n_heads=12, n_kv_heads=2, vocab_size=128256, multiple_of=256, intermediate_size=8960, ffn_dim_multiplier=None, norm_eps=1e-05, rope_theta=500000, max_seq_len=4096, depth_init=True, use_flex_attn=False, attn_mask_type='causal', eos_id=128001)
[rank3]:[titan] 2025-06-15 10:37:17,115 - root - INFO - Building llama3 1.5B with TransformerModelArgs(_enforced='This field is used to enforce all fields have defaults.', dim=1536, n_layers=28, n_heads=12, n_kv_heads=2, vocab_size=128256, multiple_of=256, intermediate_size=8960, ffn_dim_multiplier=None, norm_eps=1e-05, rope_theta=500000, max_seq_len=4096, depth_init=True, use_flex_attn=False, attn_mask_type='causal', eos_id=128001)
[rank3]:[titan] 2025-06-15 10:37:17,382 - root - INFO - TensorBoard logging enabled. Logs will be saved at logs/qwen_1_5b/_I1f1b_sl4_bs8/20250615-1037
[rank3]:[titan] 2025-06-15 10:37:17,383 - root - INFO - CUDA capacity: NVIDIA A100-PCIE-40GB with 39.50GiB memory
[rank2]:[titan] 2025-06-15 10:37:17,332 - root - INFO - CUDA capacity: NVIDIA A100-PCIE-40GB with 39.50GiB memory
[rank2]:[titan] 2025-06-15 10:37:17,374 - root - INFO - [34mModel llama3 1.5B [31msize: 1,704,285,696 total parameters[39m
[rank2]:[titan] 2025-06-15 10:37:17,375 - root - INFO - Compiling the loss function with torch.compile
[rank0]:[titan] 2025-06-15 10:37:17,396 - root - INFO - Building llama3 1.5B with TransformerModelArgs(_enforced='This field is used to enforce all fields have defaults.', dim=1536, n_layers=28, n_heads=12, n_kv_heads=2, vocab_size=128256, multiple_of=256, intermediate_size=8960, ffn_dim_multiplier=None, norm_eps=1e-05, rope_theta=500000, max_seq_len=4096, depth_init=True, use_flex_attn=False, attn_mask_type='causal', eos_id=128001)
[rank3]:[titan] 2025-06-15 10:37:17,423 - root - INFO - [34mModel llama3 1.5B [31msize: 1,704,285,696 total parameters[39m
[rank3]:[titan] 2025-06-15 10:37:17,423 - root - INFO - Compiling the loss function with torch.compile
[rank3]:[titan] 2025-06-15 10:37:17,470 - root - INFO - No 'pipeline_parallel_split_points' provided. Here is the auto-generated split, which may be sub-optimal: ['layers.6', 'layers.14', 'layers.22'].
[rank2]:[titan] 2025-06-15 10:37:17,422 - root - INFO - No 'pipeline_parallel_split_points' provided. Here is the auto-generated split, which may be sub-optimal: ['layers.6', 'layers.14', 'layers.22'].
[rank2]:[titan] 2025-06-15 10:37:17,445 - root - INFO - PP rank 2 is building stage_idx 2 with start_layer layers.14, stop_layer layers.22
[rank2]:[titan] 2025-06-15 10:37:17,445 - root - INFO - Applied full activation checkpointing to the model
[rank2]:[titan] 2025-06-15 10:37:17,447 - root - INFO - Compiling each TransformerBlock with torch.compile
[rank2]:[titan] 2025-06-15 10:37:17,448 - root - INFO - Using pipeline schedule Interleaved1F1B with 8 microbatches and 4 stages.
[rank3]:[titan] 2025-06-15 10:37:17,490 - root - INFO - PP rank 3 is building stage_idx 3 with start_layer layers.22, stop_layer None
[rank3]:[titan] 2025-06-15 10:37:17,491 - root - INFO - Applied full activation checkpointing to the model
[rank3]:[titan] 2025-06-15 10:37:17,492 - root - INFO - Compiling each TransformerBlock with torch.compile
[rank3]:[titan] 2025-06-15 10:37:17,493 - root - INFO - Using pipeline schedule Interleaved1F1B with 8 microbatches and 4 stages.
[rank0]:[titan] 2025-06-15 10:37:17,661 - root - INFO - CUDA capacity: NVIDIA A100-PCIE-40GB with 39.50GiB memory
[rank0]:[titan] 2025-06-15 10:37:17,703 - root - INFO - [34mModel llama3 1.5B [31msize: 1,704,285,696 total parameters[39m
[rank0]:[titan] 2025-06-15 10:37:17,703 - root - INFO - Compiling the loss function with torch.compile
[rank0]:[titan] 2025-06-15 10:37:17,750 - root - INFO - No 'pipeline_parallel_split_points' provided. Here is the auto-generated split, which may be sub-optimal: ['layers.6', 'layers.14', 'layers.22'].
[rank0]:[titan] 2025-06-15 10:37:17,770 - root - INFO - PP rank 0 is building stage_idx 0 with start_layer None, stop_layer layers.6
[rank0]:[titan] 2025-06-15 10:37:17,770 - root - INFO - Applied full activation checkpointing to the model
[rank0]:[titan] 2025-06-15 10:37:17,772 - root - INFO - Compiling each TransformerBlock with torch.compile
[rank0]:[titan] 2025-06-15 10:37:17,772 - root - INFO - Using pipeline schedule Interleaved1F1B with 8 microbatches and 4 stages.
[rank3]:[titan] 2025-06-15 10:37:17,691 - root - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank3]:[titan] 2025-06-15 10:37:17,691 - root - INFO - CUDA memory usage for model: 1.80GiB(4.56%)
[rank3]:[titan] 2025-06-15 10:37:17,692 - root - INFO - Trainer is initialized with local batch size 8, global batch size 8, gradient accumulation steps 1, sequence length 4096, total steps 10 (warmup 2).
[rank3]:[titan] 2025-06-15 10:37:17,692 - root - INFO - Training starts at step 1.
[rank3]:[titan] 2025-06-15 10:37:17,692 - root - INFO - Profiling active. Traces will be saved at logs/qwen_1_5b/_I1f1b_sl4_bs8/
[rank2]:[titan] 2025-06-15 10:37:17,697 - root - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank2]:[titan] 2025-06-15 10:37:17,698 - root - INFO - CUDA memory usage for model: 1.41GiB(3.57%)
[rank2]:[titan] 2025-06-15 10:37:17,699 - root - INFO - Trainer is initialized with local batch size 8, global batch size 8, gradient accumulation steps 1, sequence length 4096, total steps 10 (warmup 2).
[rank2]:[titan] 2025-06-15 10:37:17,699 - root - INFO - Training starts at step 1.
[rank2]:[titan] 2025-06-15 10:37:17,699 - root - INFO - Profiling active. Traces will be saved at logs/qwen_1_5b/_I1f1b_sl4_bs8/
[rank0]:[titan] 2025-06-15 10:37:17,940 - root - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank0]:[titan] 2025-06-15 10:37:17,940 - root - INFO - CUDA memory usage for model: 1.80GiB(4.56%)
[rank0]:[titan] 2025-06-15 10:37:17,940 - root - INFO - Trainer is initialized with local batch size 8, global batch size 8, gradient accumulation steps 1, sequence length 4096, total steps 10 (warmup 2).
[rank0]:[titan] 2025-06-15 10:37:17,941 - root - INFO - Training starts at step 1.
[rank0]:[titan] 2025-06-15 10:37:17,941 - root - INFO - Profiling active. Traces will be saved at logs/qwen_1_5b/_I1f1b_sl4_bs8/
[rank1]:[titan] 2025-06-15 10:37:18,996 - root - INFO - Building llama3 1.5B with TransformerModelArgs(_enforced='This field is used to enforce all fields have defaults.', dim=1536, n_layers=28, n_heads=12, n_kv_heads=2, vocab_size=128256, multiple_of=256, intermediate_size=8960, ffn_dim_multiplier=None, norm_eps=1e-05, rope_theta=500000, max_seq_len=4096, depth_init=True, use_flex_attn=False, attn_mask_type='causal', eos_id=128001)
[rank1]:[titan] 2025-06-15 10:37:19,258 - root - INFO - CUDA capacity: NVIDIA A100-PCIE-40GB with 39.50GiB memory
[rank1]:[titan] 2025-06-15 10:37:19,299 - root - INFO - [34mModel llama3 1.5B [31msize: 1,704,285,696 total parameters[39m
[rank1]:[titan] 2025-06-15 10:37:19,299 - root - INFO - Compiling the loss function with torch.compile
[rank1]:[titan] 2025-06-15 10:37:19,359 - root - INFO - No 'pipeline_parallel_split_points' provided. Here is the auto-generated split, which may be sub-optimal: ['layers.6', 'layers.14', 'layers.22'].
[rank1]:[titan] 2025-06-15 10:37:19,381 - root - INFO - PP rank 1 is building stage_idx 1 with start_layer layers.6, stop_layer layers.14
[rank1]:[titan] 2025-06-15 10:37:19,381 - root - INFO - Applied full activation checkpointing to the model
[rank1]:[titan] 2025-06-15 10:37:19,384 - root - INFO - Compiling each TransformerBlock with torch.compile
[rank1]:[titan] 2025-06-15 10:37:19,384 - root - INFO - Using pipeline schedule Interleaved1F1B with 8 microbatches and 4 stages.
[rank1]:[titan] 2025-06-15 10:37:19,567 - root - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank1]:[titan] 2025-06-15 10:37:19,567 - root - INFO - CUDA memory usage for model: 1.41GiB(3.57%)
[rank1]:[titan] 2025-06-15 10:37:19,568 - root - INFO - Trainer is initialized with local batch size 8, global batch size 8, gradient accumulation steps 1, sequence length 4096, total steps 10 (warmup 2).
[rank1]:[titan] 2025-06-15 10:37:19,568 - root - INFO - Training starts at step 1.
[rank1]:[titan] 2025-06-15 10:37:19,568 - root - INFO - Profiling active. Traces will be saved at logs/qwen_1_5b/_I1f1b_sl4_bs8/
[rank0]:/home2/yeonjae/tp_partition/pytorch/torch/_inductor/lowering.py:1884: UserWarning: Torchinductor does not support code generation for complex operators. Performance may be worse than eager.
[rank0]:  warnings.warn(
[rank1]:/home2/yeonjae/tp_partition/pytorch/torch/_inductor/lowering.py:1884: UserWarning: Torchinductor does not support code generation for complex operators. Performance may be worse than eager.
[rank1]:  warnings.warn(
[rank2]:/home2/yeonjae/tp_partition/pytorch/torch/_inductor/lowering.py:1884: UserWarning: Torchinductor does not support code generation for complex operators. Performance may be worse than eager.
[rank2]:  warnings.warn(
[rank3]:/home2/yeonjae/tp_partition/pytorch/torch/_inductor/lowering.py:1884: UserWarning: Torchinductor does not support code generation for complex operators. Performance may be worse than eager.
[rank3]:  warnings.warn(
[rank2]:[titan] 2025-06-15 10:37:32,684 - root - INFO - [31mstep:  1  [32mloss: -1.0000  [33mmemory:  6.18GiB(15.64%)  [34mtps: 535  [36mtflops: 5.97  [35mmfu: 1.91%[39m
[rank2]:[titan] 2025-06-15 10:37:32,685 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:01:40
[rank0]:[titan] 2025-06-15 10:37:32,696 - root - INFO - [31mstep:  1  [32mloss: -1.0000  [33mmemory:  7.37GiB(18.65%)  [34mtps: 546  [36mtflops: 6.10  [35mmfu: 1.95%[39m
[rank0]:[titan] 2025-06-15 10:37:32,697 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:01:40
[rank3]:[titan] 2025-06-15 10:37:32,657 - root - INFO - [31mstep:  1  [32mloss: 12.2528  [33mmemory: 19.48GiB(49.31%)  [34mtps: 538  [36mtflops: 6.00  [35mmfu: 1.92%[39m
[rank3]:[titan] 2025-06-15 10:37:32,658 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:01:40
[rank1]:[titan] 2025-06-15 10:37:32,680 - root - INFO - [31mstep:  1  [32mloss: -1.0000  [33mmemory:  6.20GiB(15.69%)  [34mtps: 612  [36mtflops: 6.83  [35mmfu: 2.19%[39m
[rank1]:[titan] 2025-06-15 10:37:32,680 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:01:40
[rank1]:[titan] 2025-06-15 10:37:34,221 - root - INFO - [31mstep:  2  [32mloss: -1.0000  [33mmemory:  7.68GiB(19.44%)  [34mtps: 5,315  [36mtflops: 59.31  [35mmfu: 19.01%[39m
[rank2]:[titan] 2025-06-15 10:37:34,222 - root - INFO - [31mstep:  2  [32mloss: -1.0000  [33mmemory:  7.35GiB(18.60%)  [34mtps: 5,331  [36mtflops: 59.48  [35mmfu: 19.07%[39m
[rank3]:[titan] 2025-06-15 10:37:34,226 - root - INFO - [31mstep:  2  [32mloss: 11.5964  [33mmemory: 22.97GiB(58.16%)  [34mtps: 5,226  [36mtflops: 58.31  [35mmfu: 18.69%[39m
[rank0]:[titan] 2025-06-15 10:37:34,225 - root - INFO - [31mstep:  2  [32mloss: -1.0000  [33mmemory:  9.32GiB(23.60%)  [34mtps: 5,361  [36mtflops: 59.81  [35mmfu: 19.17%[39m
[rank3]:[titan] 2025-06-15 10:37:35,745 - root - INFO - [31mstep:  3  [32mloss: 14.2108  [33mmemory: 24.44GiB(61.87%)  [34mtps: 5,396  [36mtflops: 60.21  [35mmfu: 19.30%[39m
[rank2]:[titan] 2025-06-15 10:37:35,742 - root - INFO - [31mstep:  3  [32mloss: -1.0000  [33mmemory:  7.35GiB(18.60%)  [34mtps: 5,391  [36mtflops: 60.15  [35mmfu: 19.28%[39m
[rank0]:[titan] 2025-06-15 10:37:35,745 - root - INFO - [31mstep:  3  [32mloss: -1.0000  [33mmemory:  9.32GiB(23.60%)  [34mtps: 5,390  [36mtflops: 60.14  [35mmfu: 19.28%[39m
[rank1]:[titan] 2025-06-15 10:37:35,742 - root - INFO - [31mstep:  3  [32mloss: -1.0000  [33mmemory:  7.68GiB(19.44%)  [34mtps: 5,391  [36mtflops: 60.15  [35mmfu: 19.28%[39m
[rank3]:[titan] 2025-06-15 10:37:37,264 - root - INFO - [31mstep:  4  [32mloss: 11.2393  [33mmemory: 24.44GiB(61.87%)  [34mtps: 5,396  [36mtflops: 60.20  [35mmfu: 19.30%[39m
[rank0]:[titan] 2025-06-15 10:37:37,264 - root - INFO - [31mstep:  4  [32mloss: -1.0000  [33mmemory:  9.32GiB(23.60%)  [34mtps: 5,394  [36mtflops: 60.18  [35mmfu: 19.29%[39m
[rank2]:[titan] 2025-06-15 10:37:37,261 - root - INFO - [31mstep:  4  [32mloss: -1.0000  [33mmemory:  7.35GiB(18.60%)  [34mtps: 5,393  [36mtflops: 60.18  [35mmfu: 19.29%[39m
[rank1]:[titan] 2025-06-15 10:37:37,261 - root - INFO - [31mstep:  4  [32mloss: -1.0000  [33mmemory:  7.68GiB(19.44%)  [34mtps: 5,393  [36mtflops: 60.18  [35mmfu: 19.29%[39m
[rank3]:[titan] 2025-06-15 10:37:38,778 - root - INFO - [31mstep:  5  [32mloss: 10.0441  [33mmemory: 24.44GiB(61.87%)  [34mtps: 5,417  [36mtflops: 60.44  [35mmfu: 19.37%[39m
[rank0]:[titan] 2025-06-15 10:37:38,777 - root - INFO - [31mstep:  5  [32mloss: -1.0000  [33mmemory:  9.32GiB(23.60%)  [34mtps: 5,414  [36mtflops: 60.41  [35mmfu: 19.36%[39m
[rank2]:[titan] 2025-06-15 10:37:38,774 - root - INFO - [31mstep:  5  [32mloss: -1.0000  [33mmemory:  7.35GiB(18.60%)  [34mtps: 5,414  [36mtflops: 60.41  [35mmfu: 19.36%[39m
[rank1]:[titan] 2025-06-15 10:37:38,774 - root - INFO - [31mstep:  5  [32mloss: -1.0000  [33mmemory:  7.68GiB(19.44%)  [34mtps: 5,415  [36mtflops: 60.42  [35mmfu: 19.36%[39m
[rank3]:[titan] 2025-06-15 10:37:40,286 - root - INFO - [31mstep:  6  [32mloss:  9.7066  [33mmemory: 24.44GiB(61.87%)  [34mtps: 5,436  [36mtflops: 60.66  [35mmfu: 19.44%[39m
[rank0]:[titan] 2025-06-15 10:37:40,285 - root - INFO - [31mstep:  6  [32mloss: -1.0000  [33mmemory:  9.32GiB(23.60%)  [34mtps: 5,434  [36mtflops: 60.63  [35mmfu: 19.43%[39m
[rank2]:[titan] 2025-06-15 10:37:40,282 - root - INFO - [31mstep:  6  [32mloss: -1.0000  [33mmemory:  7.35GiB(18.60%)  [34mtps: 5,434  [36mtflops: 60.63  [35mmfu: 19.43%[39m
[rank1]:[titan] 2025-06-15 10:37:40,282 - root - INFO - [31mstep:  6  [32mloss: -1.0000  [33mmemory:  7.68GiB(19.44%)  [34mtps: 5,434  [36mtflops: 60.63  [35mmfu: 19.43%[39m
[rank3]:[titan] 2025-06-15 10:37:41,878 - root - INFO - [31mstep:  7  [32mloss:  9.4262  [33mmemory: 24.44GiB(61.87%)  [34mtps: 5,149  [36mtflops: 57.45  [35mmfu: 18.41%[39m
[rank0]:[titan] 2025-06-15 10:37:41,877 - root - INFO - [31mstep:  7  [32mloss: -1.0000  [33mmemory:  9.32GiB(23.60%)  [34mtps: 5,146  [36mtflops: 57.42  [35mmfu: 18.40%[39m
[rank2]:[titan] 2025-06-15 10:37:41,874 - root - INFO - [31mstep:  7  [32mloss: -1.0000  [33mmemory:  7.35GiB(18.60%)  [34mtps: 5,146  [36mtflops: 57.42  [35mmfu: 18.40%[39m
[rank1]:[titan] 2025-06-15 10:37:41,874 - root - INFO - [31mstep:  7  [32mloss: -1.0000  [33mmemory:  7.68GiB(19.44%)  [34mtps: 5,147  [36mtflops: 57.43  [35mmfu: 18.41%[39m
[rank0]:[titan] 2025-06-15 10:37:43,395 - root - INFO - [31mstep:  8  [32mloss: -1.0000  [33mmemory:  9.32GiB(23.60%)  [34mtps: 5,398  [36mtflops: 60.23  [35mmfu: 19.30%[39m
[rank3]:[titan] 2025-06-15 10:37:43,396 - root - INFO - [31mstep:  8  [32mloss:  9.1902  [33mmemory: 24.44GiB(61.87%)  [34mtps: 5,399  [36mtflops: 60.24  [35mmfu: 19.31%[39m
[rank2]:[titan] 2025-06-15 10:37:43,392 - root - INFO - [31mstep:  8  [32mloss: -1.0000  [33mmemory:  7.35GiB(18.60%)  [34mtps: 5,398  [36mtflops: 60.23  [35mmfu: 19.30%[39m
[rank1]:[titan] 2025-06-15 10:37:43,392 - root - INFO - [31mstep:  8  [32mloss: -1.0000  [33mmemory:  7.68GiB(19.44%)  [34mtps: 5,398  [36mtflops: 60.22  [35mmfu: 19.30%[39m
[rank0]:[titan] 2025-06-15 10:37:44,922 - root - INFO - [31mstep:  9  [32mloss: -1.0000  [33mmemory:  9.32GiB(23.60%)  [34mtps: 5,369  [36mtflops: 59.91  [35mmfu: 19.20%[39m
[rank3]:[titan] 2025-06-15 10:37:44,922 - root - INFO - [31mstep:  9  [32mloss:  8.9129  [33mmemory: 24.44GiB(61.87%)  [34mtps: 5,371  [36mtflops: 59.92  [35mmfu: 19.21%[39m
[rank2]:[titan] 2025-06-15 10:37:44,919 - root - INFO - [31mstep:  9  [32mloss: -1.0000  [33mmemory:  7.35GiB(18.60%)  [34mtps: 5,369  [36mtflops: 59.91  [35mmfu: 19.20%[39m
[rank1]:[titan] 2025-06-15 10:37:44,919 - root - INFO - [31mstep:  9  [32mloss: -1.0000  [33mmemory:  7.68GiB(19.44%)  [34mtps: 5,369  [36mtflops: 59.91  [35mmfu: 19.20%[39m
[rank2]:[titan] 2025-06-15 10:37:46,441 - root - INFO - [31mstep: 10  [32mloss: -1.0000  [33mmemory:  7.35GiB(18.60%)  [34mtps: 5,383  [36mtflops: 60.06  [35mmfu: 19.25%[39m
[rank1]:[titan] 2025-06-15 10:37:46,441 - root - INFO - [31mstep: 10  [32mloss: -1.0000  [33mmemory:  7.68GiB(19.44%)  [34mtps: 5,383  [36mtflops: 60.06  [35mmfu: 19.25%[39m
[rank0]:[titan] 2025-06-15 10:37:46,444 - root - INFO - [31mstep: 10  [32mloss: -1.0000  [33mmemory:  9.32GiB(23.60%)  [34mtps: 5,383  [36mtflops: 60.06  [35mmfu: 19.25%[39m
[rank3]:[titan] 2025-06-15 10:37:46,444 - root - INFO - [31mstep: 10  [32mloss:  8.7864  [33mmemory: 24.44GiB(61.87%)  [34mtps: 5,386  [36mtflops: 60.09  [35mmfu: 19.26%[39m
[rank0]:[titan] 2025-06-15 10:37:46,697 - root - INFO - Dumping profiler traces at step 10
[rank3]:[titan] 2025-06-15 10:37:46,704 - root - INFO - Dumping profiler traces at step 10
[rank0]:[titan] 2025-06-15 10:37:46,815 - root - INFO - Finished dumping profiler traces in 0.12 seconds
[rank0]:[titan] 2025-06-15 10:37:46,816 - root - INFO - Sleeping 2 seconds for other ranks to complete
[rank3]:[titan] 2025-06-15 10:37:46,819 - root - INFO - Finished dumping profiler traces in 0.11 seconds
[rank3]:[titan] 2025-06-15 10:37:46,821 - root - INFO - Training completed
[rank2]:[titan] 2025-06-15 10:37:46,775 - root - INFO - Dumping profiler traces at step 10
[rank1]:[titan] 2025-06-15 10:37:46,772 - root - INFO - Dumping profiler traces at step 10
[rank2]:[titan] 2025-06-15 10:37:46,924 - root - INFO - Finished dumping profiler traces in 0.15 seconds
[rank2]:[titan] 2025-06-15 10:37:46,925 - root - INFO - Training completed
[rank1]:[titan] 2025-06-15 10:37:46,921 - root - INFO - Finished dumping profiler traces in 0.15 seconds
[rank1]:[titan] 2025-06-15 10:37:46,921 - root - INFO - Training completed
[rank2]:[titan] 2025-06-15 10:37:47,367 - root - INFO - Process group destroyed.
[rank0]:[titan] 2025-06-15 10:37:48,818 - root - INFO - Training completed
[rank3]:[titan] 2025-06-15 10:37:48,948 - root - INFO - Process group destroyed.
[rank1]:[titan] 2025-06-15 10:37:48,980 - root - INFO - Process group destroyed.
[rank0]:[titan] 2025-06-15 10:37:49,266 - root - INFO - Process group destroyed.
