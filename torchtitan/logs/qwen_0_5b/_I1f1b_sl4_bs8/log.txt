
============================================================
- exec time: 2025-06-15 09:53:01
- command: ./run_train.sh --parallelism.pipeline_parallel_degree=4 --parallelism.tensor_parallel_degree=1 --parallelism.pipeline_parallel_schedule=Interleaved1F1B --parallelism.pipeline_parallel_num_stages_per_rank=2 --training.seq_len=4096 --training.local_batch_size=8 --job.dump_folder=logs/qwen_0_5b/_I1f1b_sl4_bs8 --profiling.save_traces_folder= --metrics.save_tb_folder=
- config file: /home2/yeonjae/tp_partition/torchtitan/torchtitan/models/llama3/train_configs/qwen_0_5b.toml
- overrides: {'pipeline_parallel_degree': 4, 'tensor_parallel_degree': 1, 'pipeline_parallel_schedule': 'Interleaved1F1B', 'pipeline_parallel_num_stages_per_rank': 2, 'seq_len': 4096, 'local_batch_size': 8}
============================================================

--- STDOUT ---

--- STDERR ---
+ NGPU=4
+ export LOG_RANK=0,1,2,3
+ LOG_RANK=0,1,2,3
+ CONFIG_FILE=/home2/yeonjae/tp_partition/torchtitan/torchtitan/models/llama3/train_configs/qwen_0_5b.toml
+ overrides=
+ '[' 9 -ne 0 ']'
+ overrides='--parallelism.pipeline_parallel_degree=4 --parallelism.tensor_parallel_degree=1 --parallelism.pipeline_parallel_schedule=Interleaved1F1B --parallelism.pipeline_parallel_num_stages_per_rank=2 --training.seq_len=4096 --training.local_batch_size=8 --job.dump_folder=logs/qwen_0_5b/_I1f1b_sl4_bs8 --profiling.save_traces_folder= --metrics.save_tb_folder='
+ TORCHFT_LIGHTHOUSE=http://localhost:29510
+ PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
+ TORCHFT_LIGHTHOUSE=http://localhost:29510
+ torchrun --nproc_per_node=4 --rdzv_backend c10d --rdzv_endpoint=localhost:0 --local-ranks-filter 0,1,2,3 --role rank --tee 3 -m torchtitan.train --job.config_file /home2/yeonjae/tp_partition/torchtitan/torchtitan/models/llama3/train_configs/qwen_0_5b.toml --parallelism.pipeline_parallel_degree=4 --parallelism.tensor_parallel_degree=1 --parallelism.pipeline_parallel_schedule=Interleaved1F1B --parallelism.pipeline_parallel_num_stages_per_rank=2 --training.seq_len=4096 --training.local_batch_size=8 --job.dump_folder=logs/qwen_0_5b/_I1f1b_sl4_bs8 --profiling.save_traces_folder= --metrics.save_tb_folder=
W0615 09:53:02.390000 2078514 torch/distributed/run.py:766] 
W0615 09:53:02.390000 2078514 torch/distributed/run.py:766] *****************************************
W0615 09:53:02.390000 2078514 torch/distributed/run.py:766] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0615 09:53:02.390000 2078514 torch/distributed/run.py:766] *****************************************
[rank0]:[titan] 2025-06-15 09:53:08,283 - root - INFO - Starting job: Llama 3 8B training
[rank0]:[titan] 2025-06-15 09:53:08,343 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank0]:[titan] 2025-06-15 09:53:08,347 - root - INFO - Building 1-D device mesh with ['pp'], [4]
[rank0]:[titan] 2025-06-15 09:53:08,349 - root - INFO - [GC] Initial GC collection. 0.00 seconds.
[rank1]:[titan] 2025-06-15 09:53:08,290 - root - INFO - Starting job: Llama 3 8B training
[rank2]:[titan] 2025-06-15 09:53:08,297 - root - INFO - Starting job: Llama 3 8B training
[rank3]:[titan] 2025-06-15 09:53:08,283 - root - INFO - Starting job: Llama 3 8B training
[rank1]:[titan] 2025-06-15 09:53:09,326 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank1]:[titan] 2025-06-15 09:53:09,330 - root - INFO - Building 1-D device mesh with ['pp'], [4]
[rank1]:[titan] 2025-06-15 09:53:09,331 - root - INFO - [GC] Initial GC collection. 0.00 seconds.
[rank3]:[titan] 2025-06-15 09:53:09,317 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank3]:[titan] 2025-06-15 09:53:09,321 - root - INFO - Building 1-D device mesh with ['pp'], [4]
[rank3]:[titan] 2025-06-15 09:53:09,323 - root - INFO - [GC] Initial GC collection. 0.00 seconds.
[rank2]:[titan] 2025-06-15 09:53:09,405 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank2]:[titan] 2025-06-15 09:53:09,409 - root - INFO - Building 1-D device mesh with ['pp'], [4]
[rank2]:[titan] 2025-06-15 09:53:09,411 - root - INFO - [GC] Initial GC collection. 0.00 seconds.
[rank0]:[rank0]:[W615 09:53:09.986617556 Utils.hpp:137] Warning: Environment variable TORCH_NCCL_TRACE_BUFFER_SIZE is deprecated; use TORCH_FR_BUFFER_SIZE instead (function operator())
[rank1]:[rank1]:[W615 09:53:09.001027248 Utils.hpp:137] Warning: Environment variable TORCH_NCCL_TRACE_BUFFER_SIZE is deprecated; use TORCH_FR_BUFFER_SIZE instead (function operator())
[rank3]:[rank3]:[W615 09:53:09.984716546 Utils.hpp:137] Warning: Environment variable TORCH_NCCL_TRACE_BUFFER_SIZE is deprecated; use TORCH_FR_BUFFER_SIZE instead (function operator())
[rank2]:[rank2]:[W615 09:53:09.986625786 Utils.hpp:137] Warning: Environment variable TORCH_NCCL_TRACE_BUFFER_SIZE is deprecated; use TORCH_FR_BUFFER_SIZE instead (function operator())
[rank0]:[titan] 2025-06-15 09:53:10,156 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
[rank0]:[titan] 2025-06-15 09:53:10,156 - root - INFO - Preparing c4 dataset from allenai/c4
[rank3]:[titan] 2025-06-15 09:53:10,136 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
[rank3]:[titan] 2025-06-15 09:53:10,136 - root - INFO - Preparing c4 dataset from allenai/c4
[rank2]:[titan] 2025-06-15 09:53:10,153 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
[rank2]:[titan] 2025-06-15 09:53:10,153 - root - INFO - Preparing c4 dataset from allenai/c4
[rank1]:[titan] 2025-06-15 09:53:10,172 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
[rank1]:[titan] 2025-06-15 09:53:10,172 - root - INFO - Preparing c4 dataset from allenai/c4
[rank0]:[titan] 2025-06-15 09:53:28,606 - root - INFO - Building llama3 0.5B with TransformerModelArgs(_enforced='This field is used to enforce all fields have defaults.', dim=896, n_layers=24, n_heads=14, n_kv_heads=2, vocab_size=128256, multiple_of=256, intermediate_size=4864, ffn_dim_multiplier=None, norm_eps=1e-05, rope_theta=500000, max_seq_len=4096, depth_init=True, use_flex_attn=False, attn_mask_type='causal', eos_id=128001)
[rank0]:[titan] 2025-06-15 09:53:28,837 - root - INFO - CUDA capacity: NVIDIA A100-PCIE-40GB with 39.50GiB memory
[rank0]:[titan] 2025-06-15 09:53:28,877 - root - INFO - [34mModel llama3 0.5B [31msize: 587,705,216 total parameters[39m
[rank0]:[titan] 2025-06-15 09:53:28,877 - root - INFO - Compiling the loss function with torch.compile
[rank0]:[titan] 2025-06-15 09:53:28,924 - root - INFO - No 'pipeline_parallel_split_points' provided. Here is the auto-generated split, which may be sub-optimal: ['layers.5', 'layers.12', 'layers.19'].
[rank0]:[titan] 2025-06-15 09:53:28,942 - root - INFO - PP rank 0 is building stage_idx 0 with start_layer None, stop_layer layers.5
[rank0]:[titan] 2025-06-15 09:53:28,942 - root - INFO - Applied full activation checkpointing to the model
[rank0]:[titan] 2025-06-15 09:53:28,943 - root - INFO - Compiling each TransformerBlock with torch.compile
[rank0]:[titan] 2025-06-15 09:53:28,944 - root - INFO - Using pipeline schedule Interleaved1F1B with 8 microbatches and 4 stages.
[rank0]:[titan] 2025-06-15 09:53:29,104 - root - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank0]:[titan] 2025-06-15 09:53:29,104 - root - INFO - CUDA memory usage for model: 0.71GiB(1.80%)
[rank0]:[titan] 2025-06-15 09:53:29,106 - root - INFO - Trainer is initialized with local batch size 8, global batch size 8, gradient accumulation steps 1, sequence length 4096, total steps 10 (warmup 2).
[rank0]:[titan] 2025-06-15 09:53:29,106 - root - INFO - Training starts at step 1.
[rank0]:[titan] 2025-06-15 09:53:29,106 - root - INFO - Profiling active. Traces will be saved at logs/qwen_0_5b/_I1f1b_sl4_bs8/
[rank2]:[titan] 2025-06-15 09:53:29,289 - root - INFO - Building llama3 0.5B with TransformerModelArgs(_enforced='This field is used to enforce all fields have defaults.', dim=896, n_layers=24, n_heads=14, n_kv_heads=2, vocab_size=128256, multiple_of=256, intermediate_size=4864, ffn_dim_multiplier=None, norm_eps=1e-05, rope_theta=500000, max_seq_len=4096, depth_init=True, use_flex_attn=False, attn_mask_type='causal', eos_id=128001)
[rank2]:[titan] 2025-06-15 09:53:29,508 - root - INFO - CUDA capacity: NVIDIA A100-PCIE-40GB with 39.50GiB memory
[rank2]:[titan] 2025-06-15 09:53:29,546 - root - INFO - [34mModel llama3 0.5B [31msize: 587,705,216 total parameters[39m
[rank2]:[titan] 2025-06-15 09:53:29,546 - root - INFO - Compiling the loss function with torch.compile
[rank2]:[titan] 2025-06-15 09:53:29,592 - root - INFO - No 'pipeline_parallel_split_points' provided. Here is the auto-generated split, which may be sub-optimal: ['layers.5', 'layers.12', 'layers.19'].
[rank2]:[titan] 2025-06-15 09:53:29,609 - root - INFO - PP rank 2 is building stage_idx 2 with start_layer layers.12, stop_layer layers.19
[rank2]:[titan] 2025-06-15 09:53:29,609 - root - INFO - Applied full activation checkpointing to the model
[rank2]:[titan] 2025-06-15 09:53:29,612 - root - INFO - Compiling each TransformerBlock with torch.compile
[rank2]:[titan] 2025-06-15 09:53:29,612 - root - INFO - Using pipeline schedule Interleaved1F1B with 8 microbatches and 4 stages.
[rank2]:[titan] 2025-06-15 09:53:29,770 - root - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank2]:[titan] 2025-06-15 09:53:29,770 - root - INFO - CUDA memory usage for model: 0.40GiB(1.01%)
[rank2]:[titan] 2025-06-15 09:53:29,771 - root - INFO - Trainer is initialized with local batch size 8, global batch size 8, gradient accumulation steps 1, sequence length 4096, total steps 10 (warmup 2).
[rank2]:[titan] 2025-06-15 09:53:29,771 - root - INFO - Training starts at step 1.
[rank2]:[titan] 2025-06-15 09:53:29,771 - root - INFO - Profiling active. Traces will be saved at logs/qwen_0_5b/_I1f1b_sl4_bs8/
[rank1]:[titan] 2025-06-15 09:53:29,810 - root - INFO - Building llama3 0.5B with TransformerModelArgs(_enforced='This field is used to enforce all fields have defaults.', dim=896, n_layers=24, n_heads=14, n_kv_heads=2, vocab_size=128256, multiple_of=256, intermediate_size=4864, ffn_dim_multiplier=None, norm_eps=1e-05, rope_theta=500000, max_seq_len=4096, depth_init=True, use_flex_attn=False, attn_mask_type='causal', eos_id=128001)
[rank1]:[titan] 2025-06-15 09:53:30,032 - root - INFO - CUDA capacity: NVIDIA A100-PCIE-40GB with 39.50GiB memory
[rank1]:[titan] 2025-06-15 09:53:30,070 - root - INFO - [34mModel llama3 0.5B [31msize: 587,705,216 total parameters[39m
[rank1]:[titan] 2025-06-15 09:53:30,070 - root - INFO - Compiling the loss function with torch.compile
[rank1]:[titan] 2025-06-15 09:53:30,116 - root - INFO - No 'pipeline_parallel_split_points' provided. Here is the auto-generated split, which may be sub-optimal: ['layers.5', 'layers.12', 'layers.19'].
[rank1]:[titan] 2025-06-15 09:53:30,134 - root - INFO - PP rank 1 is building stage_idx 1 with start_layer layers.5, stop_layer layers.12
[rank1]:[titan] 2025-06-15 09:53:30,134 - root - INFO - Applied full activation checkpointing to the model
[rank1]:[titan] 2025-06-15 09:53:30,136 - root - INFO - Compiling each TransformerBlock with torch.compile
[rank1]:[titan] 2025-06-15 09:53:30,137 - root - INFO - Using pipeline schedule Interleaved1F1B with 8 microbatches and 4 stages.
[rank1]:[titan] 2025-06-15 09:53:30,295 - root - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank1]:[titan] 2025-06-15 09:53:30,295 - root - INFO - CUDA memory usage for model: 0.40GiB(1.01%)
[rank1]:[titan] 2025-06-15 09:53:30,296 - root - INFO - Trainer is initialized with local batch size 8, global batch size 8, gradient accumulation steps 1, sequence length 4096, total steps 10 (warmup 2).
[rank1]:[titan] 2025-06-15 09:53:30,296 - root - INFO - Training starts at step 1.
[rank1]:[titan] 2025-06-15 09:53:30,296 - root - INFO - Profiling active. Traces will be saved at logs/qwen_0_5b/_I1f1b_sl4_bs8/
[rank3]:[titan] 2025-06-15 09:53:31,114 - root - INFO - Building llama3 0.5B with TransformerModelArgs(_enforced='This field is used to enforce all fields have defaults.', dim=896, n_layers=24, n_heads=14, n_kv_heads=2, vocab_size=128256, multiple_of=256, intermediate_size=4864, ffn_dim_multiplier=None, norm_eps=1e-05, rope_theta=500000, max_seq_len=4096, depth_init=True, use_flex_attn=False, attn_mask_type='causal', eos_id=128001)
[rank3]:[titan] 2025-06-15 09:53:31,344 - root - INFO - TensorBoard logging enabled. Logs will be saved at logs/qwen_0_5b/_I1f1b_sl4_bs8/20250615-0953
[rank3]:[titan] 2025-06-15 09:53:31,345 - root - INFO - CUDA capacity: NVIDIA A100-PCIE-40GB with 39.50GiB memory
[rank3]:[titan] 2025-06-15 09:53:31,381 - root - INFO - [34mModel llama3 0.5B [31msize: 587,705,216 total parameters[39m
[rank3]:[titan] 2025-06-15 09:53:31,381 - root - INFO - Compiling the loss function with torch.compile
[rank3]:[titan] 2025-06-15 09:53:31,427 - root - INFO - No 'pipeline_parallel_split_points' provided. Here is the auto-generated split, which may be sub-optimal: ['layers.5', 'layers.12', 'layers.19'].
[rank3]:[titan] 2025-06-15 09:53:31,444 - root - INFO - PP rank 3 is building stage_idx 3 with start_layer layers.19, stop_layer None
[rank3]:[titan] 2025-06-15 09:53:31,445 - root - INFO - Applied full activation checkpointing to the model
[rank3]:[titan] 2025-06-15 09:53:31,446 - root - INFO - Compiling each TransformerBlock with torch.compile
[rank3]:[titan] 2025-06-15 09:53:31,446 - root - INFO - Using pipeline schedule Interleaved1F1B with 8 microbatches and 4 stages.
[rank3]:[titan] 2025-06-15 09:53:31,608 - root - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank3]:[titan] 2025-06-15 09:53:31,608 - root - INFO - CUDA memory usage for model: 0.71GiB(1.80%)
[rank3]:[titan] 2025-06-15 09:53:31,609 - root - INFO - Trainer is initialized with local batch size 8, global batch size 8, gradient accumulation steps 1, sequence length 4096, total steps 10 (warmup 2).
[rank3]:[titan] 2025-06-15 09:53:31,609 - root - INFO - Training starts at step 1.
[rank3]:[titan] 2025-06-15 09:53:31,609 - root - INFO - Profiling active. Traces will be saved at logs/qwen_0_5b/_I1f1b_sl4_bs8/
[rank0]:/home2/yeonjae/tp_partition/pytorch/torch/_inductor/lowering.py:1884: UserWarning: Torchinductor does not support code generation for complex operators. Performance may be worse than eager.
[rank0]:  warnings.warn(
[rank1]:/home2/yeonjae/tp_partition/pytorch/torch/_inductor/lowering.py:1884: UserWarning: Torchinductor does not support code generation for complex operators. Performance may be worse than eager.
[rank1]:  warnings.warn(
[rank2]:/home2/yeonjae/tp_partition/pytorch/torch/_inductor/lowering.py:1884: UserWarning: Torchinductor does not support code generation for complex operators. Performance may be worse than eager.
[rank2]:  warnings.warn(
[rank3]:/home2/yeonjae/tp_partition/pytorch/torch/_inductor/lowering.py:1884: UserWarning: Torchinductor does not support code generation for complex operators. Performance may be worse than eager.
[rank3]:  warnings.warn(
[rank2]:[titan] 2025-06-15 09:53:43,375 - root - INFO - [31mstep:  1  [32mloss: -1.0000  [33mmemory:  1.94GiB(4.92%)  [34mtps: 592  [36mtflops: 2.31  [35mmfu: 0.74%[39m
[rank2]:[titan] 2025-06-15 09:53:43,375 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:01:40
[rank0]:[titan] 2025-06-15 09:53:43,397 - root - INFO - [31mstep:  1  [32mloss: -1.0000  [33mmemory:  3.03GiB(7.66%)  [34mtps: 564  [36mtflops: 2.20  [35mmfu: 0.70%[39m
[rank0]:[titan] 2025-06-15 09:53:43,397 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:01:40
[rank1]:[titan] 2025-06-15 09:53:43,374 - root - INFO - [31mstep:  1  [32mloss: -1.0000  [33mmemory:  1.92GiB(4.87%)  [34mtps: 616  [36mtflops: 2.40  [35mmfu: 0.77%[39m
[rank1]:[titan] 2025-06-15 09:53:43,374 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:01:40
[rank3]:[titan] 2025-06-15 09:53:43,368 - root - INFO - [31mstep:  1  [32mloss: 12.2356  [33mmemory: 17.23GiB(43.62%)  [34mtps: 683  [36mtflops: 2.66  [35mmfu: 0.85%[39m
[rank3]:[titan] 2025-06-15 09:53:43,369 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:01:40
[rank1]:[titan] 2025-06-15 09:53:44,156 - root - INFO - [31mstep:  2  [32mloss: -1.0000  [33mmemory:  2.69GiB(6.80%)  [34mtps: 10,475  [36mtflops: 40.79  [35mmfu: 13.07%[39m
[rank0]:[titan] 2025-06-15 09:53:44,159 - root - INFO - [31mstep:  2  [32mloss: -1.0000  [33mmemory:  4.01GiB(10.15%)  [34mtps: 10,758  [36mtflops: 41.89  [35mmfu: 13.43%[39m
[rank2]:[titan] 2025-06-15 09:53:44,156 - root - INFO - [31mstep:  2  [32mloss: -1.0000  [33mmemory:  2.51GiB(6.36%)  [34mtps: 10,486  [36mtflops: 40.83  [35mmfu: 13.09%[39m
[rank3]:[titan] 2025-06-15 09:53:44,160 - root - INFO - [31mstep:  2  [32mloss: 11.8330  [33mmemory: 18.46GiB(46.74%)  [34mtps: 10,368  [36mtflops: 40.37  [35mmfu: 12.94%[39m
[rank1]:[titan] 2025-06-15 09:53:44,920 - root - INFO - [31mstep:  3  [32mloss: -1.0000  [33mmemory:  2.69GiB(6.80%)  [34mtps: 10,731  [36mtflops: 41.78  [35mmfu: 13.39%[39m
[rank2]:[titan] 2025-06-15 09:53:44,920 - root - INFO - [31mstep:  3  [32mloss: -1.0000  [33mmemory:  2.51GiB(6.36%)  [34mtps: 10,731  [36mtflops: 41.78  [35mmfu: 13.39%[39m
[rank0]:[titan] 2025-06-15 09:53:44,923 - root - INFO - [31mstep:  3  [32mloss: -1.0000  [33mmemory:  4.01GiB(10.15%)  [34mtps: 10,728  [36mtflops: 41.77  [35mmfu: 13.39%[39m
[rank3]:[titan] 2025-06-15 09:53:44,923 - root - INFO - [31mstep:  3  [32mloss: 10.7885  [33mmemory: 19.30GiB(48.87%)  [34mtps: 10,742  [36mtflops: 41.83  [35mmfu: 13.41%[39m
[rank1]:[titan] 2025-06-15 09:53:45,683 - root - INFO - [31mstep:  4  [32mloss: -1.0000  [33mmemory:  2.69GiB(6.80%)  [34mtps: 10,743  [36mtflops: 41.83  [35mmfu: 13.41%[39m
[rank2]:[titan] 2025-06-15 09:53:45,683 - root - INFO - [31mstep:  4  [32mloss: -1.0000  [33mmemory:  2.51GiB(6.36%)  [34mtps: 10,743  [36mtflops: 41.83  [35mmfu: 13.41%[39m
[rank0]:[titan] 2025-06-15 09:53:45,686 - root - INFO - [31mstep:  4  [32mloss: -1.0000  [33mmemory:  4.01GiB(10.15%)  [34mtps: 10,743  [36mtflops: 41.83  [35mmfu: 13.41%[39m
[rank3]:[titan] 2025-06-15 09:53:45,686 - root - INFO - [31mstep:  4  [32mloss: 10.8587  [33mmemory: 19.30GiB(48.87%)  [34mtps: 10,749  [36mtflops: 41.85  [35mmfu: 13.41%[39m
[rank1]:[titan] 2025-06-15 09:53:46,439 - root - INFO - [31mstep:  5  [32mloss: -1.0000  [33mmemory:  2.69GiB(6.80%)  [34mtps: 10,840  [36mtflops: 42.21  [35mmfu: 13.53%[39m
[rank0]:[titan] 2025-06-15 09:53:46,442 - root - INFO - [31mstep:  5  [32mloss: -1.0000  [33mmemory:  4.01GiB(10.15%)  [34mtps: 10,840  [36mtflops: 42.21  [35mmfu: 13.53%[39m
[rank2]:[titan] 2025-06-15 09:53:46,439 - root - INFO - [31mstep:  5  [32mloss: -1.0000  [33mmemory:  2.51GiB(6.36%)  [34mtps: 10,840  [36mtflops: 42.21  [35mmfu: 13.53%[39m
[rank3]:[titan] 2025-06-15 09:53:46,442 - root - INFO - [31mstep:  5  [32mloss:  9.9002  [33mmemory: 19.30GiB(48.87%)  [34mtps: 10,846  [36mtflops: 42.23  [35mmfu: 13.54%[39m
[rank0]:[titan] 2025-06-15 09:53:47,198 - root - INFO - [31mstep:  6  [32mloss: -1.0000  [33mmemory:  4.01GiB(10.15%)  [34mtps: 10,829  [36mtflops: 42.17  [35mmfu: 13.51%[39m
[rank2]:[titan] 2025-06-15 09:53:47,196 - root - INFO - [31mstep:  6  [32mloss: -1.0000  [33mmemory:  2.51GiB(6.36%)  [34mtps: 10,831  [36mtflops: 42.17  [35mmfu: 13.52%[39m
[rank1]:[titan] 2025-06-15 09:53:47,196 - root - INFO - [31mstep:  6  [32mloss: -1.0000  [33mmemory:  2.69GiB(6.80%)  [34mtps: 10,832  [36mtflops: 42.18  [35mmfu: 13.52%[39m
[rank3]:[titan] 2025-06-15 09:53:47,199 - root - INFO - [31mstep:  6  [32mloss:  9.8404  [33mmemory: 19.30GiB(48.87%)  [34mtps: 10,835  [36mtflops: 42.19  [35mmfu: 13.52%[39m
[rank2]:[titan] 2025-06-15 09:53:48,029 - root - INFO - [31mstep:  7  [32mloss: -1.0000  [33mmemory:  2.51GiB(6.36%)  [34mtps: 9,838  [36mtflops: 38.30  [35mmfu: 12.28%[39m
[rank1]:[titan] 2025-06-15 09:53:48,029 - root - INFO - [31mstep:  7  [32mloss: -1.0000  [33mmemory:  2.69GiB(6.80%)  [34mtps: 9,838  [36mtflops: 38.30  [35mmfu: 12.28%[39m
[rank3]:[titan] 2025-06-15 09:53:48,032 - root - INFO - [31mstep:  7  [32mloss:  9.5932  [33mmemory: 19.30GiB(48.87%)  [34mtps: 9,842  [36mtflops: 38.32  [35mmfu: 12.28%[39m
[rank0]:[titan] 2025-06-15 09:53:48,032 - root - INFO - [31mstep:  7  [32mloss: -1.0000  [33mmemory:  4.01GiB(10.15%)  [34mtps: 9,838  [36mtflops: 38.30  [35mmfu: 12.28%[39m
[rank3]:[titan] 2025-06-15 09:53:48,793 - root - INFO - [31mstep:  8  [32mloss:  9.5255  [33mmemory: 19.30GiB(48.87%)  [34mtps: 10,769  [36mtflops: 41.93  [35mmfu: 13.44%[39m
[rank2]:[titan] 2025-06-15 09:53:48,791 - root - INFO - [31mstep:  8  [32mloss: -1.0000  [33mmemory:  2.51GiB(6.36%)  [34mtps: 10,764  [36mtflops: 41.91  [35mmfu: 13.43%[39m
[rank0]:[titan] 2025-06-15 09:53:48,793 - root - INFO - [31mstep:  8  [32mloss: -1.0000  [33mmemory:  4.01GiB(10.15%)  [34mtps: 10,766  [36mtflops: 41.92  [35mmfu: 13.44%[39m
[rank1]:[titan] 2025-06-15 09:53:48,791 - root - INFO - [31mstep:  8  [32mloss: -1.0000  [33mmemory:  2.69GiB(6.80%)  [34mtps: 10,765  [36mtflops: 41.91  [35mmfu: 13.43%[39m
[rank3]:[titan] 2025-06-15 09:53:49,554 - root - INFO - [31mstep:  9  [32mloss:  9.3811  [33mmemory: 19.30GiB(48.87%)  [34mtps: 10,780  [36mtflops: 41.98  [35mmfu: 13.45%[39m
[rank0]:[titan] 2025-06-15 09:53:49,554 - root - INFO - [31mstep:  9  [32mloss: -1.0000  [33mmemory:  4.01GiB(10.15%)  [34mtps: 10,774  [36mtflops: 41.95  [35mmfu: 13.45%[39m
[rank2]:[titan] 2025-06-15 09:53:49,551 - root - INFO - [31mstep:  9  [32mloss: -1.0000  [33mmemory:  2.51GiB(6.36%)  [34mtps: 10,775  [36mtflops: 41.95  [35mmfu: 13.45%[39m
[rank1]:[titan] 2025-06-15 09:53:49,551 - root - INFO - [31mstep:  9  [32mloss: -1.0000  [33mmemory:  2.69GiB(6.80%)  [34mtps: 10,775  [36mtflops: 41.95  [35mmfu: 13.45%[39m
[rank3]:[titan] 2025-06-15 09:53:50,315 - root - INFO - [31mstep: 10  [32mloss:  9.3558  [33mmemory: 19.30GiB(48.87%)  [34mtps: 10,774  [36mtflops: 41.95  [35mmfu: 13.45%[39m
[rank0]:[titan] 2025-06-15 09:53:50,315 - root - INFO - [31mstep: 10  [32mloss: -1.0000  [33mmemory:  4.01GiB(10.15%)  [34mtps: 10,766  [36mtflops: 41.92  [35mmfu: 13.44%[39m
[rank2]:[titan] 2025-06-15 09:53:50,312 - root - INFO - [31mstep: 10  [32mloss: -1.0000  [33mmemory:  2.51GiB(6.36%)  [34mtps: 10,769  [36mtflops: 41.93  [35mmfu: 13.44%[39m
[rank1]:[titan] 2025-06-15 09:53:50,312 - root - INFO - [31mstep: 10  [32mloss: -1.0000  [33mmemory:  2.69GiB(6.80%)  [34mtps: 10,769  [36mtflops: 41.93  [35mmfu: 13.44%[39m
[rank3]:[titan] 2025-06-15 09:53:50,527 - root - INFO - Dumping profiler traces at step 10
[rank0]:[titan] 2025-06-15 09:53:50,524 - root - INFO - Dumping profiler traces at step 10
[rank3]:[titan] 2025-06-15 09:53:50,628 - root - INFO - Finished dumping profiler traces in 0.10 seconds
[rank3]:[titan] 2025-06-15 09:53:50,630 - root - INFO - Training completed
[rank2]:[titan] 2025-06-15 09:53:50,604 - root - INFO - Dumping profiler traces at step 10
[rank0]:[titan] 2025-06-15 09:53:50,621 - root - INFO - Finished dumping profiler traces in 0.10 seconds
[rank0]:[titan] 2025-06-15 09:53:50,621 - root - INFO - Sleeping 2 seconds for other ranks to complete
[rank1]:[titan] 2025-06-15 09:53:50,609 - root - INFO - Dumping profiler traces at step 10
[rank2]:[titan] 2025-06-15 09:53:50,728 - root - INFO - Finished dumping profiler traces in 0.12 seconds
[rank2]:[titan] 2025-06-15 09:53:50,729 - root - INFO - Training completed
[rank1]:[titan] 2025-06-15 09:53:50,733 - root - INFO - Finished dumping profiler traces in 0.12 seconds
[rank1]:[titan] 2025-06-15 09:53:50,733 - root - INFO - Training completed
[rank2]:[titan] 2025-06-15 09:53:51,205 - root - INFO - Process group destroyed.
[rank0]:[titan] 2025-06-15 09:53:52,623 - root - INFO - Training completed
[rank3]:[titan] 2025-06-15 09:53:52,671 - root - INFO - Process group destroyed.
[rank1]:[titan] 2025-06-15 09:53:52,689 - root - INFO - Process group destroyed.
[rank0]:[titan] 2025-06-15 09:53:53,033 - root - INFO - Process group destroyed.
