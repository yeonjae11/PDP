
============================================================
- exec time: 2025-06-15 09:56:08
- command: ./run_train.sh --parallelism.pipeline_parallel_degree=4 --parallelism.tensor_parallel_degree=1 --parallelism.pipeline_parallel_schedule=1F1B --parallelism.pipeline_parallel_num_stages_per_rank=1 --training.seq_len=2048 --training.local_batch_size=16 --job.dump_folder=logs/qwen_0_5b/_1f1b_sl2_bs16 --profiling.save_traces_folder= --metrics.save_tb_folder=
- config file: /home2/yeonjae/tp_partition/torchtitan/torchtitan/models/llama3/train_configs/qwen_0_5b.toml
- overrides: {'pipeline_parallel_degree': 4, 'tensor_parallel_degree': 1, 'pipeline_parallel_schedule': '1F1B', 'pipeline_parallel_num_stages_per_rank': 1, 'seq_len': 2048, 'local_batch_size': 16}
============================================================

--- STDOUT ---

--- STDERR ---
+ NGPU=4
+ export LOG_RANK=0,1,2,3
+ LOG_RANK=0,1,2,3
+ CONFIG_FILE=/home2/yeonjae/tp_partition/torchtitan/torchtitan/models/llama3/train_configs/qwen_0_5b.toml
+ overrides=
+ '[' 9 -ne 0 ']'
+ overrides='--parallelism.pipeline_parallel_degree=4 --parallelism.tensor_parallel_degree=1 --parallelism.pipeline_parallel_schedule=1F1B --parallelism.pipeline_parallel_num_stages_per_rank=1 --training.seq_len=2048 --training.local_batch_size=16 --job.dump_folder=logs/qwen_0_5b/_1f1b_sl2_bs16 --profiling.save_traces_folder= --metrics.save_tb_folder='
+ TORCHFT_LIGHTHOUSE=http://localhost:29510
+ PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
+ TORCHFT_LIGHTHOUSE=http://localhost:29510
+ torchrun --nproc_per_node=4 --rdzv_backend c10d --rdzv_endpoint=localhost:0 --local-ranks-filter 0,1,2,3 --role rank --tee 3 -m torchtitan.train --job.config_file /home2/yeonjae/tp_partition/torchtitan/torchtitan/models/llama3/train_configs/qwen_0_5b.toml --parallelism.pipeline_parallel_degree=4 --parallelism.tensor_parallel_degree=1 --parallelism.pipeline_parallel_schedule=1F1B --parallelism.pipeline_parallel_num_stages_per_rank=1 --training.seq_len=2048 --training.local_batch_size=16 --job.dump_folder=logs/qwen_0_5b/_1f1b_sl2_bs16 --profiling.save_traces_folder= --metrics.save_tb_folder=
W0615 09:56:09.467000 2085872 torch/distributed/run.py:766] 
W0615 09:56:09.467000 2085872 torch/distributed/run.py:766] *****************************************
W0615 09:56:09.467000 2085872 torch/distributed/run.py:766] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0615 09:56:09.467000 2085872 torch/distributed/run.py:766] *****************************************
[rank0]:[titan] 2025-06-15 09:56:15,189 - root - INFO - Starting job: Llama 3 8B training
[rank1]:[titan] 2025-06-15 09:56:15,209 - root - INFO - Starting job: Llama 3 8B training
[rank0]:[titan] 2025-06-15 09:56:15,246 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank0]:[titan] 2025-06-15 09:56:15,250 - root - INFO - Building 1-D device mesh with ['pp'], [4]
[rank0]:[titan] 2025-06-15 09:56:15,252 - root - INFO - [GC] Initial GC collection. 0.00 seconds.
[rank3]:[titan] 2025-06-15 09:56:15,275 - root - INFO - Starting job: Llama 3 8B training
[rank2]:[titan] 2025-06-15 09:56:15,258 - root - INFO - Starting job: Llama 3 8B training
[rank1]:[titan] 2025-06-15 09:56:16,163 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank1]:[titan] 2025-06-15 09:56:16,167 - root - INFO - Building 1-D device mesh with ['pp'], [4]
[rank1]:[titan] 2025-06-15 09:56:16,168 - root - INFO - [GC] Initial GC collection. 0.00 seconds.
[rank3]:[titan] 2025-06-15 09:56:16,319 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank2]:[titan] 2025-06-15 09:56:16,278 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank2]:[titan] 2025-06-15 09:56:16,302 - root - INFO - Building 1-D device mesh with ['pp'], [4]
[rank2]:[titan] 2025-06-15 09:56:16,307 - root - INFO - [GC] Initial GC collection. 0.00 seconds.
[rank3]:[titan] 2025-06-15 09:56:16,328 - root - INFO - Building 1-D device mesh with ['pp'], [4]
[rank3]:[titan] 2025-06-15 09:56:16,331 - root - INFO - [GC] Initial GC collection. 0.00 seconds.
[rank0]:[rank0]:[W615 09:56:16.900161949 Utils.hpp:137] Warning: Environment variable TORCH_NCCL_TRACE_BUFFER_SIZE is deprecated; use TORCH_FR_BUFFER_SIZE instead (function operator())
[rank3]:[rank3]:[W615 09:56:16.892854578 Utils.hpp:137] Warning: Environment variable TORCH_NCCL_TRACE_BUFFER_SIZE is deprecated; use TORCH_FR_BUFFER_SIZE instead (function operator())
[rank1]:[rank1]:[W615 09:56:16.915290281 Utils.hpp:137] Warning: Environment variable TORCH_NCCL_TRACE_BUFFER_SIZE is deprecated; use TORCH_FR_BUFFER_SIZE instead (function operator())
[rank2]:[rank2]:[W615 09:56:16.915224131 Utils.hpp:137] Warning: Environment variable TORCH_NCCL_TRACE_BUFFER_SIZE is deprecated; use TORCH_FR_BUFFER_SIZE instead (function operator())
[rank0]:[titan] 2025-06-15 09:56:17,107 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
[rank0]:[titan] 2025-06-15 09:56:17,107 - root - INFO - Preparing c4 dataset from allenai/c4
[rank1]:[titan] 2025-06-15 09:56:17,110 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
[rank1]:[titan] 2025-06-15 09:56:17,110 - root - INFO - Preparing c4 dataset from allenai/c4
[rank3]:[titan] 2025-06-15 09:56:17,102 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
[rank3]:[titan] 2025-06-15 09:56:17,102 - root - INFO - Preparing c4 dataset from allenai/c4
[rank2]:[titan] 2025-06-15 09:56:17,098 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
[rank2]:[titan] 2025-06-15 09:56:17,098 - root - INFO - Preparing c4 dataset from allenai/c4
[rank2]:[titan] 2025-06-15 09:56:34,320 - root - INFO - Building llama3 0.5B with TransformerModelArgs(_enforced='This field is used to enforce all fields have defaults.', dim=896, n_layers=24, n_heads=14, n_kv_heads=2, vocab_size=128256, multiple_of=256, intermediate_size=4864, ffn_dim_multiplier=None, norm_eps=1e-05, rope_theta=500000, max_seq_len=2048, depth_init=True, use_flex_attn=False, attn_mask_type='causal', eos_id=128001)
[rank2]:[titan] 2025-06-15 09:56:34,553 - root - INFO - CUDA capacity: NVIDIA A100-PCIE-40GB with 39.50GiB memory
[rank2]:[titan] 2025-06-15 09:56:34,595 - root - INFO - [34mModel llama3 0.5B [31msize: 587,705,216 total parameters[39m
[rank2]:[titan] 2025-06-15 09:56:34,596 - root - INFO - Compiling the loss function with torch.compile
[rank2]:[titan] 2025-06-15 09:56:34,644 - root - INFO - No 'pipeline_parallel_split_points' provided. Here is the auto-generated split, which may be sub-optimal: ['layers.6', 'layers.13', 'layers.19'].
[rank1]:[titan] 2025-06-15 09:56:34,710 - root - INFO - Building llama3 0.5B with TransformerModelArgs(_enforced='This field is used to enforce all fields have defaults.', dim=896, n_layers=24, n_heads=14, n_kv_heads=2, vocab_size=128256, multiple_of=256, intermediate_size=4864, ffn_dim_multiplier=None, norm_eps=1e-05, rope_theta=500000, max_seq_len=2048, depth_init=True, use_flex_attn=False, attn_mask_type='causal', eos_id=128001)
[rank2]:[titan] 2025-06-15 09:56:34,663 - root - INFO - PP rank 2 is building stage_idx 2 with start_layer layers.13, stop_layer layers.19
[rank2]:[titan] 2025-06-15 09:56:34,663 - root - INFO - Applied full activation checkpointing to the model
[rank2]:[titan] 2025-06-15 09:56:34,665 - root - INFO - Compiling each TransformerBlock with torch.compile
[rank2]:[titan] 2025-06-15 09:56:34,665 - root - INFO - Using pipeline schedule 1F1B with 16 microbatches and 4 stages.
[rank2]:[titan] 2025-06-15 09:56:34,831 - root - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank2]:[titan] 2025-06-15 09:56:34,832 - root - INFO - CUDA memory usage for model: 0.34GiB(0.86%)
[rank2]:[titan] 2025-06-15 09:56:34,833 - root - INFO - Trainer is initialized with local batch size 16, global batch size 16, gradient accumulation steps 1, sequence length 2048, total steps 10 (warmup 2).
[rank2]:[titan] 2025-06-15 09:56:34,833 - root - INFO - Training starts at step 1.
[rank2]:[titan] 2025-06-15 09:56:34,833 - root - INFO - Profiling active. Traces will be saved at logs/qwen_0_5b/_1f1b_sl2_bs16/
[rank1]:[titan] 2025-06-15 09:56:34,940 - root - INFO - CUDA capacity: NVIDIA A100-PCIE-40GB with 39.50GiB memory
[rank1]:[titan] 2025-06-15 09:56:34,981 - root - INFO - [34mModel llama3 0.5B [31msize: 587,705,216 total parameters[39m
[rank1]:[titan] 2025-06-15 09:56:34,981 - root - INFO - Compiling the loss function with torch.compile
[rank1]:[titan] 2025-06-15 09:56:35,027 - root - INFO - No 'pipeline_parallel_split_points' provided. Here is the auto-generated split, which may be sub-optimal: ['layers.6', 'layers.13', 'layers.19'].
[rank1]:[titan] 2025-06-15 09:56:35,046 - root - INFO - PP rank 1 is building stage_idx 1 with start_layer layers.6, stop_layer layers.13
[rank1]:[titan] 2025-06-15 09:56:35,046 - root - INFO - Applied full activation checkpointing to the model
[rank1]:[titan] 2025-06-15 09:56:35,048 - root - INFO - Compiling each TransformerBlock with torch.compile
[rank1]:[titan] 2025-06-15 09:56:35,048 - root - INFO - Using pipeline schedule 1F1B with 16 microbatches and 4 stages.
[rank3]:[titan] 2025-06-15 09:56:35,084 - root - INFO - Building llama3 0.5B with TransformerModelArgs(_enforced='This field is used to enforce all fields have defaults.', dim=896, n_layers=24, n_heads=14, n_kv_heads=2, vocab_size=128256, multiple_of=256, intermediate_size=4864, ffn_dim_multiplier=None, norm_eps=1e-05, rope_theta=500000, max_seq_len=2048, depth_init=True, use_flex_attn=False, attn_mask_type='causal', eos_id=128001)
[rank1]:[titan] 2025-06-15 09:56:35,208 - root - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank1]:[titan] 2025-06-15 09:56:35,208 - root - INFO - CUDA memory usage for model: 0.40GiB(1.01%)
[rank1]:[titan] 2025-06-15 09:56:35,209 - root - INFO - Trainer is initialized with local batch size 16, global batch size 16, gradient accumulation steps 1, sequence length 2048, total steps 10 (warmup 2).
[rank1]:[titan] 2025-06-15 09:56:35,209 - root - INFO - Training starts at step 1.
[rank1]:[titan] 2025-06-15 09:56:35,209 - root - INFO - Profiling active. Traces will be saved at logs/qwen_0_5b/_1f1b_sl2_bs16/
[rank0]:[titan] 2025-06-15 09:56:35,313 - root - INFO - Building llama3 0.5B with TransformerModelArgs(_enforced='This field is used to enforce all fields have defaults.', dim=896, n_layers=24, n_heads=14, n_kv_heads=2, vocab_size=128256, multiple_of=256, intermediate_size=4864, ffn_dim_multiplier=None, norm_eps=1e-05, rope_theta=500000, max_seq_len=2048, depth_init=True, use_flex_attn=False, attn_mask_type='causal', eos_id=128001)
[rank3]:[titan] 2025-06-15 09:56:35,316 - root - INFO - TensorBoard logging enabled. Logs will be saved at logs/qwen_0_5b/_1f1b_sl2_bs16/20250615-0956
[rank3]:[titan] 2025-06-15 09:56:35,317 - root - INFO - CUDA capacity: NVIDIA A100-PCIE-40GB with 39.50GiB memory
[rank3]:[titan] 2025-06-15 09:56:35,357 - root - INFO - [34mModel llama3 0.5B [31msize: 587,705,216 total parameters[39m
[rank3]:[titan] 2025-06-15 09:56:35,357 - root - INFO - Compiling the loss function with torch.compile
[rank3]:[titan] 2025-06-15 09:56:35,418 - root - INFO - No 'pipeline_parallel_split_points' provided. Here is the auto-generated split, which may be sub-optimal: ['layers.6', 'layers.13', 'layers.19'].
[rank3]:[titan] 2025-06-15 09:56:35,436 - root - INFO - PP rank 3 is building stage_idx 3 with start_layer layers.19, stop_layer None
[rank3]:[titan] 2025-06-15 09:56:35,437 - root - INFO - Applied full activation checkpointing to the model
[rank3]:[titan] 2025-06-15 09:56:35,438 - root - INFO - Compiling each TransformerBlock with torch.compile
[rank3]:[titan] 2025-06-15 09:56:35,439 - root - INFO - Using pipeline schedule 1F1B with 16 microbatches and 4 stages.
[rank0]:[titan] 2025-06-15 09:56:35,542 - root - INFO - CUDA capacity: NVIDIA A100-PCIE-40GB with 39.50GiB memory
[rank0]:[titan] 2025-06-15 09:56:35,582 - root - INFO - [34mModel llama3 0.5B [31msize: 587,705,216 total parameters[39m
[rank0]:[titan] 2025-06-15 09:56:35,582 - root - INFO - Compiling the loss function with torch.compile
[rank0]:[titan] 2025-06-15 09:56:35,627 - root - INFO - No 'pipeline_parallel_split_points' provided. Here is the auto-generated split, which may be sub-optimal: ['layers.6', 'layers.13', 'layers.19'].
[rank0]:[titan] 2025-06-15 09:56:35,645 - root - INFO - PP rank 0 is building stage_idx 0 with start_layer None, stop_layer layers.6
[rank0]:[titan] 2025-06-15 09:56:35,645 - root - INFO - Applied full activation checkpointing to the model
[rank0]:[titan] 2025-06-15 09:56:35,647 - root - INFO - Compiling each TransformerBlock with torch.compile
[rank0]:[titan] 2025-06-15 09:56:35,647 - root - INFO - Using pipeline schedule 1F1B with 16 microbatches and 4 stages.
[rank3]:[titan] 2025-06-15 09:56:35,595 - root - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank3]:[titan] 2025-06-15 09:56:35,595 - root - INFO - CUDA memory usage for model: 0.71GiB(1.80%)
[rank3]:[titan] 2025-06-15 09:56:35,596 - root - INFO - Trainer is initialized with local batch size 16, global batch size 16, gradient accumulation steps 1, sequence length 2048, total steps 10 (warmup 2).
[rank3]:[titan] 2025-06-15 09:56:35,596 - root - INFO - Training starts at step 1.
[rank3]:[titan] 2025-06-15 09:56:35,596 - root - INFO - Profiling active. Traces will be saved at logs/qwen_0_5b/_1f1b_sl2_bs16/
[rank0]:[titan] 2025-06-15 09:56:35,803 - root - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank0]:[titan] 2025-06-15 09:56:35,803 - root - INFO - CUDA memory usage for model: 0.77GiB(1.95%)
[rank0]:[titan] 2025-06-15 09:56:35,804 - root - INFO - Trainer is initialized with local batch size 16, global batch size 16, gradient accumulation steps 1, sequence length 2048, total steps 10 (warmup 2).
[rank0]:[titan] 2025-06-15 09:56:35,804 - root - INFO - Training starts at step 1.
[rank0]:[titan] 2025-06-15 09:56:35,804 - root - INFO - Profiling active. Traces will be saved at logs/qwen_0_5b/_1f1b_sl2_bs16/
[rank0]:/home2/yeonjae/tp_partition/pytorch/torch/_inductor/lowering.py:1884: UserWarning: Torchinductor does not support code generation for complex operators. Performance may be worse than eager.
[rank0]:  warnings.warn(
[rank1]:/home2/yeonjae/tp_partition/pytorch/torch/_inductor/lowering.py:1884: UserWarning: Torchinductor does not support code generation for complex operators. Performance may be worse than eager.
[rank1]:  warnings.warn(
[rank2]:/home2/yeonjae/tp_partition/pytorch/torch/_inductor/lowering.py:1884: UserWarning: Torchinductor does not support code generation for complex operators. Performance may be worse than eager.
[rank2]:  warnings.warn(
[rank3]:/home2/yeonjae/tp_partition/pytorch/torch/_inductor/lowering.py:1884: UserWarning: Torchinductor does not support code generation for complex operators. Performance may be worse than eager.
[rank3]:  warnings.warn(
[rank1]:[titan] 2025-06-15 09:56:48,965 - root - INFO - [31mstep:  1  [32mloss: -1.0000  [33mmemory:  1.94GiB(4.92%)  [34mtps: 586  [36mtflops: 1.97  [35mmfu: 0.63%[39m
[rank1]:[titan] 2025-06-15 09:56:48,965 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:01:40
[rank0]:[titan] 2025-06-15 09:56:48,973 - root - INFO - [31mstep:  1  [32mloss: -1.0000  [33mmemory:  3.23GiB(8.17%)  [34mtps: 612  [36mtflops: 2.06  [35mmfu: 0.66%[39m
[rank0]:[titan] 2025-06-15 09:56:48,973 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:01:40
[rank3]:[titan] 2025-06-15 09:56:48,942 - root - INFO - [31mstep:  1  [32mloss: 12.2638  [33mmemory: 17.27GiB(43.71%)  [34mtps: 603  [36mtflops: 2.03  [35mmfu: 0.65%[39m
[rank3]:[titan] 2025-06-15 09:56:48,943 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:01:40
[rank2]:[titan] 2025-06-15 09:56:48,964 - root - INFO - [31mstep:  1  [32mloss: -1.0000  [33mmemory:  1.70GiB(4.31%)  [34mtps: 570  [36mtflops: 1.92  [35mmfu: 0.61%[39m
[rank2]:[titan] 2025-06-15 09:56:48,964 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:01:40
[rank1]:[titan] 2025-06-15 09:56:49,566 - root - INFO - [31mstep:  2  [32mloss: -1.0000  [33mmemory:  2.24GiB(5.67%)  [34mtps: 13,642  [36mtflops: 45.91  [35mmfu: 14.71%[39m
[rank0]:[titan] 2025-06-15 09:56:49,569 - root - INFO - [31mstep:  2  [32mloss: -1.0000  [33mmemory:  4.09GiB(10.35%)  [34mtps: 13,758  [36mtflops: 46.30  [35mmfu: 14.84%[39m
[rank3]:[titan] 2025-06-15 09:56:49,569 - root - INFO - [31mstep:  2  [32mloss: 11.8613  [33mmemory: 18.44GiB(46.69%)  [34mtps: 13,089  [36mtflops: 44.05  [35mmfu: 14.12%[39m
[rank2]:[titan] 2025-06-15 09:56:49,565 - root - INFO - [31mstep:  2  [32mloss: -1.0000  [33mmemory:  1.96GiB(4.96%)  [34mtps: 13,618  [36mtflops: 45.83  [35mmfu: 14.69%[39m
[rank1]:[titan] 2025-06-15 09:56:50,166 - root - INFO - [31mstep:  3  [32mloss: -1.0000  [33mmemory:  2.24GiB(5.67%)  [34mtps: 13,678  [36mtflops: 46.03  [35mmfu: 14.75%[39m
[rank0]:[titan] 2025-06-15 09:56:50,168 - root - INFO - [31mstep:  3  [32mloss: -1.0000  [33mmemory:  4.09GiB(10.35%)  [34mtps: 13,676  [36mtflops: 46.02  [35mmfu: 14.75%[39m
[rank3]:[titan] 2025-06-15 09:56:50,168 - root - INFO - [31mstep:  3  [32mloss: 10.8111  [33mmemory: 19.30GiB(48.87%)  [34mtps: 13,698  [36mtflops: 46.10  [35mmfu: 14.77%[39m
[rank2]:[titan] 2025-06-15 09:56:50,165 - root - INFO - [31mstep:  3  [32mloss: -1.0000  [33mmemory:  1.96GiB(4.96%)  [34mtps: 13,673  [36mtflops: 46.01  [35mmfu: 14.75%[39m
[rank1]:[titan] 2025-06-15 09:56:50,771 - root - INFO - [31mstep:  4  [32mloss: -1.0000  [33mmemory:  2.24GiB(5.67%)  [34mtps: 13,547  [36mtflops: 45.59  [35mmfu: 14.61%[39m
[rank0]:[titan] 2025-06-15 09:56:50,774 - root - INFO - [31mstep:  4  [32mloss: -1.0000  [33mmemory:  4.09GiB(10.35%)  [34mtps: 13,541  [36mtflops: 45.57  [35mmfu: 14.60%[39m
[rank3]:[titan] 2025-06-15 09:56:50,774 - root - INFO - [31mstep:  4  [32mloss: 11.0402  [33mmemory: 19.30GiB(48.87%)  [34mtps: 13,548  [36mtflops: 45.59  [35mmfu: 14.61%[39m
[rank2]:[titan] 2025-06-15 09:56:50,771 - root - INFO - [31mstep:  4  [32mloss: -1.0000  [33mmemory:  1.96GiB(4.96%)  [34mtps: 13,541  [36mtflops: 45.57  [35mmfu: 14.60%[39m
[rank1]:[titan] 2025-06-15 09:56:51,379 - root - INFO - [31mstep:  5  [32mloss: -1.0000  [33mmemory:  2.24GiB(5.67%)  [34mtps: 13,478  [36mtflops: 45.36  [35mmfu: 14.54%[39m
[rank0]:[titan] 2025-06-15 09:56:51,382 - root - INFO - [31mstep:  5  [32mloss: -1.0000  [33mmemory:  4.09GiB(10.35%)  [34mtps: 13,476  [36mtflops: 45.35  [35mmfu: 14.53%[39m
[rank3]:[titan] 2025-06-15 09:56:51,382 - root - INFO - [31mstep:  5  [32mloss: 10.1164  [33mmemory: 19.30GiB(48.87%)  [34mtps: 13,484  [36mtflops: 45.38  [35mmfu: 14.54%[39m
[rank2]:[titan] 2025-06-15 09:56:51,379 - root - INFO - [31mstep:  5  [32mloss: -1.0000  [33mmemory:  1.96GiB(4.96%)  [34mtps: 13,491  [36mtflops: 45.40  [35mmfu: 14.55%[39m
[rank1]:[titan] 2025-06-15 09:56:51,984 - root - INFO - [31mstep:  6  [32mloss: -1.0000  [33mmemory:  2.24GiB(5.67%)  [34mtps: 13,556  [36mtflops: 45.62  [35mmfu: 14.62%[39m
[rank0]:[titan] 2025-06-15 09:56:51,987 - root - INFO - [31mstep:  6  [32mloss: -1.0000  [33mmemory:  4.09GiB(10.35%)  [34mtps: 13,549  [36mtflops: 45.59  [35mmfu: 14.61%[39m
[rank3]:[titan] 2025-06-15 09:56:51,987 - root - INFO - [31mstep:  6  [32mloss:  9.8313  [33mmemory: 19.30GiB(48.87%)  [34mtps: 13,559  [36mtflops: 45.63  [35mmfu: 14.62%[39m
[rank2]:[titan] 2025-06-15 09:56:51,984 - root - INFO - [31mstep:  6  [32mloss: -1.0000  [33mmemory:  1.96GiB(4.96%)  [34mtps: 13,559  [36mtflops: 45.63  [35mmfu: 14.63%[39m
[rank1]:[titan] 2025-06-15 09:56:52,783 - root - INFO - [31mstep:  7  [32mloss: -1.0000  [33mmemory:  2.24GiB(5.67%)  [34mtps: 10,261  [36mtflops: 34.53  [35mmfu: 11.07%[39m
[rank0]:[titan] 2025-06-15 09:56:52,786 - root - INFO - [31mstep:  7  [32mloss: -1.0000  [33mmemory:  4.09GiB(10.35%)  [34mtps: 10,256  [36mtflops: 34.51  [35mmfu: 11.06%[39m
[rank3]:[titan] 2025-06-15 09:56:52,786 - root - INFO - [31mstep:  7  [32mloss:  9.5931  [33mmemory: 19.30GiB(48.87%)  [34mtps: 10,268  [36mtflops: 34.55  [35mmfu: 11.07%[39m
[rank2]:[titan] 2025-06-15 09:56:52,782 - root - INFO - [31mstep:  7  [32mloss: -1.0000  [33mmemory:  1.96GiB(4.96%)  [34mtps: 10,268  [36mtflops: 34.55  [35mmfu: 11.08%[39m
[rank1]:[titan] 2025-06-15 09:56:53,479 - root - INFO - [31mstep:  8  [32mloss: -1.0000  [33mmemory:  2.24GiB(5.67%)  [34mtps: 11,774  [36mtflops: 39.62  [35mmfu: 12.70%[39m
[rank0]:[titan] 2025-06-15 09:56:53,482 - root - INFO - [31mstep:  8  [32mloss: -1.0000  [33mmemory:  4.09GiB(10.35%)  [34mtps: 11,772  [36mtflops: 39.62  [35mmfu: 12.70%[39m
[rank3]:[titan] 2025-06-15 09:56:53,482 - root - INFO - [31mstep:  8  [32mloss:  9.5239  [33mmemory: 19.30GiB(48.87%)  [34mtps: 11,777  [36mtflops: 39.63  [35mmfu: 12.70%[39m
[rank2]:[titan] 2025-06-15 09:56:53,479 - root - INFO - [31mstep:  8  [32mloss: -1.0000  [33mmemory:  1.96GiB(4.96%)  [34mtps: 11,774  [36mtflops: 39.62  [35mmfu: 12.70%[39m
[rank1]:[titan] 2025-06-15 09:56:54,207 - root - INFO - [31mstep:  9  [32mloss: -1.0000  [33mmemory:  2.24GiB(5.67%)  [34mtps: 11,362  [36mtflops: 38.23  [35mmfu: 12.25%[39m
[rank0]:[titan] 2025-06-15 09:56:54,210 - root - INFO - [31mstep:  9  [32mloss: -1.0000  [33mmemory:  4.09GiB(10.35%)  [34mtps: 11,313  [36mtflops: 38.07  [35mmfu: 12.20%[39m
[rank3]:[titan] 2025-06-15 09:56:54,210 - root - INFO - [31mstep:  9  [32mloss:  9.3892  [33mmemory: 19.30GiB(48.87%)  [34mtps: 11,324  [36mtflops: 38.11  [35mmfu: 12.21%[39m
[rank2]:[titan] 2025-06-15 09:56:54,207 - root - INFO - [31mstep:  9  [32mloss: -1.0000  [33mmemory:  1.96GiB(4.96%)  [34mtps: 11,263  [36mtflops: 37.90  [35mmfu: 12.15%[39m
[rank1]:[titan] 2025-06-15 09:56:55,103 - root - INFO - [31mstep: 10  [32mloss: -1.0000  [33mmemory:  2.24GiB(5.67%)  [34mtps: 9,149  [36mtflops: 30.79  [35mmfu: 9.87%[39m
[rank0]:[titan] 2025-06-15 09:56:55,107 - root - INFO - [31mstep: 10  [32mloss: -1.0000  [33mmemory:  4.09GiB(10.35%)  [34mtps: 9,146  [36mtflops: 30.78  [35mmfu: 9.86%[39m
[rank3]:[titan] 2025-06-15 09:56:55,106 - root - INFO - [31mstep: 10  [32mloss:  9.3618  [33mmemory: 19.30GiB(48.87%)  [34mtps: 9,149  [36mtflops: 30.79  [35mmfu: 9.87%[39m
[rank2]:[titan] 2025-06-15 09:56:55,103 - root - INFO - [31mstep: 10  [32mloss: -1.0000  [33mmemory:  1.96GiB(4.96%)  [34mtps: 9,147  [36mtflops: 30.78  [35mmfu: 9.87%[39m
[rank3]:[titan] 2025-06-15 09:56:55,534 - root - INFO - Dumping profiler traces at step 10
[rank2]:[titan] 2025-06-15 09:56:55,586 - root - INFO - Dumping profiler traces at step 10
[rank1]:[titan] 2025-06-15 09:56:55,671 - root - INFO - Dumping profiler traces at step 10
[rank0]:[titan] 2025-06-15 09:56:55,601 - root - INFO - Dumping profiler traces at step 10
[rank3]:[titan] 2025-06-15 09:56:55,715 - root - INFO - Finished dumping profiler traces in 0.18 seconds
[rank3]:[titan] 2025-06-15 09:56:55,717 - root - INFO - Training completed
[rank2]:[titan] 2025-06-15 09:56:55,786 - root - INFO - Finished dumping profiler traces in 0.20 seconds
[rank2]:[titan] 2025-06-15 09:56:55,787 - root - INFO - Training completed
[rank0]:[titan] 2025-06-15 09:56:55,810 - root - INFO - Finished dumping profiler traces in 0.21 seconds
[rank0]:[titan] 2025-06-15 09:56:55,810 - root - INFO - Sleeping 2 seconds for other ranks to complete
[rank1]:[titan] 2025-06-15 09:56:55,901 - root - INFO - Finished dumping profiler traces in 0.23 seconds
[rank1]:[titan] 2025-06-15 09:56:55,902 - root - INFO - Training completed
[rank2]:[titan] 2025-06-15 09:56:56,145 - root - INFO - Process group destroyed.
[rank1]:[titan] 2025-06-15 09:56:57,880 - root - INFO - Process group destroyed.
[rank0]:[titan] 2025-06-15 09:56:57,813 - root - INFO - Training completed
[rank3]:[titan] 2025-06-15 09:56:57,862 - root - INFO - Process group destroyed.
[rank0]:[titan] 2025-06-15 09:56:58,206 - root - INFO - Process group destroyed.
