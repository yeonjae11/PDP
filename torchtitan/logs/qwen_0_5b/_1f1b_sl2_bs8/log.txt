
============================================================
- exec time: 2025-06-15 09:47:41
- command: ./run_train.sh --parallelism.pipeline_parallel_degree=4 --parallelism.tensor_parallel_degree=1 --parallelism.pipeline_parallel_schedule=1F1B --parallelism.pipeline_parallel_num_stages_per_rank=1 --training.seq_len=2048 --training.local_batch_size=8 --job.dump_folder=logs/qwen_0_5b/_1f1b_sl2_bs8 --profiling.save_traces_folder= --metrics.save_tb_folder=
- config file: /home2/yeonjae/tp_partition/torchtitan/torchtitan/models/llama3/train_configs/qwen_0_5b.toml
- overrides: {'pipeline_parallel_degree': 4, 'tensor_parallel_degree': 1, 'pipeline_parallel_schedule': '1F1B', 'pipeline_parallel_num_stages_per_rank': 1, 'seq_len': 2048, 'local_batch_size': 8}
============================================================

--- STDOUT ---

--- STDERR ---
+ NGPU=4
+ export LOG_RANK=0,1,2,3
+ LOG_RANK=0,1,2,3
+ CONFIG_FILE=/home2/yeonjae/tp_partition/torchtitan/torchtitan/models/llama3/train_configs/qwen_0_5b.toml
+ overrides=
+ '[' 9 -ne 0 ']'
+ overrides='--parallelism.pipeline_parallel_degree=4 --parallelism.tensor_parallel_degree=1 --parallelism.pipeline_parallel_schedule=1F1B --parallelism.pipeline_parallel_num_stages_per_rank=1 --training.seq_len=2048 --training.local_batch_size=8 --job.dump_folder=logs/qwen_0_5b/_1f1b_sl2_bs8 --profiling.save_traces_folder= --metrics.save_tb_folder='
+ TORCHFT_LIGHTHOUSE=http://localhost:29510
+ PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
+ TORCHFT_LIGHTHOUSE=http://localhost:29510
+ torchrun --nproc_per_node=4 --rdzv_backend c10d --rdzv_endpoint=localhost:0 --local-ranks-filter 0,1,2,3 --role rank --tee 3 -m torchtitan.train --job.config_file /home2/yeonjae/tp_partition/torchtitan/torchtitan/models/llama3/train_configs/qwen_0_5b.toml --parallelism.pipeline_parallel_degree=4 --parallelism.tensor_parallel_degree=1 --parallelism.pipeline_parallel_schedule=1F1B --parallelism.pipeline_parallel_num_stages_per_rank=1 --training.seq_len=2048 --training.local_batch_size=8 --job.dump_folder=logs/qwen_0_5b/_1f1b_sl2_bs8 --profiling.save_traces_folder= --metrics.save_tb_folder=
W0615 09:47:42.323000 2068715 torch/distributed/run.py:766] 
W0615 09:47:42.323000 2068715 torch/distributed/run.py:766] *****************************************
W0615 09:47:42.323000 2068715 torch/distributed/run.py:766] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0615 09:47:42.323000 2068715 torch/distributed/run.py:766] *****************************************
[rank0]:[titan] 2025-06-15 09:47:47,916 - root - INFO - Starting job: Llama 3 8B training
[rank0]:[titan] 2025-06-15 09:47:47,978 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank0]:[titan] 2025-06-15 09:47:47,982 - root - INFO - Building 1-D device mesh with ['pp'], [4]
[rank0]:[titan] 2025-06-15 09:47:47,983 - root - INFO - [GC] Initial GC collection. 0.00 seconds.
[rank3]:[titan] 2025-06-15 09:47:47,945 - root - INFO - Starting job: Llama 3 8B training
[rank1]:[titan] 2025-06-15 09:47:48,000 - root - INFO - Starting job: Llama 3 8B training
[rank2]:[titan] 2025-06-15 09:47:48,022 - root - INFO - Starting job: Llama 3 8B training
[rank3]:[titan] 2025-06-15 09:47:48,950 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank3]:[titan] 2025-06-15 09:47:48,953 - root - INFO - Building 1-D device mesh with ['pp'], [4]
[rank3]:[titan] 2025-06-15 09:47:48,954 - root - INFO - [GC] Initial GC collection. 0.00 seconds.
[rank2]:[titan] 2025-06-15 09:47:49,027 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank2]:[titan] 2025-06-15 09:47:49,035 - root - INFO - Building 1-D device mesh with ['pp'], [4]
[rank2]:[titan] 2025-06-15 09:47:49,038 - root - INFO - [GC] Initial GC collection. 0.00 seconds.
[rank1]:[titan] 2025-06-15 09:47:49,079 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank1]:[titan] 2025-06-15 09:47:49,083 - root - INFO - Building 1-D device mesh with ['pp'], [4]
[rank1]:[titan] 2025-06-15 09:47:49,084 - root - INFO - [GC] Initial GC collection. 0.00 seconds.
[rank2]:[rank2]:[W615 09:47:49.685790837 Utils.hpp:137] Warning: Environment variable TORCH_NCCL_TRACE_BUFFER_SIZE is deprecated; use TORCH_FR_BUFFER_SIZE instead (function operator())
[rank3]:[rank3]:[W615 09:47:49.686267027 Utils.hpp:137] Warning: Environment variable TORCH_NCCL_TRACE_BUFFER_SIZE is deprecated; use TORCH_FR_BUFFER_SIZE instead (function operator())
[rank1]:[rank1]:[W615 09:47:49.687516387 Utils.hpp:137] Warning: Environment variable TORCH_NCCL_TRACE_BUFFER_SIZE is deprecated; use TORCH_FR_BUFFER_SIZE instead (function operator())
[rank0]:[rank0]:[W615 09:47:49.700969311 Utils.hpp:137] Warning: Environment variable TORCH_NCCL_TRACE_BUFFER_SIZE is deprecated; use TORCH_FR_BUFFER_SIZE instead (function operator())
[rank2]:[titan] 2025-06-15 09:47:49,844 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
[rank2]:[titan] 2025-06-15 09:47:49,845 - root - INFO - Preparing c4 dataset from allenai/c4
[rank0]:[titan] 2025-06-15 09:47:49,849 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
[rank0]:[titan] 2025-06-15 09:47:49,850 - root - INFO - Preparing c4 dataset from allenai/c4
[rank3]:[titan] 2025-06-15 09:47:49,862 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
[rank3]:[titan] 2025-06-15 09:47:49,863 - root - INFO - Preparing c4 dataset from allenai/c4
[rank1]:[titan] 2025-06-15 09:47:49,856 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
[rank1]:[titan] 2025-06-15 09:47:49,856 - root - INFO - Preparing c4 dataset from allenai/c4
[rank1]:[titan] 2025-06-15 09:48:07,788 - root - INFO - Building llama3 0.5B with TransformerModelArgs(_enforced='This field is used to enforce all fields have defaults.', dim=896, n_layers=24, n_heads=14, n_kv_heads=2, vocab_size=128256, multiple_of=256, intermediate_size=4864, ffn_dim_multiplier=None, norm_eps=1e-05, rope_theta=500000, max_seq_len=2048, depth_init=True, use_flex_attn=False, attn_mask_type='causal', eos_id=128001)
[rank1]:[titan] 2025-06-15 09:48:08,019 - root - INFO - CUDA capacity: NVIDIA A100-PCIE-40GB with 39.50GiB memory
[rank1]:[titan] 2025-06-15 09:48:08,057 - root - INFO - [34mModel llama3 0.5B [31msize: 587,705,216 total parameters[39m
[rank1]:[titan] 2025-06-15 09:48:08,057 - root - INFO - Compiling the loss function with torch.compile
[rank1]:[titan] 2025-06-15 09:48:08,103 - root - INFO - No 'pipeline_parallel_split_points' provided. Here is the auto-generated split, which may be sub-optimal: ['layers.6', 'layers.13', 'layers.19'].
[rank1]:[titan] 2025-06-15 09:48:08,121 - root - INFO - PP rank 1 is building stage_idx 1 with start_layer layers.6, stop_layer layers.13
[rank1]:[titan] 2025-06-15 09:48:08,121 - root - INFO - Applied full activation checkpointing to the model
[rank1]:[titan] 2025-06-15 09:48:08,123 - root - INFO - Compiling each TransformerBlock with torch.compile
[rank1]:[titan] 2025-06-15 09:48:08,123 - root - INFO - Using pipeline schedule 1F1B with 8 microbatches and 4 stages.
[rank1]:[titan] 2025-06-15 09:48:08,287 - root - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank1]:[titan] 2025-06-15 09:48:08,287 - root - INFO - CUDA memory usage for model: 0.40GiB(1.01%)
[rank1]:[titan] 2025-06-15 09:48:08,288 - root - INFO - Trainer is initialized with local batch size 8, global batch size 8, gradient accumulation steps 1, sequence length 2048, total steps 10 (warmup 2).
[rank1]:[titan] 2025-06-15 09:48:08,288 - root - INFO - Training starts at step 1.
[rank1]:[titan] 2025-06-15 09:48:08,288 - root - INFO - Profiling active. Traces will be saved at logs/qwen_0_5b/_1f1b_sl2_bs8/
[rank0]:[titan] 2025-06-15 09:48:09,326 - root - INFO - Building llama3 0.5B with TransformerModelArgs(_enforced='This field is used to enforce all fields have defaults.', dim=896, n_layers=24, n_heads=14, n_kv_heads=2, vocab_size=128256, multiple_of=256, intermediate_size=4864, ffn_dim_multiplier=None, norm_eps=1e-05, rope_theta=500000, max_seq_len=2048, depth_init=True, use_flex_attn=False, attn_mask_type='causal', eos_id=128001)
[rank2]:[titan] 2025-06-15 09:48:09,631 - root - INFO - Building llama3 0.5B with TransformerModelArgs(_enforced='This field is used to enforce all fields have defaults.', dim=896, n_layers=24, n_heads=14, n_kv_heads=2, vocab_size=128256, multiple_of=256, intermediate_size=4864, ffn_dim_multiplier=None, norm_eps=1e-05, rope_theta=500000, max_seq_len=2048, depth_init=True, use_flex_attn=False, attn_mask_type='causal', eos_id=128001)
[rank0]:[titan] 2025-06-15 09:48:09,557 - root - INFO - CUDA capacity: NVIDIA A100-PCIE-40GB with 39.50GiB memory
[rank0]:[titan] 2025-06-15 09:48:09,597 - root - INFO - [34mModel llama3 0.5B [31msize: 587,705,216 total parameters[39m
[rank0]:[titan] 2025-06-15 09:48:09,597 - root - INFO - Compiling the loss function with torch.compile
[rank0]:[titan] 2025-06-15 09:48:09,657 - root - INFO - No 'pipeline_parallel_split_points' provided. Here is the auto-generated split, which may be sub-optimal: ['layers.6', 'layers.13', 'layers.19'].
[rank0]:[titan] 2025-06-15 09:48:09,674 - root - INFO - PP rank 0 is building stage_idx 0 with start_layer None, stop_layer layers.6
[rank0]:[titan] 2025-06-15 09:48:09,675 - root - INFO - Applied full activation checkpointing to the model
[rank0]:[titan] 2025-06-15 09:48:09,677 - root - INFO - Compiling each TransformerBlock with torch.compile
[rank0]:[titan] 2025-06-15 09:48:09,677 - root - INFO - Using pipeline schedule 1F1B with 8 microbatches and 4 stages.
[rank0]:[titan] 2025-06-15 09:48:09,836 - root - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank0]:[titan] 2025-06-15 09:48:09,837 - root - INFO - CUDA memory usage for model: 0.77GiB(1.95%)
[rank0]:[titan] 2025-06-15 09:48:09,837 - root - INFO - Trainer is initialized with local batch size 8, global batch size 8, gradient accumulation steps 1, sequence length 2048, total steps 10 (warmup 2).
[rank0]:[titan] 2025-06-15 09:48:09,837 - root - INFO - Training starts at step 1.
[rank0]:[titan] 2025-06-15 09:48:09,837 - root - INFO - Profiling active. Traces will be saved at logs/qwen_0_5b/_1f1b_sl2_bs8/
[rank2]:[titan] 2025-06-15 09:48:09,863 - root - INFO - CUDA capacity: NVIDIA A100-PCIE-40GB with 39.50GiB memory
[rank2]:[titan] 2025-06-15 09:48:09,903 - root - INFO - [34mModel llama3 0.5B [31msize: 587,705,216 total parameters[39m
[rank2]:[titan] 2025-06-15 09:48:09,903 - root - INFO - Compiling the loss function with torch.compile
[rank2]:[titan] 2025-06-15 09:48:09,964 - root - INFO - No 'pipeline_parallel_split_points' provided. Here is the auto-generated split, which may be sub-optimal: ['layers.6', 'layers.13', 'layers.19'].
[rank2]:[titan] 2025-06-15 09:48:09,982 - root - INFO - PP rank 2 is building stage_idx 2 with start_layer layers.13, stop_layer layers.19
[rank2]:[titan] 2025-06-15 09:48:09,983 - root - INFO - Applied full activation checkpointing to the model
[rank2]:[titan] 2025-06-15 09:48:09,985 - root - INFO - Compiling each TransformerBlock with torch.compile
[rank2]:[titan] 2025-06-15 09:48:09,985 - root - INFO - Using pipeline schedule 1F1B with 8 microbatches and 4 stages.
[rank2]:[titan] 2025-06-15 09:48:10,132 - root - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank2]:[titan] 2025-06-15 09:48:10,132 - root - INFO - CUDA memory usage for model: 0.34GiB(0.86%)
[rank2]:[titan] 2025-06-15 09:48:10,133 - root - INFO - Trainer is initialized with local batch size 8, global batch size 8, gradient accumulation steps 1, sequence length 2048, total steps 10 (warmup 2).
[rank2]:[titan] 2025-06-15 09:48:10,133 - root - INFO - Training starts at step 1.
[rank2]:[titan] 2025-06-15 09:48:10,133 - root - INFO - Profiling active. Traces will be saved at logs/qwen_0_5b/_1f1b_sl2_bs8/
[rank0]:/home2/yeonjae/tp_partition/pytorch/torch/_inductor/lowering.py:1884: UserWarning: Torchinductor does not support code generation for complex operators. Performance may be worse than eager.
[rank0]:  warnings.warn(
[rank1]:/home2/yeonjae/tp_partition/pytorch/torch/_inductor/lowering.py:1884: UserWarning: Torchinductor does not support code generation for complex operators. Performance may be worse than eager.
[rank1]:  warnings.warn(
[rank2]:/home2/yeonjae/tp_partition/pytorch/torch/_inductor/lowering.py:1884: UserWarning: Torchinductor does not support code generation for complex operators. Performance may be worse than eager.
[rank2]:  warnings.warn(
[rank3]:[titan] 2025-06-15 09:48:57,023 - root - INFO - Building llama3 0.5B with TransformerModelArgs(_enforced='This field is used to enforce all fields have defaults.', dim=896, n_layers=24, n_heads=14, n_kv_heads=2, vocab_size=128256, multiple_of=256, intermediate_size=4864, ffn_dim_multiplier=None, norm_eps=1e-05, rope_theta=500000, max_seq_len=2048, depth_init=True, use_flex_attn=False, attn_mask_type='causal', eos_id=128001)
[rank3]:[titan] 2025-06-15 09:48:57,254 - root - INFO - TensorBoard logging enabled. Logs will be saved at logs/qwen_0_5b/_1f1b_sl2_bs8/20250615-0948
[rank3]:[titan] 2025-06-15 09:48:57,255 - root - INFO - CUDA capacity: NVIDIA A100-PCIE-40GB with 39.50GiB memory
[rank3]:[titan] 2025-06-15 09:48:57,290 - root - INFO - [34mModel llama3 0.5B [31msize: 587,705,216 total parameters[39m
[rank3]:[titan] 2025-06-15 09:48:57,290 - root - INFO - Compiling the loss function with torch.compile
[rank3]:[titan] 2025-06-15 09:48:57,335 - root - INFO - No 'pipeline_parallel_split_points' provided. Here is the auto-generated split, which may be sub-optimal: ['layers.6', 'layers.13', 'layers.19'].
[rank3]:[titan] 2025-06-15 09:48:57,352 - root - INFO - PP rank 3 is building stage_idx 3 with start_layer layers.19, stop_layer None
[rank3]:[titan] 2025-06-15 09:48:57,353 - root - INFO - Applied full activation checkpointing to the model
[rank3]:[titan] 2025-06-15 09:48:57,354 - root - INFO - Compiling each TransformerBlock with torch.compile
[rank3]:[titan] 2025-06-15 09:48:57,354 - root - INFO - Using pipeline schedule 1F1B with 8 microbatches and 4 stages.
[rank3]:[titan] 2025-06-15 09:48:57,510 - root - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank3]:[titan] 2025-06-15 09:48:57,511 - root - INFO - CUDA memory usage for model: 0.71GiB(1.80%)
[rank3]:[titan] 2025-06-15 09:48:57,512 - root - INFO - Trainer is initialized with local batch size 8, global batch size 8, gradient accumulation steps 1, sequence length 2048, total steps 10 (warmup 2).
[rank3]:[titan] 2025-06-15 09:48:57,512 - root - INFO - Training starts at step 1.
[rank3]:[titan] 2025-06-15 09:48:57,512 - root - INFO - Profiling active. Traces will be saved at logs/qwen_0_5b/_1f1b_sl2_bs8/
[rank3]:/home2/yeonjae/tp_partition/pytorch/torch/_inductor/lowering.py:1884: UserWarning: Torchinductor does not support code generation for complex operators. Performance may be worse than eager.
[rank3]:  warnings.warn(
[rank3]:[titan] 2025-06-15 09:49:06,984 - root - INFO - [31mstep:  1  [32mloss: 12.2585  [33mmemory:  9.39GiB(23.79%)  [34mtps: 423  [36mtflops: 1.42  [35mmfu: 0.46%[39m
[rank3]:[titan] 2025-06-15 09:49:06,984 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:01:40
[rank2]:[titan] 2025-06-15 09:49:06,992 - root - INFO - [31mstep:  1  [32mloss: -1.0000  [33mmemory:  1.55GiB(3.92%)  [34mtps: 72  [36mtflops: 0.24  [35mmfu: 0.08%[39m
[rank2]:[titan] 2025-06-15 09:49:06,993 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:01:40
[rank1]:[titan] 2025-06-15 09:49:06,997 - root - INFO - [31mstep:  1  [32mloss: -1.0000  [33mmemory:  1.77GiB(4.47%)  [34mtps: 69  [36mtflops: 0.23  [35mmfu: 0.07%[39m
[rank1]:[titan] 2025-06-15 09:49:06,997 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:01:40
[rank0]:[titan] 2025-06-15 09:49:07,017 - root - INFO - [31mstep:  1  [32mloss: -1.0000  [33mmemory:  3.17GiB(8.02%)  [34mtps: 71  [36mtflops: 0.24  [35mmfu: 0.08%[39m
[rank0]:[titan] 2025-06-15 09:49:07,017 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:01:40
[rank3]:[titan] 2025-06-15 09:49:07,357 - root - INFO - [31mstep:  2  [32mloss: 11.8431  [33mmemory: 10.51GiB(26.61%)  [34mtps: 10,989  [36mtflops: 36.98  [35mmfu: 11.85%[39m
[rank2]:[titan] 2025-06-15 09:49:07,354 - root - INFO - [31mstep:  2  [32mloss: -1.0000  [33mmemory:  1.80GiB(4.57%)  [34mtps: 11,347  [36mtflops: 38.18  [35mmfu: 12.24%[39m
[rank1]:[titan] 2025-06-15 09:49:07,354 - root - INFO - [31mstep:  2  [32mloss: -1.0000  [33mmemory:  2.08GiB(5.27%)  [34mtps: 11,476  [36mtflops: 38.62  [35mmfu: 12.38%[39m
[rank0]:[titan] 2025-06-15 09:49:07,357 - root - INFO - [31mstep:  2  [32mloss: -1.0000  [33mmemory:  4.03GiB(10.21%)  [34mtps: 12,048  [36mtflops: 40.55  [35mmfu: 13.00%[39m
[rank3]:[titan] 2025-06-15 09:49:07,703 - root - INFO - [31mstep:  3  [32mloss: 11.0950  [33mmemory: 11.37GiB(28.79%)  [34mtps: 11,890  [36mtflops: 40.01  [35mmfu: 12.82%[39m
[rank2]:[titan] 2025-06-15 09:49:07,699 - root - INFO - [31mstep:  3  [32mloss: -1.0000  [33mmemory:  1.80GiB(4.57%)  [34mtps: 11,855  [36mtflops: 39.89  [35mmfu: 12.79%[39m
[rank1]:[titan] 2025-06-15 09:49:07,700 - root - INFO - [31mstep:  3  [32mloss: -1.0000  [33mmemory:  2.08GiB(5.27%)  [34mtps: 11,864  [36mtflops: 39.92  [35mmfu: 12.80%[39m
[rank0]:[titan] 2025-06-15 09:49:07,703 - root - INFO - [31mstep:  3  [32mloss: -1.0000  [33mmemory:  4.03GiB(10.21%)  [34mtps: 11,860  [36mtflops: 39.91  [35mmfu: 12.79%[39m
[rank3]:[titan] 2025-06-15 09:49:08,044 - root - INFO - [31mstep:  4  [32mloss: 11.3972  [33mmemory: 11.37GiB(28.79%)  [34mtps: 12,023  [36mtflops: 40.46  [35mmfu: 12.97%[39m
[rank2]:[titan] 2025-06-15 09:49:08,041 - root - INFO - [31mstep:  4  [32mloss: -1.0000  [33mmemory:  1.80GiB(4.57%)  [34mtps: 12,009  [36mtflops: 40.41  [35mmfu: 12.95%[39m
[rank0]:[titan] 2025-06-15 09:49:08,044 - root - INFO - [31mstep:  4  [32mloss: -1.0000  [33mmemory:  4.03GiB(10.21%)  [34mtps: 12,014  [36mtflops: 40.43  [35mmfu: 12.96%[39m
[rank1]:[titan] 2025-06-15 09:49:08,041 - root - INFO - [31mstep:  4  [32mloss: -1.0000  [33mmemory:  2.08GiB(5.27%)  [34mtps: 12,015  [36mtflops: 40.43  [35mmfu: 12.96%[39m
[rank3]:[titan] 2025-06-15 09:49:08,391 - root - INFO - [31mstep:  5  [32mloss: 10.1138  [33mmemory: 11.37GiB(28.79%)  [34mtps: 11,818  [36mtflops: 39.77  [35mmfu: 12.75%[39m
[rank2]:[titan] 2025-06-15 09:49:08,388 - root - INFO - [31mstep:  5  [32mloss: -1.0000  [33mmemory:  1.80GiB(4.57%)  [34mtps: 11,807  [36mtflops: 39.73  [35mmfu: 12.73%[39m
[rank0]:[titan] 2025-06-15 09:49:08,392 - root - INFO - [31mstep:  5  [32mloss: -1.0000  [33mmemory:  4.03GiB(10.21%)  [34mtps: 11,812  [36mtflops: 39.75  [35mmfu: 12.74%[39m
[rank1]:[titan] 2025-06-15 09:49:08,389 - root - INFO - [31mstep:  5  [32mloss: -1.0000  [33mmemory:  2.08GiB(5.27%)  [34mtps: 11,811  [36mtflops: 39.75  [35mmfu: 12.74%[39m
[rank3]:[titan] 2025-06-15 09:49:08,734 - root - INFO - [31mstep:  6  [32mloss:  9.7561  [33mmemory: 11.37GiB(28.79%)  [34mtps: 11,988  [36mtflops: 40.34  [35mmfu: 12.93%[39m
[rank2]:[titan] 2025-06-15 09:49:08,726 - root - INFO - [31mstep:  6  [32mloss: -1.0000  [33mmemory:  1.80GiB(4.57%)  [34mtps: 12,121  [36mtflops: 40.79  [35mmfu: 13.07%[39m
[rank0]:[titan] 2025-06-15 09:49:08,730 - root - INFO - [31mstep:  6  [32mloss: -1.0000  [33mmemory:  4.03GiB(10.21%)  [34mtps: 12,126  [36mtflops: 40.81  [35mmfu: 13.08%[39m
[rank1]:[titan] 2025-06-15 09:49:08,727 - root - INFO - [31mstep:  6  [32mloss: -1.0000  [33mmemory:  2.08GiB(5.27%)  [34mtps: 12,128  [36mtflops: 40.81  [35mmfu: 13.08%[39m
[rank3]:[titan] 2025-06-15 09:49:09,223 - root - INFO - [31mstep:  7  [32mloss:  9.6335  [33mmemory: 11.37GiB(28.79%)  [34mtps: 8,395  [36mtflops: 28.25  [35mmfu: 9.05%[39m
[rank2]:[titan] 2025-06-15 09:49:09,219 - root - INFO - [31mstep:  7  [32mloss: -1.0000  [33mmemory:  1.80GiB(4.57%)  [34mtps: 8,312  [36mtflops: 27.97  [35mmfu: 8.97%[39m
[rank0]:[titan] 2025-06-15 09:49:09,223 - root - INFO - [31mstep:  7  [32mloss: -1.0000  [33mmemory:  4.03GiB(10.21%)  [34mtps: 8,309  [36mtflops: 27.96  [35mmfu: 8.96%[39m
[rank1]:[titan] 2025-06-15 09:49:09,220 - root - INFO - [31mstep:  7  [32mloss: -1.0000  [33mmemory:  2.08GiB(5.27%)  [34mtps: 8,314  [36mtflops: 27.98  [35mmfu: 8.97%[39m
[rank3]:[titan] 2025-06-15 09:49:09,571 - root - INFO - [31mstep:  8  [32mloss:  9.6467  [33mmemory: 11.37GiB(28.79%)  [34mtps: 11,785  [36mtflops: 39.66  [35mmfu: 12.71%[39m
[rank2]:[titan] 2025-06-15 09:49:09,568 - root - INFO - [31mstep:  8  [32mloss: -1.0000  [33mmemory:  1.80GiB(4.57%)  [34mtps: 11,774  [36mtflops: 39.62  [35mmfu: 12.70%[39m
[rank1]:[titan] 2025-06-15 09:49:09,568 - root - INFO - [31mstep:  8  [32mloss: -1.0000  [33mmemory:  2.08GiB(5.27%)  [34mtps: 11,786  [36mtflops: 39.66  [35mmfu: 12.71%[39m
[rank0]:[titan] 2025-06-15 09:49:09,571 - root - INFO - [31mstep:  8  [32mloss: -1.0000  [33mmemory:  4.03GiB(10.21%)  [34mtps: 11,779  [36mtflops: 39.64  [35mmfu: 12.70%[39m
[rank3]:[titan] 2025-06-15 09:49:09,934 - root - INFO - [31mstep:  9  [32mloss:  9.3930  [33mmemory: 11.37GiB(28.79%)  [34mtps: 11,315  [36mtflops: 38.08  [35mmfu: 12.20%[39m
[rank2]:[titan] 2025-06-15 09:49:09,931 - root - INFO - [31mstep:  9  [32mloss: -1.0000  [33mmemory:  1.80GiB(4.57%)  [34mtps: 11,305  [36mtflops: 38.04  [35mmfu: 12.19%[39m
[rank1]:[titan] 2025-06-15 09:49:09,931 - root - INFO - [31mstep:  9  [32mloss: -1.0000  [33mmemory:  2.08GiB(5.27%)  [34mtps: 11,312  [36mtflops: 38.07  [35mmfu: 12.20%[39m
[rank0]:[titan] 2025-06-15 09:49:09,934 - root - INFO - [31mstep:  9  [32mloss: -1.0000  [33mmemory:  4.03GiB(10.21%)  [34mtps: 11,313  [36mtflops: 38.07  [35mmfu: 12.20%[39m
[rank2]:[titan] 2025-06-15 09:49:10,354 - root - INFO - [31mstep: 10  [32mloss: -1.0000  [33mmemory:  1.80GiB(4.57%)  [34mtps: 9,689  [36mtflops: 32.61  [35mmfu: 10.45%[39m
[rank1]:[titan] 2025-06-15 09:49:10,354 - root - INFO - [31mstep: 10  [32mloss: -1.0000  [33mmemory:  2.08GiB(5.27%)  [34mtps: 9,692  [36mtflops: 32.62  [35mmfu: 10.45%[39m
[rank0]:[titan] 2025-06-15 09:49:10,357 - root - INFO - [31mstep: 10  [32mloss: -1.0000  [33mmemory:  4.03GiB(10.21%)  [34mtps: 9,683  [36mtflops: 32.59  [35mmfu: 10.44%[39m
[rank3]:[titan] 2025-06-15 09:49:10,357 - root - INFO - [31mstep: 10  [32mloss:  9.2590  [33mmemory: 11.37GiB(28.79%)  [34mtps: 9,695  [36mtflops: 32.63  [35mmfu: 10.46%[39m
[rank3]:[titan] 2025-06-15 09:49:10,569 - root - INFO - Dumping profiler traces at step 10
[rank2]:[titan] 2025-06-15 09:49:10,595 - root - INFO - Dumping profiler traces at step 10
[rank1]:[titan] 2025-06-15 09:49:10,629 - root - INFO - Dumping profiler traces at step 10
[rank0]:[titan] 2025-06-15 09:49:10,601 - root - INFO - Dumping profiler traces at step 10
[rank3]:[titan] 2025-06-15 09:49:10,660 - root - INFO - Finished dumping profiler traces in 0.09 seconds
[rank3]:[titan] 2025-06-15 09:49:10,662 - root - INFO - Training completed
[rank2]:[titan] 2025-06-15 09:49:10,697 - root - INFO - Finished dumping profiler traces in 0.10 seconds
[rank2]:[titan] 2025-06-15 09:49:10,697 - root - INFO - Training completed
[rank1]:[titan] 2025-06-15 09:49:10,746 - root - INFO - Finished dumping profiler traces in 0.12 seconds
[rank1]:[titan] 2025-06-15 09:49:10,747 - root - INFO - Training completed
[rank0]:[titan] 2025-06-15 09:49:10,707 - root - INFO - Finished dumping profiler traces in 0.11 seconds
[rank0]:[titan] 2025-06-15 09:49:10,708 - root - INFO - Sleeping 2 seconds for other ranks to complete
[rank2]:[titan] 2025-06-15 09:49:10,970 - root - INFO - Process group destroyed.
[rank0]:[titan] 2025-06-15 09:49:12,710 - root - INFO - Training completed
[rank3]:[titan] 2025-06-15 09:49:12,766 - root - INFO - Process group destroyed.
[rank1]:[titan] 2025-06-15 09:49:12,778 - root - INFO - Process group destroyed.
[rank0]:[titan] 2025-06-15 09:49:12,854 - root - INFO - Process group destroyed.
