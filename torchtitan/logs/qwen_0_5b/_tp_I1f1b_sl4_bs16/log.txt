
============================================================
- exec time: 2025-06-15 10:03:54
- command: ./run_train.sh --parallelism.tensor_parallel_degree=2 --parallelism.pipeline_parallel_degree=2 --parallelism.pipeline_parallel_schedule=Interleaved1F1B --parallelism.pipeline_parallel_num_stages_per_rank=2 --training.seq_len=4096 --training.local_batch_size=16 --job.dump_folder=logs/qwen_0_5b/_tp_I1f1b_sl4_bs16 --profiling.save_traces_folder= --metrics.save_tb_folder=
- config file: /home2/yeonjae/tp_partition/torchtitan/torchtitan/models/llama3/train_configs/qwen_0_5b.toml
- overrides: {'tensor_parallel_degree': 2, 'pipeline_parallel_degree': 2, 'pipeline_parallel_schedule': 'Interleaved1F1B', 'pipeline_parallel_num_stages_per_rank': 2, 'seq_len': 4096, 'local_batch_size': 16}
============================================================

--- STDOUT ---

--- STDERR ---
+ NGPU=4
+ export LOG_RANK=0,1,2,3
+ LOG_RANK=0,1,2,3
+ CONFIG_FILE=/home2/yeonjae/tp_partition/torchtitan/torchtitan/models/llama3/train_configs/qwen_0_5b.toml
+ overrides=
+ '[' 9 -ne 0 ']'
+ overrides='--parallelism.tensor_parallel_degree=2 --parallelism.pipeline_parallel_degree=2 --parallelism.pipeline_parallel_schedule=Interleaved1F1B --parallelism.pipeline_parallel_num_stages_per_rank=2 --training.seq_len=4096 --training.local_batch_size=16 --job.dump_folder=logs/qwen_0_5b/_tp_I1f1b_sl4_bs16 --profiling.save_traces_folder= --metrics.save_tb_folder='
+ TORCHFT_LIGHTHOUSE=http://localhost:29510
+ PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
+ TORCHFT_LIGHTHOUSE=http://localhost:29510
+ torchrun --nproc_per_node=4 --rdzv_backend c10d --rdzv_endpoint=localhost:0 --local-ranks-filter 0,1,2,3 --role rank --tee 3 -m torchtitan.train --job.config_file /home2/yeonjae/tp_partition/torchtitan/torchtitan/models/llama3/train_configs/qwen_0_5b.toml --parallelism.tensor_parallel_degree=2 --parallelism.pipeline_parallel_degree=2 --parallelism.pipeline_parallel_schedule=Interleaved1F1B --parallelism.pipeline_parallel_num_stages_per_rank=2 --training.seq_len=4096 --training.local_batch_size=16 --job.dump_folder=logs/qwen_0_5b/_tp_I1f1b_sl4_bs16 --profiling.save_traces_folder= --metrics.save_tb_folder=
W0615 10:03:56.146000 2099832 torch/distributed/run.py:766] 
W0615 10:03:56.146000 2099832 torch/distributed/run.py:766] *****************************************
W0615 10:03:56.146000 2099832 torch/distributed/run.py:766] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0615 10:03:56.146000 2099832 torch/distributed/run.py:766] *****************************************
[rank1]:[titan] 2025-06-15 10:04:02,005 - root - INFO - Starting job: Llama 3 8B training
[rank3]:[titan] 2025-06-15 10:04:02,011 - root - INFO - Starting job: Llama 3 8B training
[rank2]:[titan] 2025-06-15 10:04:01,984 - root - INFO - Starting job: Llama 3 8B training
[rank0]:[titan] 2025-06-15 10:04:02,027 - root - INFO - Starting job: Llama 3 8B training
[rank0]:[titan] 2025-06-15 10:04:02,324 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank0]:[titan] 2025-06-15 10:04:02,329 - root - INFO - Building 2-D device mesh with ['pp', 'tp'], [2, 2]
[rank0]:[titan] 2025-06-15 10:04:02,332 - root - INFO - [GC] Initial GC collection. 0.00 seconds.
[rank2]:[titan] 2025-06-15 10:04:02,927 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank2]:[titan] 2025-06-15 10:04:02,935 - root - INFO - Building 2-D device mesh with ['pp', 'tp'], [2, 2]
[rank2]:[titan] 2025-06-15 10:04:02,942 - root - INFO - [GC] Initial GC collection. 0.00 seconds.
[rank3]:[titan] 2025-06-15 10:04:03,114 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank3]:[titan] 2025-06-15 10:04:03,119 - root - INFO - Building 2-D device mesh with ['pp', 'tp'], [2, 2]
[rank3]:[titan] 2025-06-15 10:04:03,121 - root - INFO - [GC] Initial GC collection. 0.00 seconds.
[rank1]:[titan] 2025-06-15 10:04:03,146 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank1]:[titan] 2025-06-15 10:04:03,174 - root - INFO - Building 2-D device mesh with ['pp', 'tp'], [2, 2]
[rank1]:[titan] 2025-06-15 10:04:03,183 - root - INFO - [GC] Initial GC collection. 0.00 seconds.
[rank1]:[rank1]:[W615 10:04:03.745921507 Utils.hpp:137] Warning: Environment variable TORCH_NCCL_TRACE_BUFFER_SIZE is deprecated; use TORCH_FR_BUFFER_SIZE instead (function operator())
[rank3]:[rank3]:[W615 10:04:03.745643907 Utils.hpp:137] Warning: Environment variable TORCH_NCCL_TRACE_BUFFER_SIZE is deprecated; use TORCH_FR_BUFFER_SIZE instead (function operator())
[rank2]:[rank2]:[W615 10:04:03.745963647 Utils.hpp:137] Warning: Environment variable TORCH_NCCL_TRACE_BUFFER_SIZE is deprecated; use TORCH_FR_BUFFER_SIZE instead (function operator())
[rank0]:[rank0]:[W615 10:04:03.746140247 Utils.hpp:137] Warning: Environment variable TORCH_NCCL_TRACE_BUFFER_SIZE is deprecated; use TORCH_FR_BUFFER_SIZE instead (function operator())
[rank1]:[titan] 2025-06-15 10:04:03,918 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
[rank1]:[titan] 2025-06-15 10:04:03,919 - root - INFO - Preparing c4 dataset from allenai/c4
[rank3]:[titan] 2025-06-15 10:04:03,900 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
[rank3]:[titan] 2025-06-15 10:04:03,901 - root - INFO - Preparing c4 dataset from allenai/c4
[rank2]:[titan] 2025-06-15 10:04:03,897 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
[rank2]:[titan] 2025-06-15 10:04:03,898 - root - INFO - Preparing c4 dataset from allenai/c4
[rank0]:[titan] 2025-06-15 10:04:03,919 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
[rank0]:[titan] 2025-06-15 10:04:03,919 - root - INFO - Preparing c4 dataset from allenai/c4
[rank0]:[titan] 2025-06-15 10:04:20,695 - root - INFO - Building llama3 0.5B with TransformerModelArgs(_enforced='This field is used to enforce all fields have defaults.', dim=896, n_layers=24, n_heads=14, n_kv_heads=2, vocab_size=128256, multiple_of=256, intermediate_size=4864, ffn_dim_multiplier=None, norm_eps=1e-05, rope_theta=500000, max_seq_len=4096, depth_init=True, use_flex_attn=False, attn_mask_type='causal', eos_id=128001)
[rank0]:[titan] 2025-06-15 10:04:20,927 - root - INFO - CUDA capacity: NVIDIA A100-PCIE-40GB with 39.50GiB memory
[rank0]:[titan] 2025-06-15 10:04:20,967 - root - INFO - [34mModel llama3 0.5B [31msize: 587,705,216 total parameters[39m
[rank0]:[titan] 2025-06-15 10:04:20,967 - root - INFO - Compiling the loss function with torch.compile
[rank0]:[titan] 2025-06-15 10:04:21,013 - root - INFO - No 'pipeline_parallel_split_points' provided. Here is the auto-generated split, which may be sub-optimal: ['layers.12'].
[rank0]:[titan] 2025-06-15 10:04:21,030 - root - INFO - PP rank 0 is building stage_idx 0 with start_layer None, stop_layer layers.12
[rank0]:[titan] 2025-06-15 10:04:21,079 - root - INFO - Applied Tensor Parallelism to the model
[rank0]:[titan] 2025-06-15 10:04:21,079 - root - INFO - Applied full activation checkpointing to the model
[rank3]:[titan] 2025-06-15 10:04:21,112 - root - INFO - Building llama3 0.5B with TransformerModelArgs(_enforced='This field is used to enforce all fields have defaults.', dim=896, n_layers=24, n_heads=14, n_kv_heads=2, vocab_size=128256, multiple_of=256, intermediate_size=4864, ffn_dim_multiplier=None, norm_eps=1e-05, rope_theta=500000, max_seq_len=4096, depth_init=True, use_flex_attn=False, attn_mask_type='causal', eos_id=128001)
[rank0]:[titan] 2025-06-15 10:04:21,083 - root - INFO - Compiling each TransformerBlock with torch.compile
[rank0]:[titan] 2025-06-15 10:04:21,083 - root - INFO - Using pipeline schedule Interleaved1F1B with 16 microbatches and 2 stages.
[rank3]:[titan] 2025-06-15 10:04:21,344 - root - INFO - CUDA capacity: NVIDIA A100-PCIE-40GB with 39.50GiB memory
[rank0]:[titan] 2025-06-15 10:04:21,348 - root - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank0]:[titan] 2025-06-15 10:04:21,349 - root - INFO - CUDA memory usage for model: 0.56GiB(1.41%)
[rank0]:[titan] 2025-06-15 10:04:21,352 - root - INFO - Trainer is initialized with local batch size 16, global batch size 16, gradient accumulation steps 1, sequence length 4096, total steps 10 (warmup 2).
[rank0]:[titan] 2025-06-15 10:04:21,352 - root - INFO - Training starts at step 1.
[rank0]:[titan] 2025-06-15 10:04:21,352 - root - INFO - Profiling active. Traces will be saved at logs/qwen_0_5b/_tp_I1f1b_sl4_bs16/
[rank3]:[titan] 2025-06-15 10:04:21,385 - root - INFO - [34mModel llama3 0.5B [31msize: 587,705,216 total parameters[39m
[rank3]:[titan] 2025-06-15 10:04:21,385 - root - INFO - Compiling the loss function with torch.compile
[rank3]:[titan] 2025-06-15 10:04:21,435 - root - INFO - No 'pipeline_parallel_split_points' provided. Here is the auto-generated split, which may be sub-optimal: ['layers.12'].
[rank3]:[titan] 2025-06-15 10:04:21,456 - root - INFO - PP rank 1 is building stage_idx 1 with start_layer layers.12, stop_layer None
[rank3]:[titan] 2025-06-15 10:04:21,507 - root - INFO - Applied Tensor Parallelism to the model
[rank3]:[titan] 2025-06-15 10:04:21,508 - root - INFO - Applied full activation checkpointing to the model
[rank3]:[titan] 2025-06-15 10:04:21,512 - root - INFO - Compiling each TransformerBlock with torch.compile
[rank3]:[titan] 2025-06-15 10:04:21,512 - root - INFO - Using pipeline schedule Interleaved1F1B with 16 microbatches and 2 stages.
[rank3]:[titan] 2025-06-15 10:04:21,772 - root - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank3]:[titan] 2025-06-15 10:04:21,772 - root - INFO - CUDA memory usage for model: 0.56GiB(1.41%)
[rank3]:[titan] 2025-06-15 10:04:21,774 - root - INFO - Trainer is initialized with local batch size 16, global batch size 16, gradient accumulation steps 1, sequence length 4096, total steps 10 (warmup 2).
[rank3]:[titan] 2025-06-15 10:04:21,774 - root - INFO - Training starts at step 1.
[rank3]:[titan] 2025-06-15 10:04:21,774 - root - INFO - Profiling active. Traces will be saved at logs/qwen_0_5b/_tp_I1f1b_sl4_bs16/
[rank2]:[titan] 2025-06-15 10:04:22,123 - root - INFO - Building llama3 0.5B with TransformerModelArgs(_enforced='This field is used to enforce all fields have defaults.', dim=896, n_layers=24, n_heads=14, n_kv_heads=2, vocab_size=128256, multiple_of=256, intermediate_size=4864, ffn_dim_multiplier=None, norm_eps=1e-05, rope_theta=500000, max_seq_len=4096, depth_init=True, use_flex_attn=False, attn_mask_type='causal', eos_id=128001)
[rank1]:[titan] 2025-06-15 10:04:22,243 - root - INFO - Building llama3 0.5B with TransformerModelArgs(_enforced='This field is used to enforce all fields have defaults.', dim=896, n_layers=24, n_heads=14, n_kv_heads=2, vocab_size=128256, multiple_of=256, intermediate_size=4864, ffn_dim_multiplier=None, norm_eps=1e-05, rope_theta=500000, max_seq_len=4096, depth_init=True, use_flex_attn=False, attn_mask_type='causal', eos_id=128001)
[rank2]:[titan] 2025-06-15 10:04:22,355 - root - INFO - TensorBoard logging enabled. Logs will be saved at logs/qwen_0_5b/_tp_I1f1b_sl4_bs16/20250615-1004
[rank2]:[titan] 2025-06-15 10:04:22,356 - root - INFO - CUDA capacity: NVIDIA A100-PCIE-40GB with 39.50GiB memory
[rank2]:[titan] 2025-06-15 10:04:22,395 - root - INFO - [34mModel llama3 0.5B [31msize: 587,705,216 total parameters[39m
[rank2]:[titan] 2025-06-15 10:04:22,395 - root - INFO - Compiling the loss function with torch.compile
[rank2]:[titan] 2025-06-15 10:04:22,441 - root - INFO - No 'pipeline_parallel_split_points' provided. Here is the auto-generated split, which may be sub-optimal: ['layers.12'].
[rank2]:[titan] 2025-06-15 10:04:22,460 - root - INFO - PP rank 1 is building stage_idx 1 with start_layer layers.12, stop_layer None
[rank1]:[titan] 2025-06-15 10:04:22,465 - root - INFO - CUDA capacity: NVIDIA A100-PCIE-40GB with 39.50GiB memory
[rank2]:[titan] 2025-06-15 10:04:22,510 - root - INFO - Applied Tensor Parallelism to the model
[rank2]:[titan] 2025-06-15 10:04:22,510 - root - INFO - Applied full activation checkpointing to the model
[rank2]:[titan] 2025-06-15 10:04:22,514 - root - INFO - Compiling each TransformerBlock with torch.compile
[rank2]:[titan] 2025-06-15 10:04:22,514 - root - INFO - Using pipeline schedule Interleaved1F1B with 16 microbatches and 2 stages.
[rank1]:[titan] 2025-06-15 10:04:22,520 - root - INFO - [34mModel llama3 0.5B [31msize: 587,705,216 total parameters[39m
[rank1]:[titan] 2025-06-15 10:04:22,520 - root - INFO - Compiling the loss function with torch.compile
[rank1]:[titan] 2025-06-15 10:04:22,572 - root - INFO - No 'pipeline_parallel_split_points' provided. Here is the auto-generated split, which may be sub-optimal: ['layers.12'].
[rank1]:[titan] 2025-06-15 10:04:22,591 - root - INFO - PP rank 0 is building stage_idx 0 with start_layer None, stop_layer layers.12
[rank1]:[titan] 2025-06-15 10:04:22,641 - root - INFO - Applied Tensor Parallelism to the model
[rank1]:[titan] 2025-06-15 10:04:22,642 - root - INFO - Applied full activation checkpointing to the model
[rank1]:[titan] 2025-06-15 10:04:22,645 - root - INFO - Compiling each TransformerBlock with torch.compile
[rank1]:[titan] 2025-06-15 10:04:22,646 - root - INFO - Using pipeline schedule Interleaved1F1B with 16 microbatches and 2 stages.
[rank2]:[titan] 2025-06-15 10:04:22,760 - root - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank2]:[titan] 2025-06-15 10:04:22,761 - root - INFO - CUDA memory usage for model: 0.56GiB(1.41%)
[rank2]:[titan] 2025-06-15 10:04:22,762 - root - INFO - Trainer is initialized with local batch size 16, global batch size 16, gradient accumulation steps 1, sequence length 4096, total steps 10 (warmup 2).
[rank2]:[titan] 2025-06-15 10:04:22,762 - root - INFO - Training starts at step 1.
[rank2]:[titan] 2025-06-15 10:04:22,762 - root - INFO - Profiling active. Traces will be saved at logs/qwen_0_5b/_tp_I1f1b_sl4_bs16/
[rank1]:[titan] 2025-06-15 10:04:22,911 - root - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank1]:[titan] 2025-06-15 10:04:22,912 - root - INFO - CUDA memory usage for model: 0.56GiB(1.41%)
[rank1]:[titan] 2025-06-15 10:04:22,913 - root - INFO - Trainer is initialized with local batch size 16, global batch size 16, gradient accumulation steps 1, sequence length 4096, total steps 10 (warmup 2).
[rank1]:[titan] 2025-06-15 10:04:22,913 - root - INFO - Training starts at step 1.
[rank1]:[titan] 2025-06-15 10:04:22,913 - root - INFO - Profiling active. Traces will be saved at logs/qwen_0_5b/_tp_I1f1b_sl4_bs16/
[rank1]:/home2/yeonjae/tp_partition/pytorch/torch/_inductor/lowering.py:1884: UserWarning: Torchinductor does not support code generation for complex operators. Performance may be worse than eager.
[rank1]:  warnings.warn(
[rank0]:/home2/yeonjae/tp_partition/pytorch/torch/_inductor/lowering.py:1884: UserWarning: Torchinductor does not support code generation for complex operators. Performance may be worse than eager.
[rank0]:  warnings.warn(
[rank2]:/home2/yeonjae/tp_partition/pytorch/torch/_inductor/lowering.py:1884: UserWarning: Torchinductor does not support code generation for complex operators. Performance may be worse than eager.
[rank2]:  warnings.warn(
[rank3]:/home2/yeonjae/tp_partition/pytorch/torch/_inductor/lowering.py:1884: UserWarning: Torchinductor does not support code generation for complex operators. Performance may be worse than eager.
[rank3]:  warnings.warn(
[rank1]:[titan] 2025-06-15 10:04:39,780 - root - INFO - [31mstep:  1  [32mloss: -1.0000  [33mmemory:  2.76GiB(6.98%)  [34mtps: 949  [36mtflops: 3.70  [35mmfu: 1.18%[39m
[rank1]:[titan] 2025-06-15 10:04:39,781 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:01:40
[rank2]:[titan] 2025-06-15 10:04:39,756 - root - INFO - [31mstep:  1  [32mloss: 12.2393  [33mmemory: 17.04GiB(43.13%)  [34mtps: 944  [36mtflops: 3.67  [35mmfu: 1.18%[39m
[rank2]:[titan] 2025-06-15 10:04:39,756 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:01:40
[rank3]:[titan] 2025-06-15 10:04:39,756 - root - INFO - [31mstep:  1  [32mloss: 12.2393  [33mmemory: 17.04GiB(43.13%)  [34mtps: 892  [36mtflops: 3.47  [35mmfu: 1.11%[39m
[rank3]:[titan] 2025-06-15 10:04:39,756 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:01:40
[rank0]:[titan] 2025-06-15 10:04:39,780 - root - INFO - [31mstep:  1  [32mloss: -1.0000  [33mmemory:  2.76GiB(6.98%)  [34mtps: 871  [36mtflops: 3.39  [35mmfu: 1.09%[39m
[rank0]:[titan] 2025-06-15 10:04:39,780 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:01:40
[rank1]:[titan] 2025-06-15 10:04:41,951 - root - INFO - [31mstep:  2  [32mloss: -1.0000  [33mmemory:  3.41GiB(8.63%)  [34mtps: 7,549  [36mtflops: 29.39  [35mmfu: 9.42%[39m
[rank2]:[titan] 2025-06-15 10:04:41,990 - root - INFO - [31mstep:  2  [32mloss: 11.8190  [33mmemory: 17.96GiB(45.47%)  [34mtps: 7,338  [36mtflops: 28.57  [35mmfu: 9.16%[39m
[rank3]:[titan] 2025-06-15 10:04:41,989 - root - INFO - [31mstep:  2  [32mloss: 11.8190  [33mmemory: 17.96GiB(45.47%)  [34mtps: 7,340  [36mtflops: 28.58  [35mmfu: 9.16%[39m
[rank0]:[titan] 2025-06-15 10:04:41,951 - root - INFO - [31mstep:  2  [32mloss: -1.0000  [33mmemory:  3.41GiB(8.63%)  [34mtps: 7,549  [36mtflops: 29.40  [35mmfu: 9.42%[39m
[rank1]:[titan] 2025-06-15 10:04:44,121 - root - INFO - [31mstep:  3  [32mloss: -1.0000  [33mmemory:  3.41GiB(8.63%)  [34mtps: 7,575  [36mtflops: 29.50  [35mmfu: 9.45%[39m
[rank2]:[titan] 2025-06-15 10:04:44,110 - root - INFO - [31mstep:  3  [32mloss: 10.8808  [33mmemory: 18.39GiB(46.56%)  [34mtps: 7,734  [36mtflops: 30.11  [35mmfu: 9.65%[39m
[rank0]:[titan] 2025-06-15 10:04:44,121 - root - INFO - [31mstep:  3  [32mloss: -1.0000  [33mmemory:  3.41GiB(8.63%)  [34mtps: 7,552  [36mtflops: 29.40  [35mmfu: 9.42%[39m
[rank3]:[titan] 2025-06-15 10:04:44,109 - root - INFO - [31mstep:  3  [32mloss: 10.8808  [33mmemory: 18.39GiB(46.56%)  [34mtps: 7,731  [36mtflops: 30.10  [35mmfu: 9.65%[39m
[rank1]:[titan] 2025-06-15 10:04:46,271 - root - INFO - [31mstep:  4  [32mloss: -1.0000  [33mmemory:  3.41GiB(8.63%)  [34mtps: 7,623  [36mtflops: 29.68  [35mmfu: 9.51%[39m
[rank2]:[titan] 2025-06-15 10:04:46,259 - root - INFO - [31mstep:  4  [32mloss: 10.5617  [33mmemory: 18.39GiB(46.56%)  [34mtps: 7,628  [36mtflops: 29.70  [35mmfu: 9.52%[39m
[rank0]:[titan] 2025-06-15 10:04:46,270 - root - INFO - [31mstep:  4  [32mloss: -1.0000  [33mmemory:  3.41GiB(8.63%)  [34mtps: 7,626  [36mtflops: 29.70  [35mmfu: 9.52%[39m
[rank3]:[titan] 2025-06-15 10:04:46,259 - root - INFO - [31mstep:  4  [32mloss: 10.5617  [33mmemory: 18.39GiB(46.56%)  [34mtps: 7,625  [36mtflops: 29.69  [35mmfu: 9.52%[39m
[rank1]:[titan] 2025-06-15 10:04:48,413 - root - INFO - [31mstep:  5  [32mloss: -1.0000  [33mmemory:  3.41GiB(8.63%)  [34mtps: 7,651  [36mtflops: 29.79  [35mmfu: 9.55%[39m
[rank2]:[titan] 2025-06-15 10:04:48,399 - root - INFO - [31mstep:  5  [32mloss:  9.9059  [33mmemory: 18.39GiB(46.56%)  [34mtps: 7,660  [36mtflops: 29.83  [35mmfu: 9.56%[39m
[rank0]:[titan] 2025-06-15 10:04:48,412 - root - INFO - [31mstep:  5  [32mloss: -1.0000  [33mmemory:  3.41GiB(8.63%)  [34mtps: 7,652  [36mtflops: 29.79  [35mmfu: 9.55%[39m
[rank3]:[titan] 2025-06-15 10:04:48,399 - root - INFO - [31mstep:  5  [32mloss:  9.9059  [33mmemory: 18.39GiB(46.56%)  [34mtps: 7,661  [36mtflops: 29.83  [35mmfu: 9.56%[39m
[rank1]:[titan] 2025-06-15 10:04:50,574 - root - INFO - [31mstep:  6  [32mloss: -1.0000  [33mmemory:  3.41GiB(8.63%)  [34mtps: 7,583  [36mtflops: 29.53  [35mmfu: 9.46%[39m
[rank2]:[titan] 2025-06-15 10:04:50,561 - root - INFO - [31mstep:  6  [32mloss:  9.7298  [33mmemory: 18.39GiB(46.56%)  [34mtps: 7,580  [36mtflops: 29.51  [35mmfu: 9.46%[39m
[rank0]:[titan] 2025-06-15 10:04:50,575 - root - INFO - [31mstep:  6  [32mloss: -1.0000  [33mmemory:  3.41GiB(8.63%)  [34mtps: 7,576  [36mtflops: 29.50  [35mmfu: 9.45%[39m
[rank3]:[titan] 2025-06-15 10:04:50,561 - root - INFO - [31mstep:  6  [32mloss:  9.7298  [33mmemory: 18.39GiB(46.56%)  [34mtps: 7,580  [36mtflops: 29.51  [35mmfu: 9.46%[39m
[rank2]:[titan] 2025-06-15 10:04:52,817 - root - INFO - [31mstep:  7  [32mloss:  9.6663  [33mmemory: 18.39GiB(46.56%)  [34mtps: 7,268  [36mtflops: 28.30  [35mmfu: 9.07%[39m
[rank1]:[titan] 2025-06-15 10:04:52,831 - root - INFO - [31mstep:  7  [32mloss: -1.0000  [33mmemory:  3.41GiB(8.63%)  [34mtps: 7,259  [36mtflops: 28.27  [35mmfu: 9.06%[39m
[rank0]:[titan] 2025-06-15 10:04:52,830 - root - INFO - [31mstep:  7  [32mloss: -1.0000  [33mmemory:  3.41GiB(8.63%)  [34mtps: 7,265  [36mtflops: 28.29  [35mmfu: 9.07%[39m
[rank3]:[titan] 2025-06-15 10:04:52,816 - root - INFO - [31mstep:  7  [32mloss:  9.6663  [33mmemory: 18.39GiB(46.56%)  [34mtps: 7,267  [36mtflops: 28.29  [35mmfu: 9.07%[39m
[rank2]:[titan] 2025-06-15 10:04:54,987 - root - INFO - [31mstep:  8  [32mloss:  9.4541  [33mmemory: 18.39GiB(46.56%)  [34mtps: 7,551  [36mtflops: 29.40  [35mmfu: 9.42%[39m
[rank1]:[titan] 2025-06-15 10:04:55,003 - root - INFO - [31mstep:  8  [32mloss: -1.0000  [33mmemory:  3.41GiB(8.63%)  [34mtps: 7,547  [36mtflops: 29.38  [35mmfu: 9.42%[39m
[rank3]:[titan] 2025-06-15 10:04:54,987 - root - INFO - [31mstep:  8  [32mloss:  9.4541  [33mmemory: 18.39GiB(46.56%)  [34mtps: 7,551  [36mtflops: 29.40  [35mmfu: 9.42%[39m
[rank0]:[titan] 2025-06-15 10:04:55,004 - root - INFO - [31mstep:  8  [32mloss: -1.0000  [33mmemory:  3.41GiB(8.63%)  [34mtps: 7,541  [36mtflops: 29.36  [35mmfu: 9.41%[39m
[rank2]:[titan] 2025-06-15 10:04:57,160 - root - INFO - [31mstep:  9  [32mloss:  9.4046  [33mmemory: 18.39GiB(46.56%)  [34mtps: 7,543  [36mtflops: 29.37  [35mmfu: 9.41%[39m
[rank1]:[titan] 2025-06-15 10:04:57,175 - root - INFO - [31mstep:  9  [32mloss: -1.0000  [33mmemory:  3.41GiB(8.63%)  [34mtps: 7,543  [36mtflops: 29.37  [35mmfu: 9.41%[39m
[rank3]:[titan] 2025-06-15 10:04:57,160 - root - INFO - [31mstep:  9  [32mloss:  9.4046  [33mmemory: 18.39GiB(46.56%)  [34mtps: 7,542  [36mtflops: 29.37  [35mmfu: 9.41%[39m
[rank0]:[titan] 2025-06-15 10:04:57,174 - root - INFO - [31mstep:  9  [32mloss: -1.0000  [33mmemory:  3.41GiB(8.63%)  [34mtps: 7,550  [36mtflops: 29.40  [35mmfu: 9.42%[39m
[rank2]:[titan] 2025-06-15 10:04:59,441 - root - INFO - [31mstep: 10  [32mloss:  9.3498  [33mmemory: 18.39GiB(46.56%)  [34mtps: 7,185  [36mtflops: 27.98  [35mmfu: 8.97%[39m
[rank3]:[titan] 2025-06-15 10:04:59,441 - root - INFO - [31mstep: 10  [32mloss:  9.3498  [33mmemory: 18.39GiB(46.56%)  [34mtps: 7,185  [36mtflops: 27.97  [35mmfu: 8.97%[39m
[rank1]:[titan] 2025-06-15 10:04:59,460 - root - INFO - [31mstep: 10  [32mloss: -1.0000  [33mmemory:  3.41GiB(8.63%)  [34mtps: 7,173  [36mtflops: 27.93  [35mmfu: 8.95%[39m
[rank0]:[titan] 2025-06-15 10:04:59,461 - root - INFO - [31mstep: 10  [32mloss: -1.0000  [33mmemory:  3.41GiB(8.63%)  [34mtps: 7,167  [36mtflops: 27.90  [35mmfu: 8.94%[39m
[rank1]:[titan] 2025-06-15 10:05:01,106 - root - INFO - Dumping profiler traces at step 10
[rank0]:[titan] 2025-06-15 10:05:01,112 - root - INFO - Dumping profiler traces at step 10
[rank3]:[titan] 2025-06-15 10:05:01,190 - root - INFO - Dumping profiler traces at step 10
[rank2]:[titan] 2025-06-15 10:05:01,282 - root - INFO - Dumping profiler traces at step 10
[rank1]:[titan] 2025-06-15 10:05:01,782 - root - INFO - Finished dumping profiler traces in 0.68 seconds
[rank1]:[titan] 2025-06-15 10:05:01,783 - root - INFO - Training completed
[rank0]:[titan] 2025-06-15 10:05:01,791 - root - INFO - Finished dumping profiler traces in 0.68 seconds
[rank0]:[titan] 2025-06-15 10:05:01,791 - root - INFO - Sleeping 2 seconds for other ranks to complete
[rank3]:[titan] 2025-06-15 10:05:01,897 - root - INFO - Finished dumping profiler traces in 0.71 seconds
[rank3]:[titan] 2025-06-15 10:05:01,898 - root - INFO - Training completed
[rank2]:[titan] 2025-06-15 10:05:02,010 - root - INFO - Finished dumping profiler traces in 0.73 seconds
[rank2]:[titan] 2025-06-15 10:05:02,012 - root - INFO - Training completed
[rank0]:[titan] 2025-06-15 10:05:03,793 - root - INFO - Training completed
[rank2]:[titan] 2025-06-15 10:05:04,300 - root - INFO - Process group destroyed.
[rank3]:[titan] 2025-06-15 10:05:04,301 - root - INFO - Process group destroyed.
[rank1]:[titan] 2025-06-15 10:05:04,726 - root - INFO - Process group destroyed.
[rank0]:[titan] 2025-06-15 10:05:04,728 - root - INFO - Process group destroyed.
