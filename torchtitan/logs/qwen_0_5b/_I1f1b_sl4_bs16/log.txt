
============================================================
- exec time: 2025-06-15 10:01:26
- command: ./run_train.sh --parallelism.pipeline_parallel_degree=4 --parallelism.tensor_parallel_degree=1 --parallelism.pipeline_parallel_schedule=Interleaved1F1B --parallelism.pipeline_parallel_num_stages_per_rank=2 --training.seq_len=4096 --training.local_batch_size=16 --job.dump_folder=logs/qwen_0_5b/_I1f1b_sl4_bs16 --profiling.save_traces_folder= --metrics.save_tb_folder=
- config file: /home2/yeonjae/tp_partition/torchtitan/torchtitan/models/llama3/train_configs/qwen_0_5b.toml
- overrides: {'pipeline_parallel_degree': 4, 'tensor_parallel_degree': 1, 'pipeline_parallel_schedule': 'Interleaved1F1B', 'pipeline_parallel_num_stages_per_rank': 2, 'seq_len': 4096, 'local_batch_size': 16}
============================================================

--- STDOUT ---

--- STDERR ---
+ NGPU=4
+ export LOG_RANK=0,1,2,3
+ LOG_RANK=0,1,2,3
+ CONFIG_FILE=/home2/yeonjae/tp_partition/torchtitan/torchtitan/models/llama3/train_configs/qwen_0_5b.toml
+ overrides=
+ '[' 9 -ne 0 ']'
+ overrides='--parallelism.pipeline_parallel_degree=4 --parallelism.tensor_parallel_degree=1 --parallelism.pipeline_parallel_schedule=Interleaved1F1B --parallelism.pipeline_parallel_num_stages_per_rank=2 --training.seq_len=4096 --training.local_batch_size=16 --job.dump_folder=logs/qwen_0_5b/_I1f1b_sl4_bs16 --profiling.save_traces_folder= --metrics.save_tb_folder='
+ TORCHFT_LIGHTHOUSE=http://localhost:29510
+ PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
+ TORCHFT_LIGHTHOUSE=http://localhost:29510
+ torchrun --nproc_per_node=4 --rdzv_backend c10d --rdzv_endpoint=localhost:0 --local-ranks-filter 0,1,2,3 --role rank --tee 3 -m torchtitan.train --job.config_file /home2/yeonjae/tp_partition/torchtitan/torchtitan/models/llama3/train_configs/qwen_0_5b.toml --parallelism.pipeline_parallel_degree=4 --parallelism.tensor_parallel_degree=1 --parallelism.pipeline_parallel_schedule=Interleaved1F1B --parallelism.pipeline_parallel_num_stages_per_rank=2 --training.seq_len=4096 --training.local_batch_size=16 --job.dump_folder=logs/qwen_0_5b/_I1f1b_sl4_bs16 --profiling.save_traces_folder= --metrics.save_tb_folder=
W0615 10:01:27.766000 2095665 torch/distributed/run.py:766] 
W0615 10:01:27.766000 2095665 torch/distributed/run.py:766] *****************************************
W0615 10:01:27.766000 2095665 torch/distributed/run.py:766] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0615 10:01:27.766000 2095665 torch/distributed/run.py:766] *****************************************
[rank1]:[titan] 2025-06-15 10:01:33,419 - root - INFO - Starting job: Llama 3 8B training
[rank0]:[titan] 2025-06-15 10:01:33,467 - root - INFO - Starting job: Llama 3 8B training
[rank2]:[titan] 2025-06-15 10:01:33,540 - root - INFO - Starting job: Llama 3 8B training
[rank0]:[titan] 2025-06-15 10:01:33,633 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank0]:[titan] 2025-06-15 10:01:33,638 - root - INFO - Building 1-D device mesh with ['pp'], [4]
[rank0]:[titan] 2025-06-15 10:01:33,639 - root - INFO - [GC] Initial GC collection. 0.00 seconds.
[rank3]:[titan] 2025-06-15 10:01:33,748 - root - INFO - Starting job: Llama 3 8B training
[rank1]:[titan] 2025-06-15 10:01:34,098 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank1]:[titan] 2025-06-15 10:01:34,101 - root - INFO - Building 1-D device mesh with ['pp'], [4]
[rank1]:[titan] 2025-06-15 10:01:34,103 - root - INFO - [GC] Initial GC collection. 0.00 seconds.
[rank2]:[titan] 2025-06-15 10:01:34,345 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank2]:[titan] 2025-06-15 10:01:34,349 - root - INFO - Building 1-D device mesh with ['pp'], [4]
[rank2]:[titan] 2025-06-15 10:01:34,351 - root - INFO - [GC] Initial GC collection. 0.00 seconds.
[rank3]:[titan] 2025-06-15 10:01:34,525 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank3]:[titan] 2025-06-15 10:01:34,528 - root - INFO - Building 1-D device mesh with ['pp'], [4]
[rank3]:[titan] 2025-06-15 10:01:34,530 - root - INFO - [GC] Initial GC collection. 0.00 seconds.
[rank0]:[rank0]:[W615 10:01:34.096340517 Utils.hpp:137] Warning: Environment variable TORCH_NCCL_TRACE_BUFFER_SIZE is deprecated; use TORCH_FR_BUFFER_SIZE instead (function operator())
[rank2]:[rank2]:[W615 10:01:34.101528098 Utils.hpp:137] Warning: Environment variable TORCH_NCCL_TRACE_BUFFER_SIZE is deprecated; use TORCH_FR_BUFFER_SIZE instead (function operator())
[rank1]:[rank1]:[W615 10:01:34.107094609 Utils.hpp:137] Warning: Environment variable TORCH_NCCL_TRACE_BUFFER_SIZE is deprecated; use TORCH_FR_BUFFER_SIZE instead (function operator())
[rank3]:[rank3]:[W615 10:01:34.107902659 Utils.hpp:137] Warning: Environment variable TORCH_NCCL_TRACE_BUFFER_SIZE is deprecated; use TORCH_FR_BUFFER_SIZE instead (function operator())
[rank1]:[titan] 2025-06-15 10:01:35,232 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
[rank1]:[titan] 2025-06-15 10:01:35,233 - root - INFO - Preparing c4 dataset from allenai/c4
[rank0]:[titan] 2025-06-15 10:01:35,241 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
[rank0]:[titan] 2025-06-15 10:01:35,241 - root - INFO - Preparing c4 dataset from allenai/c4
[rank3]:[titan] 2025-06-15 10:01:35,258 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
[rank3]:[titan] 2025-06-15 10:01:35,258 - root - INFO - Preparing c4 dataset from allenai/c4
[rank2]:[titan] 2025-06-15 10:01:35,238 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
[rank2]:[titan] 2025-06-15 10:01:35,239 - root - INFO - Preparing c4 dataset from allenai/c4
[rank3]:[titan] 2025-06-15 10:01:51,335 - root - INFO - Building llama3 0.5B with TransformerModelArgs(_enforced='This field is used to enforce all fields have defaults.', dim=896, n_layers=24, n_heads=14, n_kv_heads=2, vocab_size=128256, multiple_of=256, intermediate_size=4864, ffn_dim_multiplier=None, norm_eps=1e-05, rope_theta=500000, max_seq_len=4096, depth_init=True, use_flex_attn=False, attn_mask_type='causal', eos_id=128001)
[rank0]:[titan] 2025-06-15 10:01:51,519 - root - INFO - Building llama3 0.5B with TransformerModelArgs(_enforced='This field is used to enforce all fields have defaults.', dim=896, n_layers=24, n_heads=14, n_kv_heads=2, vocab_size=128256, multiple_of=256, intermediate_size=4864, ffn_dim_multiplier=None, norm_eps=1e-05, rope_theta=500000, max_seq_len=4096, depth_init=True, use_flex_attn=False, attn_mask_type='causal', eos_id=128001)
[rank3]:[titan] 2025-06-15 10:01:51,571 - root - INFO - TensorBoard logging enabled. Logs will be saved at logs/qwen_0_5b/_I1f1b_sl4_bs16/20250615-1001
[rank3]:[titan] 2025-06-15 10:01:51,572 - root - INFO - CUDA capacity: NVIDIA A100-PCIE-40GB with 39.50GiB memory
[rank3]:[titan] 2025-06-15 10:01:51,612 - root - INFO - [34mModel llama3 0.5B [31msize: 587,705,216 total parameters[39m
[rank3]:[titan] 2025-06-15 10:01:51,612 - root - INFO - Compiling the loss function with torch.compile
[rank3]:[titan] 2025-06-15 10:01:51,659 - root - INFO - No 'pipeline_parallel_split_points' provided. Here is the auto-generated split, which may be sub-optimal: ['layers.5', 'layers.12', 'layers.19'].
[rank3]:[titan] 2025-06-15 10:01:51,677 - root - INFO - PP rank 3 is building stage_idx 3 with start_layer layers.19, stop_layer None
[rank3]:[titan] 2025-06-15 10:01:51,677 - root - INFO - Applied full activation checkpointing to the model
[rank3]:[titan] 2025-06-15 10:01:51,679 - root - INFO - Compiling each TransformerBlock with torch.compile
[rank3]:[titan] 2025-06-15 10:01:51,679 - root - INFO - Using pipeline schedule Interleaved1F1B with 16 microbatches and 4 stages.
[rank0]:[titan] 2025-06-15 10:01:51,754 - root - INFO - CUDA capacity: NVIDIA A100-PCIE-40GB with 39.50GiB memory
[rank0]:[titan] 2025-06-15 10:01:51,795 - root - INFO - [34mModel llama3 0.5B [31msize: 587,705,216 total parameters[39m
[rank0]:[titan] 2025-06-15 10:01:51,795 - root - INFO - Compiling the loss function with torch.compile
[rank3]:[titan] 2025-06-15 10:01:51,833 - root - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank3]:[titan] 2025-06-15 10:01:51,833 - root - INFO - CUDA memory usage for model: 0.71GiB(1.80%)
[rank3]:[titan] 2025-06-15 10:01:51,834 - root - INFO - Trainer is initialized with local batch size 16, global batch size 16, gradient accumulation steps 1, sequence length 4096, total steps 10 (warmup 2).
[rank3]:[titan] 2025-06-15 10:01:51,834 - root - INFO - Training starts at step 1.
[rank3]:[titan] 2025-06-15 10:01:51,834 - root - INFO - Profiling active. Traces will be saved at logs/qwen_0_5b/_I1f1b_sl4_bs16/
[rank0]:[titan] 2025-06-15 10:01:51,860 - root - INFO - No 'pipeline_parallel_split_points' provided. Here is the auto-generated split, which may be sub-optimal: ['layers.5', 'layers.12', 'layers.19'].
[rank0]:[titan] 2025-06-15 10:01:51,878 - root - INFO - PP rank 0 is building stage_idx 0 with start_layer None, stop_layer layers.5
[rank0]:[titan] 2025-06-15 10:01:51,878 - root - INFO - Applied full activation checkpointing to the model
[rank0]:[titan] 2025-06-15 10:01:51,880 - root - INFO - Compiling each TransformerBlock with torch.compile
[rank0]:[titan] 2025-06-15 10:01:51,880 - root - INFO - Using pipeline schedule Interleaved1F1B with 16 microbatches and 4 stages.
[rank0]:[titan] 2025-06-15 10:01:52,037 - root - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank0]:[titan] 2025-06-15 10:01:52,037 - root - INFO - CUDA memory usage for model: 0.71GiB(1.80%)
[rank0]:[titan] 2025-06-15 10:01:52,045 - root - INFO - Trainer is initialized with local batch size 16, global batch size 16, gradient accumulation steps 1, sequence length 4096, total steps 10 (warmup 2).
[rank0]:[titan] 2025-06-15 10:01:52,045 - root - INFO - Training starts at step 1.
[rank0]:[titan] 2025-06-15 10:01:52,045 - root - INFO - Profiling active. Traces will be saved at logs/qwen_0_5b/_I1f1b_sl4_bs16/
[rank1]:[titan] 2025-06-15 10:01:52,578 - root - INFO - Building llama3 0.5B with TransformerModelArgs(_enforced='This field is used to enforce all fields have defaults.', dim=896, n_layers=24, n_heads=14, n_kv_heads=2, vocab_size=128256, multiple_of=256, intermediate_size=4864, ffn_dim_multiplier=None, norm_eps=1e-05, rope_theta=500000, max_seq_len=4096, depth_init=True, use_flex_attn=False, attn_mask_type='causal', eos_id=128001)
[rank1]:[titan] 2025-06-15 10:01:52,816 - root - INFO - CUDA capacity: NVIDIA A100-PCIE-40GB with 39.50GiB memory
[rank1]:[titan] 2025-06-15 10:01:52,854 - root - INFO - [34mModel llama3 0.5B [31msize: 587,705,216 total parameters[39m
[rank1]:[titan] 2025-06-15 10:01:52,854 - root - INFO - Compiling the loss function with torch.compile
[rank1]:[titan] 2025-06-15 10:01:52,902 - root - INFO - No 'pipeline_parallel_split_points' provided. Here is the auto-generated split, which may be sub-optimal: ['layers.5', 'layers.12', 'layers.19'].
[rank1]:[titan] 2025-06-15 10:01:52,920 - root - INFO - PP rank 1 is building stage_idx 1 with start_layer layers.5, stop_layer layers.12
[rank1]:[titan] 2025-06-15 10:01:52,920 - root - INFO - Applied full activation checkpointing to the model
[rank1]:[titan] 2025-06-15 10:01:52,922 - root - INFO - Compiling each TransformerBlock with torch.compile
[rank1]:[titan] 2025-06-15 10:01:52,923 - root - INFO - Using pipeline schedule Interleaved1F1B with 16 microbatches and 4 stages.
[rank1]:[titan] 2025-06-15 10:01:53,078 - root - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank1]:[titan] 2025-06-15 10:01:53,079 - root - INFO - CUDA memory usage for model: 0.40GiB(1.01%)
[rank1]:[titan] 2025-06-15 10:01:53,080 - root - INFO - Trainer is initialized with local batch size 16, global batch size 16, gradient accumulation steps 1, sequence length 4096, total steps 10 (warmup 2).
[rank1]:[titan] 2025-06-15 10:01:53,081 - root - INFO - Training starts at step 1.
[rank1]:[titan] 2025-06-15 10:01:53,081 - root - INFO - Profiling active. Traces will be saved at logs/qwen_0_5b/_I1f1b_sl4_bs16/
[rank2]:[titan] 2025-06-15 10:01:53,163 - root - INFO - Building llama3 0.5B with TransformerModelArgs(_enforced='This field is used to enforce all fields have defaults.', dim=896, n_layers=24, n_heads=14, n_kv_heads=2, vocab_size=128256, multiple_of=256, intermediate_size=4864, ffn_dim_multiplier=None, norm_eps=1e-05, rope_theta=500000, max_seq_len=4096, depth_init=True, use_flex_attn=False, attn_mask_type='causal', eos_id=128001)
[rank2]:[titan] 2025-06-15 10:01:53,397 - root - INFO - CUDA capacity: NVIDIA A100-PCIE-40GB with 39.50GiB memory
[rank2]:[titan] 2025-06-15 10:01:53,437 - root - INFO - [34mModel llama3 0.5B [31msize: 587,705,216 total parameters[39m
[rank2]:[titan] 2025-06-15 10:01:53,437 - root - INFO - Compiling the loss function with torch.compile
[rank2]:[titan] 2025-06-15 10:01:53,485 - root - INFO - No 'pipeline_parallel_split_points' provided. Here is the auto-generated split, which may be sub-optimal: ['layers.5', 'layers.12', 'layers.19'].
[rank2]:[titan] 2025-06-15 10:01:53,503 - root - INFO - PP rank 2 is building stage_idx 2 with start_layer layers.12, stop_layer layers.19
[rank2]:[titan] 2025-06-15 10:01:53,504 - root - INFO - Applied full activation checkpointing to the model
[rank2]:[titan] 2025-06-15 10:01:53,506 - root - INFO - Compiling each TransformerBlock with torch.compile
[rank2]:[titan] 2025-06-15 10:01:53,506 - root - INFO - Using pipeline schedule Interleaved1F1B with 16 microbatches and 4 stages.
[rank2]:[titan] 2025-06-15 10:01:53,659 - root - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank2]:[titan] 2025-06-15 10:01:53,660 - root - INFO - CUDA memory usage for model: 0.40GiB(1.01%)
[rank2]:[titan] 2025-06-15 10:01:53,660 - root - INFO - Trainer is initialized with local batch size 16, global batch size 16, gradient accumulation steps 1, sequence length 4096, total steps 10 (warmup 2).
[rank2]:[titan] 2025-06-15 10:01:53,660 - root - INFO - Training starts at step 1.
[rank2]:[titan] 2025-06-15 10:01:53,661 - root - INFO - Profiling active. Traces will be saved at logs/qwen_0_5b/_I1f1b_sl4_bs16/
[rank0]:/home2/yeonjae/tp_partition/pytorch/torch/_inductor/lowering.py:1884: UserWarning: Torchinductor does not support code generation for complex operators. Performance may be worse than eager.
[rank0]:  warnings.warn(
[rank1]:/home2/yeonjae/tp_partition/pytorch/torch/_inductor/lowering.py:1884: UserWarning: Torchinductor does not support code generation for complex operators. Performance may be worse than eager.
[rank1]:  warnings.warn(
[rank2]:/home2/yeonjae/tp_partition/pytorch/torch/_inductor/lowering.py:1884: UserWarning: Torchinductor does not support code generation for complex operators. Performance may be worse than eager.
[rank2]:  warnings.warn(
[rank3]:/home2/yeonjae/tp_partition/pytorch/torch/_inductor/lowering.py:1884: UserWarning: Torchinductor does not support code generation for complex operators. Performance may be worse than eager.
[rank3]:  warnings.warn(
[rank0]:[titan] 2025-06-15 10:02:06,046 - root - INFO - [31mstep:  1  [32mloss: -1.0000  [33mmemory:  3.15GiB(7.97%)  [34mtps: 1,150  [36mtflops: 4.48  [35mmfu: 1.43%[39m
[rank0]:[titan] 2025-06-15 10:02:06,047 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:01:40
[rank1]:[titan] 2025-06-15 10:02:06,024 - root - INFO - [31mstep:  1  [32mloss: -1.0000  [33mmemory:  2.25GiB(5.71%)  [34mtps: 1,244  [36mtflops: 4.84  [35mmfu: 1.55%[39m
[rank1]:[titan] 2025-06-15 10:02:06,024 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:01:40
[rank3]:[titan] 2025-06-15 10:02:06,019 - root - INFO - [31mstep:  1  [32mloss: 12.2404  [33mmemory: 33.11GiB(83.82%)  [34mtps: 1,137  [36mtflops: 4.43  [35mmfu: 1.42%[39m
[rank3]:[titan] 2025-06-15 10:02:06,020 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:01:40
[rank2]:[titan] 2025-06-15 10:02:06,025 - root - INFO - [31mstep:  1  [32mloss: -1.0000  [33mmemory:  2.27GiB(5.76%)  [34mtps: 1,302  [36mtflops: 5.07  [35mmfu: 1.62%[39m
[rank2]:[titan] 2025-06-15 10:02:06,026 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:01:40
[rank0]:[titan] 2025-06-15 10:02:07,371 - root - INFO - [31mstep:  2  [32mloss: -1.0000  [33mmemory:  4.12GiB(10.44%)  [34mtps: 12,372  [36mtflops: 48.17  [35mmfu: 15.44%[39m
[rank3]:[titan] 2025-06-15 10:02:07,373 - root - INFO - [31mstep:  2  [32mloss: 11.8100  [33mmemory: 34.32GiB(86.89%)  [34mtps: 12,117  [36mtflops: 47.18  [35mmfu: 15.12%[39m
[rank1]:[titan] 2025-06-15 10:02:07,369 - root - INFO - [31mstep:  2  [32mloss: -1.0000  [33mmemory:  3.02GiB(7.65%)  [34mtps: 12,182  [36mtflops: 47.43  [35mmfu: 15.20%[39m
[rank2]:[titan] 2025-06-15 10:02:07,369 - root - INFO - [31mstep:  2  [32mloss: -1.0000  [33mmemory:  2.84GiB(7.20%)  [34mtps: 12,196  [36mtflops: 47.49  [35mmfu: 15.22%[39m
[rank0]:[titan] 2025-06-15 10:02:08,688 - root - INFO - [31mstep:  3  [32mloss: -1.0000  [33mmemory:  4.12GiB(10.44%)  [34mtps: 12,450  [36mtflops: 48.48  [35mmfu: 15.54%[39m
[rank3]:[titan] 2025-06-15 10:02:08,688 - root - INFO - [31mstep:  3  [32mloss: 10.8549  [33mmemory: 35.18GiB(89.07%)  [34mtps: 12,465  [36mtflops: 48.53  [35mmfu: 15.56%[39m
[rank1]:[titan] 2025-06-15 10:02:08,685 - root - INFO - [31mstep:  3  [32mloss: -1.0000  [33mmemory:  3.02GiB(7.65%)  [34mtps: 12,450  [36mtflops: 48.48  [35mmfu: 15.54%[39m
[rank2]:[titan] 2025-06-15 10:02:08,685 - root - INFO - [31mstep:  3  [32mloss: -1.0000  [33mmemory:  2.84GiB(7.20%)  [34mtps: 12,450  [36mtflops: 48.48  [35mmfu: 15.54%[39m
[rank0]:[titan] 2025-06-15 10:02:10,004 - root - INFO - [31mstep:  4  [32mloss: -1.0000  [33mmemory:  4.12GiB(10.44%)  [34mtps: 12,452  [36mtflops: 48.49  [35mmfu: 15.54%[39m
[rank3]:[titan] 2025-06-15 10:02:10,004 - root - INFO - [31mstep:  4  [32mloss: 10.0634  [33mmemory: 35.18GiB(89.07%)  [34mtps: 12,457  [36mtflops: 48.50  [35mmfu: 15.55%[39m
[rank1]:[titan] 2025-06-15 10:02:10,001 - root - INFO - [31mstep:  4  [32mloss: -1.0000  [33mmemory:  3.02GiB(7.65%)  [34mtps: 12,453  [36mtflops: 48.49  [35mmfu: 15.54%[39m
[rank2]:[titan] 2025-06-15 10:02:10,001 - root - INFO - [31mstep:  4  [32mloss: -1.0000  [33mmemory:  2.84GiB(7.20%)  [34mtps: 12,453  [36mtflops: 48.49  [35mmfu: 15.54%[39m
[rank0]:[titan] 2025-06-15 10:02:11,320 - root - INFO - [31mstep:  5  [32mloss: -1.0000  [33mmemory:  4.12GiB(10.44%)  [34mtps: 12,456  [36mtflops: 48.50  [35mmfu: 15.54%[39m
[rank3]:[titan] 2025-06-15 10:02:11,320 - root - INFO - [31mstep:  5  [32mloss:  9.7739  [33mmemory: 35.18GiB(89.07%)  [34mtps: 12,459  [36mtflops: 48.51  [35mmfu: 15.55%[39m
[rank1]:[titan] 2025-06-15 10:02:11,317 - root - INFO - [31mstep:  5  [32mloss: -1.0000  [33mmemory:  3.02GiB(7.65%)  [34mtps: 12,456  [36mtflops: 48.50  [35mmfu: 15.54%[39m
[rank2]:[titan] 2025-06-15 10:02:11,317 - root - INFO - [31mstep:  5  [32mloss: -1.0000  [33mmemory:  2.84GiB(7.20%)  [34mtps: 12,456  [36mtflops: 48.50  [35mmfu: 15.54%[39m
[rank0]:[titan] 2025-06-15 10:02:12,640 - root - INFO - [31mstep:  6  [32mloss: -1.0000  [33mmemory:  4.12GiB(10.44%)  [34mtps: 12,449  [36mtflops: 48.47  [35mmfu: 15.54%[39m
[rank3]:[titan] 2025-06-15 10:02:12,641 - root - INFO - [31mstep:  6  [32mloss:  9.6579  [33mmemory: 35.18GiB(89.07%)  [34mtps: 12,449  [36mtflops: 48.47  [35mmfu: 15.54%[39m
[rank1]:[titan] 2025-06-15 10:02:12,638 - root - INFO - [31mstep:  6  [32mloss: -1.0000  [33mmemory:  3.02GiB(7.65%)  [34mtps: 12,472  [36mtflops: 48.56  [35mmfu: 15.57%[39m
[rank2]:[titan] 2025-06-15 10:02:12,638 - root - INFO - [31mstep:  6  [32mloss: -1.0000  [33mmemory:  2.84GiB(7.20%)  [34mtps: 12,408  [36mtflops: 48.31  [35mmfu: 15.48%[39m
[rank0]:[titan] 2025-06-15 10:02:14,052 - root - INFO - [31mstep:  7  [32mloss: -1.0000  [33mmemory:  4.12GiB(10.44%)  [34mtps: 11,612  [36mtflops: 45.21  [35mmfu: 14.49%[39m
[rank1]:[titan] 2025-06-15 10:02:14,049 - root - INFO - [31mstep:  7  [32mloss: -1.0000  [33mmemory:  3.02GiB(7.65%)  [34mtps: 11,612  [36mtflops: 45.22  [35mmfu: 14.49%[39m
[rank3]:[titan] 2025-06-15 10:02:14,052 - root - INFO - [31mstep:  7  [32mloss:  9.5432  [33mmemory: 35.18GiB(89.07%)  [34mtps: 11,616  [36mtflops: 45.23  [35mmfu: 14.50%[39m
[rank2]:[titan] 2025-06-15 10:02:14,049 - root - INFO - [31mstep:  7  [32mloss: -1.0000  [33mmemory:  2.84GiB(7.20%)  [34mtps: 11,612  [36mtflops: 45.21  [35mmfu: 14.49%[39m
[rank1]:[titan] 2025-06-15 10:02:15,377 - root - INFO - [31mstep:  8  [32mloss: -1.0000  [33mmemory:  3.02GiB(7.65%)  [34mtps: 12,348  [36mtflops: 48.08  [35mmfu: 15.41%[39m
[rank0]:[titan] 2025-06-15 10:02:15,379 - root - INFO - [31mstep:  8  [32mloss: -1.0000  [33mmemory:  4.12GiB(10.44%)  [34mtps: 12,347  [36mtflops: 48.08  [35mmfu: 15.41%[39m
[rank3]:[titan] 2025-06-15 10:02:15,379 - root - INFO - [31mstep:  8  [32mloss:  9.3414  [33mmemory: 35.18GiB(89.07%)  [34mtps: 12,352  [36mtflops: 48.09  [35mmfu: 15.42%[39m
[rank2]:[titan] 2025-06-15 10:02:15,377 - root - INFO - [31mstep:  8  [32mloss: -1.0000  [33mmemory:  2.84GiB(7.20%)  [34mtps: 12,347  [36mtflops: 48.07  [35mmfu: 15.41%[39m
[rank2]:[titan] 2025-06-15 10:02:16,698 - root - INFO - [31mstep:  9  [32mloss: -1.0000  [33mmemory:  2.84GiB(7.20%)  [34mtps: 12,404  [36mtflops: 48.30  [35mmfu: 15.48%[39m
[rank1]:[titan] 2025-06-15 10:02:16,698 - root - INFO - [31mstep:  9  [32mloss: -1.0000  [33mmemory:  3.02GiB(7.65%)  [34mtps: 12,404  [36mtflops: 48.30  [35mmfu: 15.48%[39m
[rank3]:[titan] 2025-06-15 10:02:16,701 - root - INFO - [31mstep:  9  [32mloss:  9.3070  [33mmemory: 35.18GiB(89.07%)  [34mtps: 12,407  [36mtflops: 48.31  [35mmfu: 15.48%[39m
[rank0]:[titan] 2025-06-15 10:02:16,700 - root - INFO - [31mstep:  9  [32mloss: -1.0000  [33mmemory:  4.12GiB(10.44%)  [34mtps: 12,404  [36mtflops: 48.30  [35mmfu: 15.48%[39m
[rank1]:[titan] 2025-06-15 10:02:18,015 - root - INFO - [31mstep: 10  [32mloss: -1.0000  [33mmemory:  3.02GiB(7.65%)  [34mtps: 12,446  [36mtflops: 48.46  [35mmfu: 15.53%[39m
[rank0]:[titan] 2025-06-15 10:02:18,018 - root - INFO - [31mstep: 10  [32mloss: -1.0000  [33mmemory:  4.12GiB(10.44%)  [34mtps: 12,442  [36mtflops: 48.44  [35mmfu: 15.53%[39m
[rank3]:[titan] 2025-06-15 10:02:18,018 - root - INFO - [31mstep: 10  [32mloss:  9.2334  [33mmemory: 35.18GiB(89.07%)  [34mtps: 12,449  [36mtflops: 48.47  [35mmfu: 15.54%[39m
[rank2]:[titan] 2025-06-15 10:02:18,015 - root - INFO - [31mstep: 10  [32mloss: -1.0000  [33mmemory:  2.84GiB(7.20%)  [34mtps: 12,444  [36mtflops: 48.45  [35mmfu: 15.53%[39m
[rank0]:[titan] 2025-06-15 10:02:18,445 - root - INFO - Dumping profiler traces at step 10
[rank3]:[titan] 2025-06-15 10:02:18,448 - root - INFO - Dumping profiler traces at step 10
[rank2]:[titan] 2025-06-15 10:02:18,597 - root - INFO - Dumping profiler traces at step 10
[rank1]:[titan] 2025-06-15 10:02:18,601 - root - INFO - Dumping profiler traces at step 10
[rank0]:[titan] 2025-06-15 10:02:18,636 - root - INFO - Finished dumping profiler traces in 0.19 seconds
[rank0]:[titan] 2025-06-15 10:02:18,636 - root - INFO - Sleeping 2 seconds for other ranks to complete
[rank3]:[titan] 2025-06-15 10:02:18,631 - root - INFO - Finished dumping profiler traces in 0.18 seconds
[rank3]:[titan] 2025-06-15 10:02:18,633 - root - INFO - Training completed
[rank1]:[titan] 2025-06-15 10:02:18,851 - root - INFO - Finished dumping profiler traces in 0.25 seconds
[rank1]:[titan] 2025-06-15 10:02:18,851 - root - INFO - Training completed
[rank2]:[titan] 2025-06-15 10:02:18,843 - root - INFO - Finished dumping profiler traces in 0.25 seconds
[rank2]:[titan] 2025-06-15 10:02:18,844 - root - INFO - Training completed
[rank2]:[titan] 2025-06-15 10:02:19,325 - root - INFO - Process group destroyed.
[rank3]:[titan] 2025-06-15 10:02:20,692 - root - INFO - Process group destroyed.
[rank0]:[titan] 2025-06-15 10:02:20,639 - root - INFO - Training completed
[rank0]:[titan] 2025-06-15 10:02:20,725 - root - INFO - Process group destroyed.
[rank1]:[titan] 2025-06-15 10:02:20,709 - root - INFO - Process group destroyed.
