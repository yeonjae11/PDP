
============================================================
- exec time: 2025-06-15 09:51:09
- command: ./run_train.sh --parallelism.tensor_parallel_degree=2 --parallelism.pipeline_parallel_degree=2 --parallelism.pipeline_parallel_schedule=Interleaved1F1B --parallelism.pipeline_parallel_num_stages_per_rank=2 --training.seq_len=2048 --training.local_batch_size=8 --job.dump_folder=logs/qwen_0_5b/_tp_I1f1b_sl2_bs8 --profiling.save_traces_folder= --metrics.save_tb_folder=
- config file: /home2/yeonjae/tp_partition/torchtitan/torchtitan/models/llama3/train_configs/qwen_0_5b.toml
- overrides: {'tensor_parallel_degree': 2, 'pipeline_parallel_degree': 2, 'pipeline_parallel_schedule': 'Interleaved1F1B', 'pipeline_parallel_num_stages_per_rank': 2, 'seq_len': 2048, 'local_batch_size': 8}
============================================================

--- STDOUT ---

--- STDERR ---
+ NGPU=4
+ export LOG_RANK=0,1,2,3
+ LOG_RANK=0,1,2,3
+ CONFIG_FILE=/home2/yeonjae/tp_partition/torchtitan/torchtitan/models/llama3/train_configs/qwen_0_5b.toml
+ overrides=
+ '[' 9 -ne 0 ']'
+ overrides='--parallelism.tensor_parallel_degree=2 --parallelism.pipeline_parallel_degree=2 --parallelism.pipeline_parallel_schedule=Interleaved1F1B --parallelism.pipeline_parallel_num_stages_per_rank=2 --training.seq_len=2048 --training.local_batch_size=8 --job.dump_folder=logs/qwen_0_5b/_tp_I1f1b_sl2_bs8 --profiling.save_traces_folder= --metrics.save_tb_folder='
+ TORCHFT_LIGHTHOUSE=http://localhost:29510
+ PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
+ TORCHFT_LIGHTHOUSE=http://localhost:29510
+ torchrun --nproc_per_node=4 --rdzv_backend c10d --rdzv_endpoint=localhost:0 --local-ranks-filter 0,1,2,3 --role rank --tee 3 -m torchtitan.train --job.config_file /home2/yeonjae/tp_partition/torchtitan/torchtitan/models/llama3/train_configs/qwen_0_5b.toml --parallelism.tensor_parallel_degree=2 --parallelism.pipeline_parallel_degree=2 --parallelism.pipeline_parallel_schedule=Interleaved1F1B --parallelism.pipeline_parallel_num_stages_per_rank=2 --training.seq_len=2048 --training.local_batch_size=8 --job.dump_folder=logs/qwen_0_5b/_tp_I1f1b_sl2_bs8 --profiling.save_traces_folder= --metrics.save_tb_folder=
W0615 09:51:10.482000 2074777 torch/distributed/run.py:766] 
W0615 09:51:10.482000 2074777 torch/distributed/run.py:766] *****************************************
W0615 09:51:10.482000 2074777 torch/distributed/run.py:766] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0615 09:51:10.482000 2074777 torch/distributed/run.py:766] *****************************************
[rank0]:[titan] 2025-06-15 09:51:16,388 - root - INFO - Starting job: Llama 3 8B training
[rank2]:[titan] 2025-06-15 09:51:16,362 - root - INFO - Starting job: Llama 3 8B training
[rank1]:[titan] 2025-06-15 09:51:16,421 - root - INFO - Starting job: Llama 3 8B training
[rank3]:[titan] 2025-06-15 09:51:16,468 - root - INFO - Starting job: Llama 3 8B training
[rank0]:[titan] 2025-06-15 09:51:16,513 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank0]:[titan] 2025-06-15 09:51:16,516 - root - INFO - Building 2-D device mesh with ['pp', 'tp'], [2, 2]
[rank0]:[titan] 2025-06-15 09:51:16,519 - root - INFO - [GC] Initial GC collection. 0.00 seconds.
[rank2]:[titan] 2025-06-15 09:51:17,197 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank2]:[titan] 2025-06-15 09:51:17,201 - root - INFO - Building 2-D device mesh with ['pp', 'tp'], [2, 2]
[rank2]:[titan] 2025-06-15 09:51:17,203 - root - INFO - [GC] Initial GC collection. 0.00 seconds.
[rank1]:[titan] 2025-06-15 09:51:17,472 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank1]:[titan] 2025-06-15 09:51:17,476 - root - INFO - Building 2-D device mesh with ['pp', 'tp'], [2, 2]
[rank1]:[titan] 2025-06-15 09:51:17,478 - root - INFO - [GC] Initial GC collection. 0.00 seconds.
[rank3]:[titan] 2025-06-15 09:51:17,458 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank3]:[titan] 2025-06-15 09:51:17,483 - root - INFO - Building 2-D device mesh with ['pp', 'tp'], [2, 2]
[rank3]:[titan] 2025-06-15 09:51:17,489 - root - INFO - [GC] Initial GC collection. 0.00 seconds.
[rank0]:[rank0]:[W615 09:51:17.081192921 Utils.hpp:137] Warning: Environment variable TORCH_NCCL_TRACE_BUFFER_SIZE is deprecated; use TORCH_FR_BUFFER_SIZE instead (function operator())
[rank1]:[rank1]:[W615 09:51:17.070130309 Utils.hpp:137] Warning: Environment variable TORCH_NCCL_TRACE_BUFFER_SIZE is deprecated; use TORCH_FR_BUFFER_SIZE instead (function operator())
[rank3]:[rank3]:[W615 09:51:17.069984479 Utils.hpp:137] Warning: Environment variable TORCH_NCCL_TRACE_BUFFER_SIZE is deprecated; use TORCH_FR_BUFFER_SIZE instead (function operator())
[rank2]:[rank2]:[W615 09:51:17.069409999 Utils.hpp:137] Warning: Environment variable TORCH_NCCL_TRACE_BUFFER_SIZE is deprecated; use TORCH_FR_BUFFER_SIZE instead (function operator())
[rank1]:[titan] 2025-06-15 09:51:18,196 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
[rank1]:[titan] 2025-06-15 09:51:18,196 - root - INFO - Preparing c4 dataset from allenai/c4
[rank0]:[titan] 2025-06-15 09:51:18,217 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
[rank0]:[titan] 2025-06-15 09:51:18,217 - root - INFO - Preparing c4 dataset from allenai/c4
[rank3]:[titan] 2025-06-15 09:51:18,220 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
[rank3]:[titan] 2025-06-15 09:51:18,220 - root - INFO - Preparing c4 dataset from allenai/c4
[rank2]:[titan] 2025-06-15 09:51:18,237 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
[rank2]:[titan] 2025-06-15 09:51:18,237 - root - INFO - Preparing c4 dataset from allenai/c4
[rank0]:[titan] 2025-06-15 09:51:34,870 - root - INFO - Building llama3 0.5B with TransformerModelArgs(_enforced='This field is used to enforce all fields have defaults.', dim=896, n_layers=24, n_heads=14, n_kv_heads=2, vocab_size=128256, multiple_of=256, intermediate_size=4864, ffn_dim_multiplier=None, norm_eps=1e-05, rope_theta=500000, max_seq_len=2048, depth_init=True, use_flex_attn=False, attn_mask_type='causal', eos_id=128001)
[rank0]:[titan] 2025-06-15 09:51:35,100 - root - INFO - CUDA capacity: NVIDIA A100-PCIE-40GB with 39.50GiB memory
[rank0]:[titan] 2025-06-15 09:51:35,140 - root - INFO - [34mModel llama3 0.5B [31msize: 587,705,216 total parameters[39m
[rank0]:[titan] 2025-06-15 09:51:35,140 - root - INFO - Compiling the loss function with torch.compile
[rank0]:[titan] 2025-06-15 09:51:35,187 - root - INFO - No 'pipeline_parallel_split_points' provided. Here is the auto-generated split, which may be sub-optimal: ['layers.12'].
[rank0]:[titan] 2025-06-15 09:51:35,204 - root - INFO - PP rank 0 is building stage_idx 0 with start_layer None, stop_layer layers.12
[rank0]:[titan] 2025-06-15 09:51:35,252 - root - INFO - Applied Tensor Parallelism to the model
[rank0]:[titan] 2025-06-15 09:51:35,252 - root - INFO - Applied full activation checkpointing to the model
[rank0]:[titan] 2025-06-15 09:51:35,256 - root - INFO - Compiling each TransformerBlock with torch.compile
[rank0]:[titan] 2025-06-15 09:51:35,256 - root - INFO - Using pipeline schedule Interleaved1F1B with 8 microbatches and 2 stages.
[rank0]:[titan] 2025-06-15 09:51:35,514 - root - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank0]:[titan] 2025-06-15 09:51:35,515 - root - INFO - CUDA memory usage for model: 0.55GiB(1.40%)
[rank0]:[titan] 2025-06-15 09:51:35,516 - root - INFO - Trainer is initialized with local batch size 8, global batch size 8, gradient accumulation steps 1, sequence length 2048, total steps 10 (warmup 2).
[rank0]:[titan] 2025-06-15 09:51:35,516 - root - INFO - Training starts at step 1.
[rank0]:[titan] 2025-06-15 09:51:35,516 - root - INFO - Profiling active. Traces will be saved at logs/qwen_0_5b/_tp_I1f1b_sl2_bs8/
[rank3]:[titan] 2025-06-15 09:51:36,377 - root - INFO - Building llama3 0.5B with TransformerModelArgs(_enforced='This field is used to enforce all fields have defaults.', dim=896, n_layers=24, n_heads=14, n_kv_heads=2, vocab_size=128256, multiple_of=256, intermediate_size=4864, ffn_dim_multiplier=None, norm_eps=1e-05, rope_theta=500000, max_seq_len=2048, depth_init=True, use_flex_attn=False, attn_mask_type='causal', eos_id=128001)
[rank3]:[titan] 2025-06-15 09:51:36,610 - root - INFO - CUDA capacity: NVIDIA A100-PCIE-40GB with 39.50GiB memory
[rank3]:[titan] 2025-06-15 09:51:36,652 - root - INFO - [34mModel llama3 0.5B [31msize: 587,705,216 total parameters[39m
[rank3]:[titan] 2025-06-15 09:51:36,652 - root - INFO - Compiling the loss function with torch.compile
[rank3]:[titan] 2025-06-15 09:51:36,702 - root - INFO - No 'pipeline_parallel_split_points' provided. Here is the auto-generated split, which may be sub-optimal: ['layers.12'].
[rank3]:[titan] 2025-06-15 09:51:36,720 - root - INFO - PP rank 1 is building stage_idx 1 with start_layer layers.12, stop_layer None
[rank3]:[titan] 2025-06-15 09:51:36,770 - root - INFO - Applied Tensor Parallelism to the model
[rank3]:[titan] 2025-06-15 09:51:36,770 - root - INFO - Applied full activation checkpointing to the model
[rank3]:[titan] 2025-06-15 09:51:36,774 - root - INFO - Compiling each TransformerBlock with torch.compile
[rank3]:[titan] 2025-06-15 09:51:36,774 - root - INFO - Using pipeline schedule Interleaved1F1B with 8 microbatches and 2 stages.
[rank3]:[titan] 2025-06-15 09:51:37,031 - root - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank3]:[titan] 2025-06-15 09:51:37,031 - root - INFO - CUDA memory usage for model: 0.55GiB(1.40%)
[rank3]:[titan] 2025-06-15 09:51:37,033 - root - INFO - Trainer is initialized with local batch size 8, global batch size 8, gradient accumulation steps 1, sequence length 2048, total steps 10 (warmup 2).
[rank3]:[titan] 2025-06-15 09:51:37,033 - root - INFO - Training starts at step 1.
[rank3]:[titan] 2025-06-15 09:51:37,033 - root - INFO - Profiling active. Traces will be saved at logs/qwen_0_5b/_tp_I1f1b_sl2_bs8/
[rank1]:[titan] 2025-06-15 09:51:37,189 - root - INFO - Building llama3 0.5B with TransformerModelArgs(_enforced='This field is used to enforce all fields have defaults.', dim=896, n_layers=24, n_heads=14, n_kv_heads=2, vocab_size=128256, multiple_of=256, intermediate_size=4864, ffn_dim_multiplier=None, norm_eps=1e-05, rope_theta=500000, max_seq_len=2048, depth_init=True, use_flex_attn=False, attn_mask_type='causal', eos_id=128001)
[rank1]:[titan] 2025-06-15 09:51:37,421 - root - INFO - CUDA capacity: NVIDIA A100-PCIE-40GB with 39.50GiB memory
[rank1]:[titan] 2025-06-15 09:51:37,460 - root - INFO - [34mModel llama3 0.5B [31msize: 587,705,216 total parameters[39m
[rank1]:[titan] 2025-06-15 09:51:37,461 - root - INFO - Compiling the loss function with torch.compile
[rank1]:[titan] 2025-06-15 09:51:37,507 - root - INFO - No 'pipeline_parallel_split_points' provided. Here is the auto-generated split, which may be sub-optimal: ['layers.12'].
[rank1]:[titan] 2025-06-15 09:51:37,525 - root - INFO - PP rank 0 is building stage_idx 0 with start_layer None, stop_layer layers.12
[rank1]:[titan] 2025-06-15 09:51:37,573 - root - INFO - Applied Tensor Parallelism to the model
[rank1]:[titan] 2025-06-15 09:51:37,574 - root - INFO - Applied full activation checkpointing to the model
[rank1]:[titan] 2025-06-15 09:51:37,577 - root - INFO - Compiling each TransformerBlock with torch.compile
[rank1]:[titan] 2025-06-15 09:51:37,578 - root - INFO - Using pipeline schedule Interleaved1F1B with 8 microbatches and 2 stages.
[rank1]:[titan] 2025-06-15 09:51:37,820 - root - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank1]:[titan] 2025-06-15 09:51:37,820 - root - INFO - CUDA memory usage for model: 0.55GiB(1.40%)
[rank1]:[titan] 2025-06-15 09:51:37,821 - root - INFO - Trainer is initialized with local batch size 8, global batch size 8, gradient accumulation steps 1, sequence length 2048, total steps 10 (warmup 2).
[rank1]:[titan] 2025-06-15 09:51:37,821 - root - INFO - Training starts at step 1.
[rank1]:[titan] 2025-06-15 09:51:37,821 - root - INFO - Profiling active. Traces will be saved at logs/qwen_0_5b/_tp_I1f1b_sl2_bs8/
[rank2]:[titan] 2025-06-15 09:51:38,030 - root - INFO - Building llama3 0.5B with TransformerModelArgs(_enforced='This field is used to enforce all fields have defaults.', dim=896, n_layers=24, n_heads=14, n_kv_heads=2, vocab_size=128256, multiple_of=256, intermediate_size=4864, ffn_dim_multiplier=None, norm_eps=1e-05, rope_theta=500000, max_seq_len=2048, depth_init=True, use_flex_attn=False, attn_mask_type='causal', eos_id=128001)
[rank2]:[titan] 2025-06-15 09:51:38,262 - root - INFO - TensorBoard logging enabled. Logs will be saved at logs/qwen_0_5b/_tp_I1f1b_sl2_bs8/20250615-0951
[rank2]:[titan] 2025-06-15 09:51:38,262 - root - INFO - CUDA capacity: NVIDIA A100-PCIE-40GB with 39.50GiB memory
[rank2]:[titan] 2025-06-15 09:51:38,303 - root - INFO - [34mModel llama3 0.5B [31msize: 587,705,216 total parameters[39m
[rank2]:[titan] 2025-06-15 09:51:38,303 - root - INFO - Compiling the loss function with torch.compile
[rank2]:[titan] 2025-06-15 09:51:38,350 - root - INFO - No 'pipeline_parallel_split_points' provided. Here is the auto-generated split, which may be sub-optimal: ['layers.12'].
[rank2]:[titan] 2025-06-15 09:51:38,367 - root - INFO - PP rank 1 is building stage_idx 1 with start_layer layers.12, stop_layer None
[rank2]:[titan] 2025-06-15 09:51:38,416 - root - INFO - Applied Tensor Parallelism to the model
[rank2]:[titan] 2025-06-15 09:51:38,416 - root - INFO - Applied full activation checkpointing to the model
[rank2]:[titan] 2025-06-15 09:51:38,420 - root - INFO - Compiling each TransformerBlock with torch.compile
[rank2]:[titan] 2025-06-15 09:51:38,420 - root - INFO - Using pipeline schedule Interleaved1F1B with 8 microbatches and 2 stages.
[rank2]:[titan] 2025-06-15 09:51:38,714 - root - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank2]:[titan] 2025-06-15 09:51:38,714 - root - INFO - CUDA memory usage for model: 0.55GiB(1.40%)
[rank2]:[titan] 2025-06-15 09:51:38,716 - root - INFO - Trainer is initialized with local batch size 8, global batch size 8, gradient accumulation steps 1, sequence length 2048, total steps 10 (warmup 2).
[rank2]:[titan] 2025-06-15 09:51:38,716 - root - INFO - Training starts at step 1.
[rank2]:[titan] 2025-06-15 09:51:38,716 - root - INFO - Profiling active. Traces will be saved at logs/qwen_0_5b/_tp_I1f1b_sl2_bs8/
[rank1]:/home2/yeonjae/tp_partition/pytorch/torch/_inductor/lowering.py:1884: UserWarning: Torchinductor does not support code generation for complex operators. Performance may be worse than eager.
[rank1]:  warnings.warn(
[rank0]:/home2/yeonjae/tp_partition/pytorch/torch/_inductor/lowering.py:1884: UserWarning: Torchinductor does not support code generation for complex operators. Performance may be worse than eager.
[rank0]:  warnings.warn(
[rank3]:/home2/yeonjae/tp_partition/pytorch/torch/_inductor/lowering.py:1884: UserWarning: Torchinductor does not support code generation for complex operators. Performance may be worse than eager.
[rank3]:  warnings.warn(
[rank2]:/home2/yeonjae/tp_partition/pytorch/torch/_inductor/lowering.py:1884: UserWarning: Torchinductor does not support code generation for complex operators. Performance may be worse than eager.
[rank2]:  warnings.warn(
[rank3]:[titan] 2025-06-15 09:51:52,281 - root - INFO - [31mstep:  1  [32mloss: 12.2581  [33mmemory:  5.06GiB(12.82%)  [34mtps: 262  [36mtflops: 0.88  [35mmfu: 0.28%[39m
[rank3]:[titan] 2025-06-15 09:51:52,281 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:01:40
[rank0]:[titan] 2025-06-15 09:51:52,297 - root - INFO - [31mstep:  1  [32mloss: -1.0000  [33mmemory:  2.70GiB(6.83%)  [34mtps: 239  [36mtflops: 0.80  [35mmfu: 0.26%[39m
[rank0]:[titan] 2025-06-15 09:51:52,297 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:01:40
[rank1]:[titan] 2025-06-15 09:51:52,296 - root - INFO - [31mstep:  1  [32mloss: -1.0000  [33mmemory:  2.70GiB(6.83%)  [34mtps: 276  [36mtflops: 0.93  [35mmfu: 0.30%[39m
[rank1]:[titan] 2025-06-15 09:51:52,297 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:01:40
[rank2]:[titan] 2025-06-15 09:51:52,281 - root - INFO - [31mstep:  1  [32mloss: 12.2581  [33mmemory:  5.06GiB(12.82%)  [34mtps: 293  [36mtflops: 0.99  [35mmfu: 0.32%[39m
[rank2]:[titan] 2025-06-15 09:51:52,282 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:01:40
[rank0]:[titan] 2025-06-15 09:51:53,019 - root - INFO - [31mstep:  2  [32mloss: -1.0000  [33mmemory:  3.35GiB(8.47%)  [34mtps: 5,678  [36mtflops: 19.11  [35mmfu: 6.12%[39m
[rank1]:[titan] 2025-06-15 09:51:53,018 - root - INFO - [31mstep:  2  [32mloss: -1.0000  [33mmemory:  3.35GiB(8.47%)  [34mtps: 5,674  [36mtflops: 19.09  [35mmfu: 6.12%[39m
[rank3]:[titan] 2025-06-15 09:51:53,109 - root - INFO - [31mstep:  2  [32mloss: 11.7888  [33mmemory:  6.04GiB(15.30%)  [34mtps: 4,949  [36mtflops: 16.66  [35mmfu: 5.34%[39m
[rank2]:[titan] 2025-06-15 09:51:53,110 - root - INFO - [31mstep:  2  [32mloss: 11.7888  [33mmemory:  6.04GiB(15.30%)  [34mtps: 4,952  [36mtflops: 16.67  [35mmfu: 5.34%[39m
[rank3]:[titan] 2025-06-15 09:51:53,834 - root - INFO - [31mstep:  3  [32mloss: 11.1086  [33mmemory:  6.47GiB(16.38%)  [34mtps: 5,654  [36mtflops: 19.03  [35mmfu: 6.10%[39m
[rank0]:[titan] 2025-06-15 09:51:53,849 - root - INFO - [31mstep:  3  [32mloss: -1.0000  [33mmemory:  3.35GiB(8.47%)  [34mtps: 4,936  [36mtflops: 16.61  [35mmfu: 5.32%[39m
[rank1]:[titan] 2025-06-15 09:51:53,849 - root - INFO - [31mstep:  3  [32mloss: -1.0000  [33mmemory:  3.35GiB(8.47%)  [34mtps: 4,937  [36mtflops: 16.61  [35mmfu: 5.32%[39m
[rank2]:[titan] 2025-06-15 09:51:53,834 - root - INFO - [31mstep:  3  [32mloss: 11.1086  [33mmemory:  6.47GiB(16.38%)  [34mtps: 5,662  [36mtflops: 19.05  [35mmfu: 6.11%[39m
[rank3]:[titan] 2025-06-15 09:51:54,551 - root - INFO - [31mstep:  4  [32mloss: 10.3035  [33mmemory:  6.47GiB(16.38%)  [34mtps: 5,712  [36mtflops: 19.22  [35mmfu: 6.16%[39m
[rank0]:[titan] 2025-06-15 09:51:54,566 - root - INFO - [31mstep:  4  [32mloss: -1.0000  [33mmemory:  3.35GiB(8.47%)  [34mtps: 5,714  [36mtflops: 19.23  [35mmfu: 6.16%[39m
[rank1]:[titan] 2025-06-15 09:51:54,566 - root - INFO - [31mstep:  4  [32mloss: -1.0000  [33mmemory:  3.35GiB(8.47%)  [34mtps: 5,711  [36mtflops: 19.22  [35mmfu: 6.16%[39m
[rank2]:[titan] 2025-06-15 09:51:54,552 - root - INFO - [31mstep:  4  [32mloss: 10.3035  [33mmemory:  6.47GiB(16.38%)  [34mtps: 5,715  [36mtflops: 19.23  [35mmfu: 6.16%[39m
[rank3]:[titan] 2025-06-15 09:51:55,307 - root - INFO - [31mstep:  5  [32mloss: 10.0082  [33mmemory:  6.47GiB(16.38%)  [34mtps: 5,421  [36mtflops: 18.24  [35mmfu: 5.85%[39m
[rank0]:[titan] 2025-06-15 09:51:55,322 - root - INFO - [31mstep:  5  [32mloss: -1.0000  [33mmemory:  3.35GiB(8.47%)  [34mtps: 5,427  [36mtflops: 18.26  [35mmfu: 5.85%[39m
[rank1]:[titan] 2025-06-15 09:51:55,322 - root - INFO - [31mstep:  5  [32mloss: -1.0000  [33mmemory:  3.35GiB(8.47%)  [34mtps: 5,422  [36mtflops: 18.24  [35mmfu: 5.85%[39m
[rank2]:[titan] 2025-06-15 09:51:55,308 - root - INFO - [31mstep:  5  [32mloss: 10.0082  [33mmemory:  6.47GiB(16.38%)  [34mtps: 5,427  [36mtflops: 18.26  [35mmfu: 5.85%[39m
[rank0]:[titan] 2025-06-15 09:51:56,044 - root - INFO - [31mstep:  6  [32mloss: -1.0000  [33mmemory:  3.35GiB(8.47%)  [34mtps: 5,671  [36mtflops: 19.08  [35mmfu: 6.12%[39m
[rank3]:[titan] 2025-06-15 09:51:56,030 - root - INFO - [31mstep:  6  [32mloss:  9.6929  [33mmemory:  6.47GiB(16.38%)  [34mtps: 5,671  [36mtflops: 19.08  [35mmfu: 6.12%[39m
[rank1]:[titan] 2025-06-15 09:51:56,045 - root - INFO - [31mstep:  6  [32mloss: -1.0000  [33mmemory:  3.35GiB(8.47%)  [34mtps: 5,670  [36mtflops: 19.08  [35mmfu: 6.12%[39m
[rank2]:[titan] 2025-06-15 09:51:56,030 - root - INFO - [31mstep:  6  [32mloss:  9.6929  [33mmemory:  6.47GiB(16.38%)  [34mtps: 5,725  [36mtflops: 19.27  [35mmfu: 6.17%[39m
[rank3]:[titan] 2025-06-15 09:51:56,929 - root - INFO - [31mstep:  7  [32mloss:  9.5690  [33mmemory:  6.47GiB(16.38%)  [34mtps: 4,561  [36mtflops: 15.35  [35mmfu: 4.92%[39m
[rank1]:[titan] 2025-06-15 09:51:56,951 - root - INFO - [31mstep:  7  [32mloss: -1.0000  [33mmemory:  3.35GiB(8.47%)  [34mtps: 4,526  [36mtflops: 15.23  [35mmfu: 4.88%[39m
[rank0]:[titan] 2025-06-15 09:51:56,950 - root - INFO - [31mstep:  7  [32mloss: -1.0000  [33mmemory:  3.35GiB(8.47%)  [34mtps: 4,526  [36mtflops: 15.23  [35mmfu: 4.88%[39m
[rank2]:[titan] 2025-06-15 09:51:56,929 - root - INFO - [31mstep:  7  [32mloss:  9.5690  [33mmemory:  6.47GiB(16.38%)  [34mtps: 4,564  [36mtflops: 15.36  [35mmfu: 4.92%[39m
[rank3]:[titan] 2025-06-15 09:51:57,742 - root - INFO - [31mstep:  8  [32mloss:  9.5508  [33mmemory:  6.47GiB(16.38%)  [34mtps: 5,036  [36mtflops: 16.95  [35mmfu: 5.43%[39m
[rank1]:[titan] 2025-06-15 09:51:57,762 - root - INFO - [31mstep:  8  [32mloss: -1.0000  [33mmemory:  3.35GiB(8.47%)  [34mtps: 5,051  [36mtflops: 17.00  [35mmfu: 5.45%[39m
[rank0]:[titan] 2025-06-15 09:51:57,762 - root - INFO - [31mstep:  8  [32mloss: -1.0000  [33mmemory:  3.35GiB(8.47%)  [34mtps: 5,049  [36mtflops: 16.99  [35mmfu: 5.45%[39m
[rank2]:[titan] 2025-06-15 09:51:57,743 - root - INFO - [31mstep:  8  [32mloss:  9.5508  [33mmemory:  6.47GiB(16.38%)  [34mtps: 5,037  [36mtflops: 16.95  [35mmfu: 5.43%[39m
[rank3]:[titan] 2025-06-15 09:51:58,571 - root - INFO - [31mstep:  9  [32mloss:  9.3002  [33mmemory:  6.47GiB(16.38%)  [34mtps: 4,945  [36mtflops: 16.64  [35mmfu: 5.33%[39m
[rank2]:[titan] 2025-06-15 09:51:58,571 - root - INFO - [31mstep:  9  [32mloss:  9.3002  [33mmemory:  6.47GiB(16.38%)  [34mtps: 4,949  [36mtflops: 16.65  [35mmfu: 5.34%[39m
[rank1]:[titan] 2025-06-15 09:51:58,591 - root - INFO - [31mstep:  9  [32mloss: -1.0000  [33mmemory:  3.35GiB(8.47%)  [34mtps: 4,948  [36mtflops: 16.65  [35mmfu: 5.34%[39m
[rank0]:[titan] 2025-06-15 09:51:58,591 - root - INFO - [31mstep:  9  [32mloss: -1.0000  [33mmemory:  3.35GiB(8.47%)  [34mtps: 4,945  [36mtflops: 16.64  [35mmfu: 5.33%[39m
[rank3]:[titan] 2025-06-15 09:51:59,670 - root - INFO - [31mstep: 10  [32mloss:  9.1664  [33mmemory:  6.47GiB(16.38%)  [34mtps: 3,730  [36mtflops: 12.55  [35mmfu: 4.02%[39m
[rank2]:[titan] 2025-06-15 09:51:59,670 - root - INFO - [31mstep: 10  [32mloss:  9.1664  [33mmemory:  6.47GiB(16.38%)  [34mtps: 3,731  [36mtflops: 12.56  [35mmfu: 4.02%[39m
[rank1]:[titan] 2025-06-15 09:51:59,694 - root - INFO - [31mstep: 10  [32mloss: -1.0000  [33mmemory:  3.35GiB(8.47%)  [34mtps: 3,716  [36mtflops: 12.51  [35mmfu: 4.01%[39m
[rank0]:[titan] 2025-06-15 09:51:59,693 - root - INFO - [31mstep: 10  [32mloss: -1.0000  [33mmemory:  3.35GiB(8.47%)  [34mtps: 3,717  [36mtflops: 12.51  [35mmfu: 4.01%[39m
[rank1]:[titan] 2025-06-15 09:52:00,511 - root - INFO - Dumping profiler traces at step 10
[rank0]:[titan] 2025-06-15 09:52:00,508 - root - INFO - Dumping profiler traces at step 10
[rank3]:[titan] 2025-06-15 09:52:00,537 - root - INFO - Dumping profiler traces at step 10
[rank2]:[titan] 2025-06-15 09:52:00,540 - root - INFO - Dumping profiler traces at step 10
[rank1]:[titan] 2025-06-15 09:52:00,844 - root - INFO - Finished dumping profiler traces in 0.33 seconds
[rank1]:[titan] 2025-06-15 09:52:00,844 - root - INFO - Training completed
[rank0]:[titan] 2025-06-15 09:52:00,837 - root - INFO - Finished dumping profiler traces in 0.33 seconds
[rank0]:[titan] 2025-06-15 09:52:00,837 - root - INFO - Sleeping 2 seconds for other ranks to complete
[rank3]:[titan] 2025-06-15 09:52:00,894 - root - INFO - Finished dumping profiler traces in 0.36 seconds
[rank3]:[titan] 2025-06-15 09:52:00,894 - root - INFO - Training completed
[rank2]:[titan] 2025-06-15 09:52:00,895 - root - INFO - Finished dumping profiler traces in 0.35 seconds
[rank2]:[titan] 2025-06-15 09:52:00,897 - root - INFO - Training completed
[rank0]:[titan] 2025-06-15 09:52:02,840 - root - INFO - Training completed
[rank2]:[titan] 2025-06-15 09:52:03,511 - root - INFO - Process group destroyed.
[rank1]:[titan] 2025-06-15 09:52:03,664 - root - INFO - Process group destroyed.
[rank3]:[titan] 2025-06-15 09:52:03,663 - root - INFO - Process group destroyed.
[rank0]:[titan] 2025-06-15 09:52:04,016 - root - INFO - Process group destroyed.
